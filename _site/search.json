[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cillian McHugh",
    "section": "",
    "text": "I am an Experimental Psychologist specialising in cognitive and social psychology. I have worked with a diverse range of research techniques, and I have applied experience in the development of new methods and materials, and in theory development. The main focus of my research to date has been on morality and moral judgement. I have applied understandings from the cognitive psychology of concepts and categories to the moral domain to propose a novel theory of Moral Judgment as Categorization (MJAC).\nMy empirical work has largely focused on the phenomenon of moral dumbfounding; when a person defends a moral judgement in the absence of reasons. The dumbfounding paradigm has been influential in moral psychology; however, there has been limited empirical work investigating it. This has meant that the main focus of my work to date has been testing, and attempting to understand the phenomenon.\nMy interests extend beyond the moral domain to include learning and knowledge acquisition, categorisation, skill/expertise, meaning, motivation, and memory. My theoretical interests include Ecological Psychology, Embodiment, and Dynamical Systems."
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Cillian McHugh",
    "section": "",
    "text": "I am an Experimental Psychologist specialising in cognitive and social psychology. I have worked with a diverse range of research techniques, and I have applied experience in the development of new methods and materials, and in theory development. The main focus of my research to date has been on morality and moral judgement. I have applied understandings from the cognitive psychology of concepts and categories to the moral domain to propose a novel theory of Moral Judgment as Categorization (MJAC).\nMy empirical work has largely focused on the phenomenon of moral dumbfounding; when a person defends a moral judgement in the absence of reasons. The dumbfounding paradigm has been influential in moral psychology; however, there has been limited empirical work investigating it. This has meant that the main focus of my work to date has been testing, and attempting to understand the phenomenon.\nMy interests extend beyond the moral domain to include learning and knowledge acquisition, categorisation, skill/expertise, meaning, motivation, and memory. My theoretical interests include Ecological Psychology, Embodiment, and Dynamical Systems."
  },
  {
    "objectID": "index.html#featured-paper",
    "href": "index.html#featured-paper",
    "title": "Cillian McHugh",
    "section": "Featured Paper",
    "text": "Featured Paper\n\n\n\n\n  \n\n\n\n\nMoral Judgment as Categorization (MJAC)\n\n\n\n\nPerspectives in Psychological Science (2022)\n\n\n\n\n\n\n1/18/22\n\n\nCillian McHugh, Marek McGann, Eric R. Igou, Elaine L. Kinsella\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#featured-video",
    "href": "index.html#featured-video",
    "title": "Cillian McHugh",
    "section": "Featured Video",
    "text": "Featured Video\nMoral Judgment as Categorization (MJAC): presentation to IMP seminar series 31st May 2021"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMoral Judgment as Categorization (MJAC) - Explanation and Development\n\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2020\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nReasons or Rationalisations\n\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2020\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nSearching for Moral Dumbfounding\n\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mjac/index.html",
    "href": "posts/mjac/index.html",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "",
    "text": "In what follows I will attempt to provide an overview of my recently published theoretical work, covering the main ideas behind the theory, while also describing the process of its development, from initial conception to final publication.\n(presentation to IMP seminar series 31st May 2021)\n(Poster presentation to SPSP 2022)"
  },
  {
    "objectID": "posts/mjac/index.html#applying-mjac",
    "href": "posts/mjac/index.html#applying-mjac",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "Applying MJAC",
    "text": "Applying MJAC\nThis approach to understanding moral judgements provides a novel perspective on how we understand particular moral phenomena.\n\nMoral Dumbfounding\nMoral dumbfounding occurs when people defend a moral judgment even though they cannot provide a reason to support it (Haidt, Björklund, and Murphy 2000; Haidt 2001; McHugh et al. 2017, 2020). Moral dumbfounding is most frequently obeserved for harmless taboo behaviors (consensual incest, cannibalism involving a body that is already dead). The taboo nature of these topics means that they are consistently identified as morally wrong without much discussion [the Scottish public petitions committee notably dismissed a call to legalize incest with no discussion at all; see Sim (2016)]. This leads to a high degree of stability in categorizing them as wrong. However, the taboo nature of these behaviors prevents them from being discussed. This means that a typical encounter with such behavior involves little more than identifying it as wrong, possibly with an expression of disgust, and changing the subject. Because of this combination of stability and This process logically leads to moral dumbfounding.\n\n\nCategorizing people versus categorizing actions\nMJAC predicts people’s judgements will focus on the actor or on the action depending on the situation. Consider the following two scenarios:\n\n\nYou find out that a colleague has been fired for stealing from your employer—they have been bringing home office equipment for their own personal use, and they have been exaggerating their expense claims.\n\n\n\n\nA close friend of yours reveals to you that they have been stealing from their employer—they have been bringing home office equipment for their own personal use, and they have been exaggerating their expense claims.\n\n\nMJAC predicts that people will be more lenient in their judgments of the person in the second scenario than the in first scenario. Indeed this is consistent with what is found in the literature (Heiphetz and Craig 2020; Forbes 2018; Lee and Holyoak 2020; Hofmann et al. 2014; Weidman et al. 2020).\nA further prediction is that for the second scenario, people will focus on the action rather than the actor. People are motivated to see close others positively (Forbes 2018; Murray, Holmes, and Griffin 1996a, 1996b). If faced with a situation in which a close other did something wrong, people would try to avoid making a negative judgment of the person (Ditto, Pizarro, and Tannenbaum 2009; Murray, Holmes, and Griffin 1996a, 1996b). One way to avoid this is to focus on the action rather than the actor. Relatedly, for favorable judgments, we expect the opposite effect. If a close other does something praiseworthy, people are likely to focus on the actor rather than the action, helping to maintain a positive view of the close other (Forbes 2018; Murray, Holmes, and Griffin 1996a, 1996b).\nA key goal of moral categorization is to distinguish ‘good’ people from ‘bad’ people, to help us navigate the social world, and effectively guide our social interactions. Learning about people’s moral character or moral ‘essence’, enables us to establish relationships with ‘good’ people, and to limit our interactions with ‘bad’ people (or at least treat interactions with ‘bad’ people with caution). This means that for strangers, we are likely to show a bias for categorizing the actor rather than the action (Uhlmann, Pizarro, and Diermeier 2015; Dunlea and Heiphetz 2020; Siegel, Crockett, and Dolan 2017; Siegel et al. 2018)."
  },
  {
    "objectID": "posts/mjac/index.html#contrasting-mjac-with-existing-approaches",
    "href": "posts/mjac/index.html#contrasting-mjac-with-existing-approaches",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "Contrasting MJAC with Existing Approaches",
    "text": "Contrasting MJAC with Existing Approaches\nPerhaps the most important difference between MJAC and existing approaches is that the focus of MJAC is on the cognitive processes, rather than on the content of moral judgements. According to the dominant dual-process approaches, different types of moral judgements are grounded in different types of cognitive processes. Deontological (principled or rule based) moral judgements are grounded in intuitive/automatic/emotional/model-free processes, while utilitarian or consequentialist judgements (where the aim is to maximise positive outcomes), are grounded in deliberative/controlled/cognitive/model-based processes. These different processes mean that our judgements are susceptible to specific kinds of contexual influences, e.g., how persona/impersonal an action is (Greene 2008, 2016), the relative amount of emotionality (Byrd and Conway 2019; Conway et al. 2018; Conway and Gawronski 2013; Goldstein-Greenwood et al. 2020), or whether the focus is on the action or the outcome (Cushman 2013; Crockett 2013). An overview of these approaches is displayed in Figure 3. Despite these important insights, there are a range of other context effects known to influence moral judgements that are not accounted for by these models. These other context effects are detailed in the green boxes in Figure 3.1\n\n\n\nFigure 3: A sketch of dual-process approaches (contextual influences not directly addressed by these approaches highlighted in green)\n\n\nMJAC does not make assumptions based on the content of moral judgements. However, MJAC predicts a distinction between habitual (or skilled, or intuitive) responses, and deliberative (or controlled) responses. This distinction does not make assumptions about specific content of moral judgments. However, thinking about the contexts in which deontological vs utilitarian judgements are generally made, it makes sense that deontological rules might become more habitual (think It’s wrong to hurt people, Thou shalt not kill, You shouldn’t hit your sister), while utilitarian judgements may require more deliberation (e.g., how should we divide these resources in the fairest manner?). MJAC therefore integrates the insights of dual-process accounts, while also allowing for greater variability, and a more diverse range of context effects. Figure 4 outlines the various influences on moral judgement according to MJAC.\n\n\n\nFigure 4: Influences on moral judgements according to MJAC\n\n\nThese differences in assumptions between MJAC and other approaches lead to differences in explanations and predictions. Above I have outlined moral dumbfounding as an example of such an explanation. The differences in assumptions and explanations are listed in Table 2. To avoid making this post too long and drawn out, I will not go into detail on these differences, however I point you to the relevant section in the main article for more detailed discussion on this.\n\nTable 2: Contrasting MJAC with other approaches\n\n\n\n\n\n\n\n\n\n\n\nGreene’s Dual-process theory\n“Soft” dual-process theory\nModel-based accounts\nTDM\nMJAC\n\n\n\n\nAssumptions:\n\n\n\n\n\n\n\nContent\nDeontology-utilitarianism / personal-impersonal\nDeontology-utilitarianism\nAction-outcome\nHarm-based, dyadic\nDynamical Context-dependent Goal-directed\n\n\nMoral “Essence”\n(Implicit)\n(Not discussed)\n(Implicit)\nExplicit\nRejected\n\n\nProcesses\nDual-processes\nDual-processes\nDual-processes\n(implicitly dual-process)\nContinuum\n\n\nMechanisms\nIntuition (emotion) / cognition\nEmotion / cognition\nModel-based / model-free\nCategorization (unspecified)\nType-token interpretation\n\n\nPhenomena Explained:\n\n\n\n\n\n\n\nDumbfounding (harmless wrongs)\n(Not discussed)\n(Not discussed)\nExplained\nDenied\nExplained: learning history\n\n\nWrongless harms\n(Not discussed)\n(Not discussed)\n(Not discussed)\nDenied\nExplained: learning history\n\n\nTypicality\n(Not discussed)\n(Not discussed)\n(Not discussed)\nMatching of “prototype”\nContext-dependent\n\n\nContextual influences\nSpecific: Personal-impersonal\nSpecific: Emotion / cognition\n\nSpecific: Action-outcome\nSpecific: Harm-based\n\n\n\nIn the opening sections I outlined two general predictions of MJAC. We have also identified various specific predictions (e.g., the categorizing of actors vs actions described above). For brevity I do not go into detail on these specific predictions, but point you to the main article for this more detailed discussion (here is probably a good place to start)."
  },
  {
    "objectID": "posts/mjac/index.html#rejections",
    "href": "posts/mjac/index.html#rejections",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "Rejections",
    "text": "Rejections\nBy July 2019 it was ready for submission. It received a fairly quick desk reject from the first outlet.\nIt went to review in the second journal we tried. I’ve had some difficult rejections, and rejections that I disagreed with, but this one was really rough. Reviewer 1 really did not like the idea. The review from Reviewer 1 contained some of the harshest review comments I have seen. A few excerpts from the (very long) review are below.\n\nThe authors compile a great deal of research findings. They argue that moral judgment is about categorization, a position that flies in the face of and does not account for the decades of research on moral judgment in developmental and educational psychology.  The paper is incoherent in narrative, inadequate and misleading in explanation, and overall does not advance the field.  The paper seems to bounce from one thing to another without a clear, coherent story. Perhaps their thin version of moral judgment is a type of categorization, one based on perceiver making the other into a dead object. But so what? What does it buy the field? How does it contribute to scholarship?\n\n\n\n…. And rejected…some good constructive feedback, and some fairly harsh words… Skin is getting thicker with each paper #AcademicTwitter #rejected … I guess I'll try again tomorrow, for now I'm going to watch The Deuce #thedeuce https://t.co/VDKDwORhev pic.twitter.com/I6wGjajslL\n\n— Cillian McHugh ((CillianMacAodh?)) November 7, 2019\n\n\nThe experience with Reviewer 1 was a bit of a blow to the confidence. We brought it to our lab and made some fairly minor changes before sending it out again. Just before the Christmas break in 2019 I submitted it to Perspectives on Psychological Science (fully expecting to receive another reject). In February 2020 I went to SPSP in New Orleans, where I also was due to present MJAC as a poster at the Justice and Morality pre-conference. Upon landing I checked my email, and was very surprised to have received an R&R."
  },
  {
    "objectID": "posts/mjac/index.html#revisions",
    "href": "posts/mjac/index.html#revisions",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "Revisions",
    "text": "Revisions\nThe reviews were really fair, but extensive. The original framing relied heavily on the parallels provided in Table 1 above. The reviewers were not convinced by this argument. We were encouraged to clearly differentiate MJAC from existing approaches, with instructions to identify and better engage with specific dual-process accounts, and with the Theory of Dyadic Morality (Schein and Gray 2018).\nSo the revisions were really quite tough. I approached them systematically, addressing each comment as well as was possible, and documenting how the changes made addressed each comment. Many of the comments addressed fairly deep conceptual questions, requiring extensive reading before I could begin attempting to address them. And, naturally, I shared my progress on social media:\n\n\nYup! Starting to see progress!Green is done ✅ lighter green is done, but not sure if it's good enough 🤷‍♂️ Blue means there was nothing to be done 👌🏼 red…. well there's still time 😬🙈 https://t.co/zPehbIWlYr pic.twitter.com/YVNVtonRfW\n\n— Cillian McHugh ((CillianMacAodh?)) May 13, 2020\n\n\nI was also able to call on social media for help in addressing specific issues that came up while doing the revisions. The replies came in quickly, and really provided excellent resources to help with the revisions.\n\n\nSimpson et al. (2016) look at third-party observers who witnessed a transgression between two targets who share a relationship, and Hofmann et al. (2018) look at social closeness and differences in desire to punish (2/2)\n\n— Rachel Forbes ((rc_forbes?)) May 10, 2020\n\n\nFollowing weeks of work, we had finally addressed all the reviewer comments. I was sick of it at this stage, and ready to submit. Unfortunately, the extent of the revisions meant that the manuscript was too long to submit. I noted that there is technically no word limit for submissions at Perspectives on Psychological Science, but my co-authors wisely convinced me to ask for an extension so we could cut down the words to something more manageable.\n\n\nSo working on major revisions of a manuscript, all comments are pretty much addressed…but now we're facing the problem of length…Original submission between 8,000 and 9,000 wordsCurrent version is 20,000 words…Aaaaaah  😫😖#AcademicTwitter #AcademicChatter✂️🤔\n\n— Cillian McHugh ((CillianMacAodh?)) May 20, 2020\n\n\nSo we spent another few weeks cutting words, and restructuring sections to be more streamlined (huge credit to coauthors in this endeavour). And eventually we submitted the revised manuscript. It was unrecognisable from the original submission.\n\n\nAnd revised manuscript submitted! 12,037 in the end!That was exhausting! 😂 https://t.co/c56NeW51Cr\n\n— Cillian McHugh ((CillianMacAodh?)) June 22, 2020\n\n\nWe submitted the revised version in June 2020, and received a decision of Conditional Accept (with minor revisions) in September. It was fully accepted in November 2020, and published online in July 2021 (almost 7 years after the original idea was formed)\n\nKey Points in the Revisions\nI think one of the most important changes that came from the review process was the clarity in the argument. The original submission simply presented the approach, but we didn’t articulate a clear problem the approach was solving. This was a tough question to address. The range of existing approaches available means that trying to go into detail on the relative strengths and weaknesses is not feasible. In contrast, a more general approach risks over-generalizing and potentially mis-representing some approaches. As the revisions progressed this core argument presented itself. We identified the variability and context dependency of moral judgements as a challenge that is not well addressed in the existing literature. Because dynamism and context dependency are core assumptions of MJAC, this means that MJAC is well positioned to address this challenge.\n\n\n\n(Whedon 1997)"
  },
  {
    "objectID": "posts/mjac/index.html#references",
    "href": "posts/mjac/index.html#references",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/mjac/index.html#footnotes",
    "href": "posts/mjac/index.html#footnotes",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe reader is referred to the wealth of literature examining such factors as emotional influences, Cameron, Payne, and Doris (2013); intentionality, evitability, benefit recipient, Christensen et al. (2014); Christensen and Gomila (2012); action-outcome distinction Crockett (2013); cushman_action_2013; trustworthiness and social evaluation Everett, Pizarro, and Crockett (2016); Everett et al. (2018); personal-impersonal distinction, Greene et al. (2001); doctrine of double effect, Mikhail (2000); level of physical contact, Valdesolo and DeSteno (2006); order effects, Wiegmann, Okan, and Nagel (2012)↩︎"
  },
  {
    "objectID": "posts/reasons/index.html",
    "href": "posts/reasons/index.html",
    "title": "Reasons or Rationalisations",
    "section": "",
    "text": "This post provides a brief overview of a paper where we tested alternative explanations of moral dumbfounding proposed by critics (McHugh et al. 2020).\n(full text available here)"
  },
  {
    "objectID": "posts/reasons/index.html#evidence-for-judgements-based-on-principles-or-reasons",
    "href": "posts/reasons/index.html#evidence-for-judgements-based-on-principles-or-reasons",
    "title": "Reasons or Rationalisations",
    "section": "Evidence for Judgements based on Principles or Reasons",
    "text": "Evidence for Judgements based on Principles or Reasons\nThe strongest challenge to moral dumbfounding came from a set of studies by Royzman, Kim, and Leeman (2015), which tested moral dumbfounding directly, using the Julie and Mark dilemma which reads:\n\nJulie and Mark, who are brother and sister, are travelling together in France. They are both on summer vacation from college. One night they are staying alone in a cabin near the beach. They decide that it would be interesting and fun if they tried making love. At very least it would be a new experience for each of them. Julie was already taking birth control pills, but Mark uses a condom too, just to be safe. They both enjoy it, but they decide not to do it again. They keep that night as a special secret between them, which makes them feel even closer to each other.\n\n\nRoyzman et al. (2015) identified two principles that appear to be guiding judgements\n\nThe Harm Principle\nThe Norm Principle\n\nParticipants who endorse either principle have a reason to judge the behaviour of Julie and Mark as wrong (and therefore cannot be dumbfounded)\n\n\nExclusion of Participants\nRoyzman et al. (2015) excluded participants who endorsed either principle from analysis, and found that dumbfounding effectively disappeared.\n\nHarm-based reasons\nParticipants were presented with the following two questions (emphasis added here):\n\n“are you able to believe that Julie and Mark’s having sex with each other will not negatively affect the quality of their relationship or how they feel about each other later on?”\n“are you able to believe that Julie and Mark’s having sex with each other will have no bad consequences for them personally and/or for those close to them?”\n\nIf participants responded “no” to either question they were excluded from analysis.\n\n\nNorm-based reasons\nParticipants were presented with the following two statements and asked to select the one they agrred with most (emphasis added here):\n\n“violating an established moral norm just for fun or personal enjoyment is wrong only in situations where someone is harmed as a result, but is acceptable otherwise.”\n“violating an established moral norm just for fun or personal enjoyment is inherently wrong even in situations where no one is harmed as a result.”\n\nParticipants who selected the second statement were excluded from analysis."
  },
  {
    "objectID": "posts/reasons/index.html#three-exclusion-criteria",
    "href": "posts/reasons/index.html#three-exclusion-criteria",
    "title": "Reasons or Rationalisations",
    "section": "Three Exclusion Criteria",
    "text": "Three Exclusion Criteria\nWe adopted the same method as Royzman et al. (2015), and excluded participants whose judgements could be attributed to either the harm principle or the norm principle. However rather than relying on endorsing alone, we developed additional, and more rigorous, exclusion criteria. We compared rates of dumbfounding based on these different criteria. We also tested the relative accuracy of the exclusion criteria (based on false exclusions).\nFirst we assessed whether participants articulated either principle. We provided an open-ended response question and asked participants to provide reasons for their judgements. The reasons provided were coded for mention of either principle.\nSecond we assessed whether participants applied the harm principle across different contexts. We asked participants 3 questions about their judgements of behaviours/activities that could potentially lead to harm2\nAcross three studies we assessed rates of dumbfounding based on these different exclusion criteria.\n\nEndorsing (Royzman et al., 2015, all studies)\nArticulating (all studies)\nApplying (Studies 2 and 3)"
  },
  {
    "objectID": "posts/reasons/index.html#measuring-dumbfounding",
    "href": "posts/reasons/index.html#measuring-dumbfounding",
    "title": "Reasons or Rationalisations",
    "section": "Measuring Dumbfounding",
    "text": "Measuring Dumbfounding\nDumbfounding was measured using the critical slide which read as follows:\n\n“Julie and Mark’s actions did not harm anyone or negatively affect anyone. How can there be anything wrong with what they did?” 1. There is nothing wrong 2. It’s wrong but I can’t think of a reason 3. It’s wrong and I can provide a valid reason\n\n(The selecting of option 2, the admission of not having reasons, was taken to be a dumbfounded response)"
  },
  {
    "objectID": "posts/reasons/index.html#rates-of-dumbfounding-depending-on-exclusion-type",
    "href": "posts/reasons/index.html#rates-of-dumbfounding-depending-on-exclusion-type",
    "title": "Reasons or Rationalisations",
    "section": "Rates of Dumbfounding depending on Exclusion type",
    "text": "Rates of Dumbfounding depending on Exclusion type\nOn the graphs below you can see the frequency of each response to the critical slide for each study. The furthest left bars represent the full sample. The responses for each of the sub-samples following the relevant exclusion criterion are displayed and labelled separately.\nThe response we are intersted in is the red bar denoting a dumbfounded response. Percentages of the full sample are displayed within the plot. Percentages of the relevant sub-samples are displayed in parenthesis below the count.\n\nStudy 1:\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nStudy 1 - Responses to critical slide for the full sample and for each exclusion criterion\n\n\n\n\n\n\nStudy 2:\n\n\n\n\n\nStudy 2 - Responses to critical slide for the full sample and for each exclusion criterion\n\n\n\n\n\n\nStudy 3:\n\n\n\n\n\nStudy 3 - Responses to critical slide for the full sample and for each exclusion criterion\n\n\n\n\nAs we can see above, dumbfounded responding is found in the full sample for each study.\nReplicating the finding by Royzman et al. (2015), if we exclude participants who endorse either principle, rates of dumbfounding are negligible.\nHowever, if we also account for whether people articulate or apply either principle, dumbfounded responding is observed. So whether or not dumbfounding is real, depends on which exclusion criterion is more accurate."
  },
  {
    "objectID": "posts/reasons/index.html#relative-accuracy-of-exclusion-criteria",
    "href": "posts/reasons/index.html#relative-accuracy-of-exclusion-criteria",
    "title": "Reasons or Rationalisations",
    "section": "Relative Accuracy of Exclusion Criteria",
    "text": "Relative Accuracy of Exclusion Criteria\nWhile we do not have a direct measure of the relative accuracy of the different exclusion criteria, we can assess the relative rates of false exclusions. All exclusions are based on attributing participants’ judgements to either principle. In this case, the judgements should be consistent with the relevant principle. As such, participants who selected “There is nothing wrong” (blue bars above) should not be excluded from analysis. Any participant who selected “There is nothing wrong” and was excluded, was falsely excluded.\nBelow we have subsetted the participants who selected “There is nothing wrong” across each study, and plot the rates of rates of false exclusion based on each exclusion criterion. The percentage of participants who selected “There is nothing wrong” is displayed within the plot, and the percentage of the full sample is displayed in parenthesis below the count.\n\n\n\n\n\nExclusion of participants who selected ‘There is nothing wrong’\n\n\n\n\nAs can be seen from the above, rates of false exclusion are much higher when endorsing is the only exclusion criterion. Some false exclusion remains for each of the other exclusion criteria, however it is a considerable improvement. Based on this we conclude that these revised criteria are more robust, and our studies provide evidence that Moral Dumbfounding is indeed real."
  },
  {
    "objectID": "posts/reasons/index.html#references",
    "href": "posts/reasons/index.html#references",
    "title": "Reasons or Rationalisations",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/reasons/index.html#footnotes",
    "href": "posts/reasons/index.html#footnotes",
    "title": "Reasons or Rationalisations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMoral dumbfounding occurs when people stubbornly maintain a moral judgement, even though they can provide no reason to support their judgements (Haidt 2001; Haidt, Björklund, and Murphy 2000; Prinz 2005; McHugh et al. 2017). Dumbfounded responses may include: (a) an admission of not having a reason for a judgement, (b) the use of an unsupported declarations (“It’s just wrong!”) to defend a judgement.↩︎\n“How would you rate the behavior of two people who engage in an activity that could potentially result in harmful consequences for either of them?”; “Do you think boxing is wrong?”; “Do you think playing contact team sports (e.g. rugby; ice-hockey; American football) is wrong?”↩︎"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nCognitive Load Can Reduce Reason-Giving in a Moral Dumbfounding Task\n\n\n\n\n\n\n\nmorality\n\n\nmoral judgment\n\n\ncategorization\n\n\ndual-processes\n\n\nmoral dumbfounding\n\n\n\n\nCollabra: Psycholgy (2023)\n\n\n\n\n\n\nApr 3, 2023\n\n\nCillian McHugh, Marek McGann, Eric R. Igou, Elaine L. Kinsella\n\n\n\n\n\n\n  \n\n\n\n\nJust wrong? Or just WEIRD?\n\n\nInvestigating the prevalence of moral dumbfounding in non-Western samples\n\n\n\n\nmorality\n\n\nmoral dumbfounding\n\n\nWEIRD\n\n\n\n\nMemory & Cognition (2023)\n\n\n\n\n\n\nJan 17, 2023\n\n\nCillian McHugh, Run Zhang, Tanuja Karnatak, Nishtha Lamba, Olga Khokhlova\n\n\n\n\n\n\n  \n\n\n\n\nPredicting attitudinal and behavioral responses to COVID-19 pandemic using machine learning\n\n\n\n\n\n\n\nCOVID-19\n\n\nmoral identity\n\n\nPolitical Ideology\n\n\nmachine learning\n\n\n\n\nPNAS Nexus (2022)\n\n\n\n\n\n\nJul 5, 2022\n\n\nTomislav PavlovićJay J. Van Bavel, Flavio Azevedo, … … …, Cillian McHugh, … … …, Siobhán M. Griffin, … … …, Jay J. Van Bavel\n\n\n\n\n\n\n  \n\n\n\n\nMoral Identity Predicts Adherence to COVID-19 Mitigation Procedures Depending on Political Ideology:\n\n\nA Comparison Between the USA and New Zealand\n\n\n\n\nmorality\n\n\nmoral identity\n\n\nCOVID-19\n\n\nPolitical Ideology\n\n\nPolarization\n\n\n\n\nPolitical Psychology (2022)\n\n\n\n\n\n\nJun 17, 2022\n\n\nCillian McHugh, Siobhán M. Griffin, Melanie J. McGrath, Joshua J. Rhee, Paul J. Maher, Darragh McCashin, Jenny Roth\n\n\n\n\n\n\n  \n\n\n\n\nPandemic threat and group cohesion:\n\n\nnational identification in the wake of COVID-19 is associated with authoritarianism\n\n\n\n\nright wing authoritarianism\n\n\nCOVID-19\n\n\nPolitical Ideology\n\n\nPolarization\n\n\n\n\nThe Journal of Social Psychology (2022)\n\n\n\n\n\n\nFeb 9, 2022\n\n\nPaul J. Maher, Jenny Roth, Siobhán M. Griffin, Aoife Marie Foran, Sarah Jay, Cillian McHugh, Megan Ryan, Daragh Bradshaw, Michael Quayle, Orla T. Muldoon\n\n\n\n\n\n\n  \n\n\n\n\nNational Identity Predicts Public Health Support during a Global Pandemic\n\n\n\n\n\n\n\nCOVID-19\n\n\nPolitical Ideology\n\n\nIdentification\n\n\n\n\nNature Communications (2022)\n\n\n\n\n\n\nJan 26, 2022\n\n\nJay J. Van Bavel, Aleksandra Cichocka, … … …, Cillian McHugh, … … …, Siobhán M. Griffin, … … …, Paulo S. Boggio\n\n\n\n\n\n\n  \n\n\n\n\nMoral Judgment as Categorization (MJAC)\n\n\n\n\n\n\n\nmorality\n\n\nmoral judgment\n\n\ncategorization\n\n\n\n\nPerspectives in Psychological Science (2022)\n\n\n\n\n\n\nJan 18, 2022\n\n\nCillian McHugh, Marek McGann, Eric R. Igou, Elaine L. Kinsella\n\n\n\n\n\n\n  \n\n\n\n\nThe Political Psychology of COVID-19\n\n\n\n\n\n\n\npolitical psychology\n\n\nCOVID-19\n\n\nPolitical Ideology\n\n\nPolarization\n\n\npolitics\n\n\n\n\nPolitical Psychology (2021)\n\n\n\n\n\n\nOct 1, 2021\n\n\nOrla T. Muldoon, James H. Liu, Cillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nSolidarity Matters:\n\n\nPrototypicality and Minority and Majority Adherence to National COVID-19 Health Advice\n\n\n\n\nsolidarity\n\n\nidentity\n\n\nCOVID-19\n\n\n\n\nInternational Review of Social Psychology (2022)\n\n\n\n\n\n\nSep 14, 2021\n\n\nAoife-Marie Foran, Jenny Roth, Sarah Jay, Siobhán M. Griffin, Paul J. Maher, Cillian McHugh, Daragh Bradshaw, Megan Ryan, Michael Quayle, Orla T. Muldoon\n\n\n\n\n\n\n  \n\n\n\n\nReasons or Rationalisations:\n\n\nThe Role of Principles in the Moral Dumbfounding Paradigm\n\n\n\n\nmorality\n\n\nmoral dumbfounding\n\n\nmethods\n\n\n\n\nJournal of Behavioral Decision Making (2020)\n\n\n\n\n\n\nJan 5, 2020\n\n\nCillian McHugh, Marek McGann, Eric R. Igou, Elaine L. Kinsella\n\n\n\n\n\n\n  \n\n\n\n\nSearching for Moral Dumbfounding:\n\n\nIdentifying Measurable Indicators of Moral Dumbfounding\n\n\n\n\nmorality\n\n\nmoral dumbfounding\n\n\nmethods\n\n\n\n\nCollabra: Psychology (2017)\n\n\n\n\n\n\nSep 20, 2017\n\n\nCillian McHugh, Marek McGann, Eric R. Igou, Elaine L. Kinsella\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/searching/index.html",
    "href": "posts/searching/index.html",
    "title": "Searching for Moral Dumbfounding",
    "section": "",
    "text": "This post provides a brief overview of a paper where we developed methods for measuring and testing moral dumbfounding (McHugh et al. 2017)."
  },
  {
    "objectID": "posts/searching/index.html#results",
    "href": "posts/searching/index.html#results",
    "title": "Searching for Moral Dumbfounding",
    "section": "Results",
    "text": "Results\nTwenty two of the 31 participants (70.97%) produced a dumbfounded response at least once. (admissions of not having reasons; or the use of an unsupported declaration as a justification)\n\nExamples of such responses included:\n\n“It just seems wrong and I cannot explain why, I don’t know”\n“because I just think it’s wrong, oh God, I don’t know why, it’s just [pause] wrong”.\n\n\nThe rates of each type of response for each Scenario are displayed in Figure @ref(fig:figdumb1Interview)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\nUse of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nRates of each type of response for each scenario"
  },
  {
    "objectID": "posts/searching/index.html#behavioural-responses",
    "href": "posts/searching/index.html#behavioural-responses",
    "title": "Searching for Moral Dumbfounding",
    "section": "Behavioural Responses",
    "text": "Behavioural Responses\nIn addition to coding the videos for dumbfounded responding, we investigated a range of verbal and non-verbal behavioural responses. We found significant differences between dumbfounded participants and non-dumbfounded participants across a small number of variables. There were significant differences in time spent\n\nLaughing\nSmiling\nin Silence\n\n(with higher rates of each in dumbfounded participants)\nWe found no differences in\n\nverbal hesitations; non-verbal hesitations\nchanging posture; fidgeting/hands on the self\nfrowning\n\nImportantly, there were no differences depending on which measure of dumbfounding was used  (Admissions of not having reasons / Unsupported declarations)"
  },
  {
    "objectID": "posts/searching/index.html#results-1",
    "href": "posts/searching/index.html#results-1",
    "title": "Searching for Moral Dumbfounding",
    "section": "Results",
    "text": "Results\nExtremely high rates of dumbfounded responding were recorded, see Figure @ref(fig:figdumb1comp1).\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\nUse of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\n\n\n\nRates of each type of response for each scenario"
  },
  {
    "objectID": "posts/searching/index.html#results---admissions-only",
    "href": "posts/searching/index.html#results---admissions-only",
    "title": "Searching for Moral Dumbfounding",
    "section": "Results - Admissions Only",
    "text": "Results - Admissions Only\nWe have provided two sets of results below, firstly the results with just the responses to the critical slide. Following this we include the open-ended responses coded for unsupported declarations. Figure @ref(fig:figdumb1comp2) shows the rate of each response to the critical slide for each scenario.\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\nUse of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\n\n\n\nRates of each type of response for each scenario"
  },
  {
    "objectID": "posts/searching/index.html#results---including-open-ended-responses",
    "href": "posts/searching/index.html#results---including-open-ended-responses",
    "title": "Searching for Moral Dumbfounding",
    "section": "Results - including open-ended responses",
    "text": "Results - including open-ended responses\nFigure @ref(fig:figdumb1comp2string) shows the rates of each type of response, when the coded open-ended responses are included.\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\nUse of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\n\n\n\nRates of each type of response for each scenario"
  },
  {
    "objectID": "posts/searching/index.html#combined-tables",
    "href": "posts/searching/index.html#combined-tables",
    "title": "Searching for Moral Dumbfounding",
    "section": "Combined Tables",
    "text": "Combined Tables\n\n\n(#tab:tab1judge)\n\n\nRatings of each scenario for each study\n\n\n\n\nStudy\nJudgement\n\nHeinz\n\nCannibal\n\nIncest\n\nTrolley\n\n\n\n\nStudy 1\nInitial: Wrong\n27\n87.1%\n25\n80.65%\n26\n83.87%\n23\n74.19%\n\n\n\nInitial: Neutral\n0\n0%\n0\n0%\n0\n0%\n0\n0%\n\n\n\nInitial: OK\n4\n12.9%\n6\n19.35%\n5\n16.13%\n8\n25.81%\n\n\n\nRevised: Wrong\n26\n83.87%\n23\n74.19%\n20\n64.52%\n22\n70.97%\n\n\n\nRevised: Neutral\n0\n0%\n0\n0%\n0\n0%\n1\n3.23%\n\n\n\nReviesd: OK\n5\n16.13%\n8\n25.81%\n11\n35.48%\n8\n25.81%\n\n\nStudy 2\nInitial: Wrong\n53\n73.61%\n68\n94.44%\n63\n87.5%\n50\n69.44%\n\n\n\nInitial: Neutral\n9\n12.5%\n3\n4.17%\n3\n4.17%\n6\n8.33%\n\n\n\nInitial: OK\n10\n13.89%\n1\n1.39%\n6\n8.33%\n16\n22.22%\n\n\n\nRevised: Wrong\n51\n70.83%\n67\n93.06%\n66\n91.67%\n48\n66.67%\n\n\n\nRevised: Neutral\n7\n9.72%\n3\n4.17%\n3\n4.17%\n9\n12.5%\n\n\n\nReviesd: OK\n14\n19.44%\n2\n2.78%\n3\n4.17%\n15\n20.83%\n\n\nStudy 3a\nInitial: Wrong\n54\n75%\n67\n93.06%\n61\n84.72%\n48\n66.67%\n\n\n\nInitial: Neutral\n6\n8.33%\n3\n4.17%\n7\n9.72%\n10\n13.89%\n\n\n\nInitial: OK\n12\n16.67%\n2\n2.78%\n4\n5.56%\n14\n19.44%\n\n\n\nRevised: Wrong\n53\n73.61%\n67\n93.06%\n57\n79.17%\n43\n59.72%\n\n\n\nRevised: Neutral\n11\n15.28%\n4\n5.56%\n12\n16.67%\n15\n20.83%\n\n\n\nReviesd: OK\n8\n11.11%\n1\n1.39%\n3\n4.17%\n14\n19.44%\n\n\nStudy 3b\nInitial: Wrong\n81\n80.2%\n85\n84.16%\n71\n70.3%\n66\n65.35%\n\n\n\nInitial: Neutral\n9\n8.91%\n13\n12.87%\n20\n19.8%\n14\n13.86%\n\n\n\nInitial: OK\n11\n10.89%\n3\n2.97%\n10\n9.9%\n21\n20.79%\n\n\n\nRevised: Wrong\n87\n86.14%\n82\n81.19%\n73\n72.28%\n59\n58.42%\n\n\n\nRevised: Neutral\n10\n9.9%\n15\n14.85%\n19\n18.81%\n17\n16.83%\n\n\n\nReviesd: OK\n4\n3.96%\n4\n3.96%\n9\n8.91%\n25\n24.75%\n\n\n\n\n\n\n(#tab:tab2dumb)\n\n\nObserved frequency and percentage of each of the responses: dumbfounded, nothing wrong, and reasons provided\n\n\n\n\n\n\n\nHeinz\n\nCannibal\n\nIncest\n\nTrolley\n\n\n\n\nStudy 1\nNothing wrong\n6\n19.35%\n8\n25.81%\n11\n35.48%\n8\n25.81%\n\n\n\nDumbfounded\n0\n0%\n11\n35.48%\n18\n58.06%\n3\n9.68%\n\n\n\n(admissions)\n0\n0%\n8\n25.81%\n10\n32.26%\n3\n9.68%\n\n\n\n(declarations)\n0\n0%\n3\n9.68%\n8\n25.81%\n0\n0%\n\n\n\nReasons\n25\n80.65%\n12\n38.71%\n2\n6.45%\n20\n64.52%\n\n\nStudy 2\nNothing wrong\n8\n11.11%\n4\n5.56%\n2\n2.78%\n10\n13.89%\n\n\n\nDumbfounded\n45\n62.5%\n46\n63.89%\n54\n75%\n45\n62.5%\n\n\n\nReasons\n19\n26.39%\n22\n30.56%\n16\n22.22%\n17\n23.61%\n\n\nStudy 3a\nNothing wrong\n14\n19.44%\n4\n5.56%\n12\n16.67%\n15\n20.83%\n\n\n(critical slide)\nDumbfounded\n13\n18.06%\n14\n19.44%\n18\n25%\n14\n19.44%\n\n\n\nReasons\n45\n62.5%\n54\n75%\n42\n58.33%\n43\n59.72%\n\n\nStudy 3a\nNothing wrong\n14\n19.44%\n4\n5.56%\n12\n16.67%\n15\n20.83%\n\n\n(coded)\nDumbfounded\n19\n26.39%\n21\n29.17%\n31\n43.06%\n22\n30.56%\n\n\n\nReasons\n39\n54.17%\n47\n65.28%\n29\n40.28%\n35\n48.61%\n\n\nStudy 3b\nNothing wrong\n21\n20.79%\n10\n9.9%\n31\n30.69%\n24\n23.76%\n\n\n(critical slide)\nDumbfounded\n12\n11.88%\n19\n18.81%\n16\n15.84%\n16\n15.84%\n\n\n\nReasons\n68\n67.33%\n72\n71.29%\n54\n53.47%\n61\n60.4%\n\n\nStudy 3b\nNothing wrong\n21\n20.79%\n10\n9.9%\n31\n30.69%\n24\n23.76%\n\n\n(coded)\nDumbfounded\n16\n15.84%\n30\n29.7%\n28\n27.72%\n22\n21.78%\n\n\n\nReasons\n64\n63.37%\n61\n60.4%\n42\n41.58%\n55\n54.46%\n\n\n\n\n\n\n(#tab:tab3Qs)\n\n\nResponses to post-discussion questionnaire questions\n\n\n\n\nStudy\nQuestion\nHeinz\nCannibal\nIncest\nTrolley\n\n\n\n\nStudy 1\nChanged mind\n2.87\n3.40\n2.63\n2.60\n\n\n\nConfidence\n5.30\n4.77\n5.40\n5.07\n\n\n\nConfused\n3.00\n3.67\n3.33\n3.70\n\n\n\nIrritated\n3.00\n3.33\n3.13\n3.37\n\n\n\n‘Gut’\n5.23\n5.20\n4.97\n5.07\n\n\n\n‘Reason’\n4.83\n4.40\n4.43\n4.77\n\n\n\nGut minus Reason\n0.40\n0.80\n0.53\n0.30\n\n\nStudy 2\nConfidence\n6.10\n5.86\n5.62\n5.26\n\n\n\nConfused\n2.40\n3.08\n4.14\n3.17\n\n\n\nIrritated\n4.58\n4.68\n4.32\n4.28\n\n\n\n‘Gut’\n5.29\n5.54\n5.82\n4.96\n\n\n\n‘Reason’\n4.89\n5.19\n4.89\n4.93\n\n\n\nGut minus Reason\n0.40\n0.35\n0.93\n0.03\n\n\nStudy 3a\nChanged mind\n2.38\n1.67\n2.00\n2.00\n\n\n\nConfidence\n5.22\n5.50\n5.38\n4.81\n\n\n\nConfused\n2.75\n2.96\n3.25\n2.89\n\n\n\nIrritated\n3.94\n4.64\n4.07\n3.60\n\n\n\n‘Gut’\n4.78\n5.44\n5.44\n4.92\n\n\n\n‘Reason’\n5.07\n5.26\n5.11\n5.06\n\n\n\nGut minus Reason\n-0.29\n0.18\n0.33\n-0.14\n\n\nStudy 3b\nChanged mind\n1.74\n1.60\n1.57\n1.83\n\n\n\nConfidence\n5.78\n6.16\n5.81\n5.36\n\n\n\nConfused\n2.06\n2.07\n2.12\n2.22\n\n\n\nIrritated\n4.42\n4.01\n3.56\n3.39\n\n\n\n‘Gut’\n4.42\n4.43\n4.47\n4.01\n\n\n\n‘Reason’\n5.46\n5.69\n5.26\n5.58\n\n\n\nGut minus Reason\n-1.04\n-1.27\n-0.79\n-1.57"
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html",
    "href": "publications/just-wrong-or-just-weird/index.html",
    "title": "Just wrong? Or just WEIRD?",
    "section": "",
    "text": "Moral dumbfounding occurs when people maintain a moral judgment even though they cannot provide a reason for this judgment. Dumbfounded responding may include admitting to not having reasons, or the use of unsupported declarations (“It’s just wrong”) as justification for a judgment. Published evidence for dumbfounding has drawn exclusively on samples of WEIRD backgrounds (Western, educated, industrialized, rich, and democratic), and it remains unclear to what extent the phenomenon is generalizable to other populations. Furthermore, the theoretical implications of moral dumbfounding have been disputed in recent years. In three studies we apply a standardized moral dumbfounding task, and show evidence for moral dumbfounding in a Chinese sample (Study 1, N = 165), an Indian sample (Study 2, N = 181), and a mixed sample primarily (but not exclusively) from North Africa and the Middle East (MENA region, Study 3, N = 264). These findings are consistent with a categorization theories of moral judgment.”\nThe phenomenon of moral dumbfounding occurs when people defend a moral judgement even though they cannot provide a reason in support of this judgement (Haidt, Björklund, and Murphy 2000; McHugh et al. 2017). It typically manifests as an explicit admission of not having reasons, or the use of unsupported declarations (e.g., “It’s just wrong”) as a justification for a judgement. For almost two decades, evidence for moral dumbfounding was limited to a single study, unpublished in peer reviewed form, and with a total sample of N = 30 (Haidt, Björklund, and Murphy 2000). This meant that, while the phenomenon was widely discussed in the literature, its existence was not well supported by empirical evidence. Recent work (McHugh et al. 2017, 2020), has provided additional evidence for the existence moral dumbfounding, demonstrating that it can be reliably elicited (though perhaps it not as widespread as previously assumed, see Royzman, Kim, and Leeman 2015; McHugh et al. 2020).\nDespite this recent work, it remains unclear how universal or generalisable the phenomenon is. Current evidence is limited to research involving exclusively WEIRD (Western, educated, industrialised, rich, and democratic, see Henrich, Heine, and Norenzayan 2010) samples. The purpose of the current research is to extend research on moral dumbfounding beyond these exclusively WEIRD contexts. Specifically we test for the presence of moral dumbfounding in a Chinese sample (Study 1), in an Indian sample (Study 2), and in a mixed sample from a diverse range of non-WEIRD countries (Study 3)."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#method",
    "href": "publications/just-wrong-or-just-weird/index.html#method",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Method",
    "text": "Method\n\nParticipants and design\nStudy 1 was a frequency based attempted replication of McHugh et al. (2017). The aim of Study 1 was to identify if dumbfounded responding could be evoked in a Chinese context. Results are primarily descriptive. We have included exploratory analyses investigating the possible influence of individualism/collectivism (Renzhi et al. 2013) on responding.\nAn initial sample of 42 (34 female, 8 male, 0 other; Mage = 21.43, min = 18, max = 27, SD = 1.74) participants took part. An additional 123 (75 female, 48 male, 0 other; Mage = 22.06, min = 18, max = 45, SD = 3.76) participants completed a follow-up study with the Cannibal scenario only, and in which we experimentally manipulated psychological distance (drawing on construal level theory e.g., Liberman, Trope, and Stephan 2007). The experimental manipulation had no influence on dumbfounded responding, \\(\\chi\\)2(2, N = 123) = 2.304, p = .316, V = 0.1023383, and as such we have included these participants in the current analysis (for clarity these studies are reported separately as Study 1a and Study 1b). Participants in this study were undergraduate students and postgraduate students, from Luoyang Normal University (China). All participants were recruited from China and had a high level of Chinese, the entire online questionnaire survey was presented in Chinese. Participation was voluntary and participants were not reimbursed for their participation.\n\n\nProcedure and materials\nData were collected through the Chinese language online survey software Wenjuanxing (“Wenjuanxing” 2006). Participants were provided with a link to the online survey. The first page of the survey was an information sheet. If participants chose to continue, they proceeded to the second page, the consent form. Participants could only proceed to the remainder of the survey if they provide consent on the consent form. Upon providing consent and proceeding, participants completed some questions relating to basic demographics.\nThe procedure and materials for the moral dumbfounding task were taken directly from McHugh et al. (2017). These were translated into Chinese by a member of the research team whose native language was Chinese. Four moral judgement scenarios were used, two “intuition” scenarios: Incest, Cannibal, and two “reasoning” scenarios Trolley, Heinz (taken from McHugh et al. 2017, see Appendix A).\nMoral dumbfounding task. The basic procedure for moral dumbfounding tasks is as follows. Participants are presented with a scenario to read. They are then asked to rate, on a 7-point Likert scale (1 = Morally wrong; 4 = Neutral; 7 = Morally right), how right or wrong they regarded the behaviour described in the scenario. Following this participants are asked to rate their confidence in their judgement (again on a 7-point Likert scale). Participants are then presented with a series of counterarguments, which refuted commonly used justifications for rating the behaviours as “wrong” (see Appendix B). After each counter-argument, participants are asked “Do you (still) think it is wrong?”, with a binary “yes/no” response option; and then they are asked “Do you have a reason for your judgement?”, with three possible response option “Yes, I have a reason”, “No I have no reason”, and “Unsure”. This sequence was repeated for each of the three counter-arguments.\nDumbfounding is measured using the “critical slide” which contains a statement defending the behaviour, and a question asking how the behaviour could be wrong (see Appendix C). There are three possible answer options: (a) “There is nothing wrong”; (b) an admission of not having reasons (“It’s wrong but I can’t think of a reason”); and finally a judgement with accompanying justification (c) “It’s wrong and I can provide a valid reason”. The selecting of option (b), the admission of not having reasons, is taken to be a dumbfounded response. Participants who selected (c) were promoted to type a reason on the next page. The order of these response options was randomised.\nFollowing the critical slide, participants rated the behaviour again, and completed the post-discussion questionnaire devised by Haidt, Björklund, and Murphy (2000). They were required to rate on a 7-point Likert scale how sure were they about their judgement; how much they changed their mind; how confused were they; how irritated were they; how much was their judgement based on reason; how much was their judgement based on “gut” feeling (see Appendix D). This process is repeated in full for each moral scenario. The order of presentation of the moral scenarios was randomised.\nCoding reasons. While there is a strong theoretical and empirical case for coding the reasons provided for unsupported declarations or tautological responses, as dumbfounded responses (see McHugh et al. 2017), this approach has been challenged by claims that these responses constitute the expression of a normative position (e.g., Royzman, Kim, and Leeman 2015). In response to this challenge, we adopt an “admission of not having reasons” as the only measure of moral dumbfounding in these studies. While this measure provides a more conservative estimate of the prevalence of moral dumbfounding, it provides a considerably less ambiguous estimate.\nIndividualism-collectivism scale. Following the dumbfounding task, participants completed the individualism-collectivism scale (Li and Aksoy 2007, see Appendix E). This sixteen-item scale includes four sub-scales: Vertical Collectivism (VC), Horizontal Collectivism (HC), Vertical Individualism (VI) and Horizontal Individualism (HI). The responses were recorded on a 9-point Likert scale ranging from 1 = strongly disagree, to 9 = strongly agree. The entire study lasted approximately twenty minutes."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#results-and-discussion",
    "href": "publications/just-wrong-or-just-weird/index.html#results-and-discussion",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nJudgements of the scenarios\nThe mean initial ratings for each scenario are as follows: MHeinz = 4.76, SDHeinz = 2.07; MCannibal = 1.52, SDCannibal = 1.13; MIncest = 2.88, SDIncest = 2.23; MTrolley = 3.29, SDTrolley = 1.95. The mean revised ratings for each scenario are as follows: MHeinz = 4.74, SDHeinz = 2.04; MCannibal = 1.6, SDCannibal = 1.08; MIncest = 2.9, SDIncest = 2.13; MTrolley = 3.36, SDTrolley = 2.03. The proportion of wrong, neutral, and ok, judgements for each scenario are displayed in Table @ref(tab:tab2judge).\n\n\n(#tab:tab2judge)\n\n\nValence of initial and revised judgements for each scenarion for each study\n\n\n\n\n\n\nN\npercent\nN\npercent\nN\npercent\nN\npercent\n\n\n\n\nStudy 1a\nInitial: Wrong\n9\n21.43%\n17\n40.48%\n25\n59.52%\n37\n88.1%\n\n\n\nInitial: Neutral\n13\n30.95%\n19\n45.24%\n8\n19.05%\n4\n9.52%\n\n\n\nInitial: ok\n20\n47.62%\n6\n14.29%\n9\n21.43%\n1\n2.38%\n\n\n\nRevised: Wrong\n10\n23.81%\n19\n45.24%\n23\n54.76%\n37\n88.1%\n\n\n\nRevised: Neutral\n12\n28.57%\n14\n33.33%\n10\n23.81%\n5\n11.9%\n\n\n\nRevised: ok\n20\n47.62%\n9\n21.43%\n9\n21.43%\n0\n0%\n\n\nStudy 1b\nInitial: Wrong\n-\n-\n-\n-\n-\n-\n84\n68.29%\n\n\n\nInitial: Neutral\n-\n-\n-\n-\n-\n-\n21\n17.07%\n\n\n\nInitial: OK\n-\n-\n-\n-\n-\n-\n18\n14.63%\n\n\n\nInitial: Wrong\n-\n-\n-\n-\n-\n-\n80\n65.04%\n\n\n\nInitial: Neutral\n-\n-\n-\n-\n-\n-\n28\n22.76%\n\n\n\nInitial: OK\n-\n-\n-\n-\n-\n-\n15\n12.2%\n\n\nStudy 2\nInitial: Wrong\n132\n70.21%\n127\n67.55%\n117\n62.23%\n146\n77.66%\n\n\n\nInitial: Neutral\n17\n9.04%\n18\n9.57%\n45\n23.94%\n27\n14.36%\n\n\n\nInitial: ok\n36\n19.15%\n39\n20.74%\n23\n12.23%\n12\n6.38%\n\n\n\nRevised: Wrong\n138\n73.4%\n124\n65.96%\n110\n58.51%\n147\n78.19%\n\n\n\nRevised: Neutral\n13\n6.91%\n23\n12.23%\n39\n20.74%\n24\n12.77%\n\n\n\nRevised: ok\n32\n17.02%\n37\n19.68%\n36\n19.15%\n14\n7.45%\n\n\nStudy 3\nInitial: Wrong\n155\n57.2%\n151\n55.72%\n82\n30.26%\n176\n64.94%\n\n\n\nInitial: Neutral\n37\n13.65%\n42\n15.5%\n99\n36.53%\n32\n11.81%\n\n\n\nInitial: ok\n36\n13.28%\n36\n13.28%\n62\n22.88%\n23\n8.49%\n\n\n\nRevised: Wrong\n148\n54.61%\n140\n51.66%\n51\n18.82%\n173\n63.84%\n\n\n\nRevised: Neutral\n37\n13.65%\n53\n19.56%\n96\n35.42%\n30\n11.07%\n\n\n\nRevised: ok\n37\n13.65%\n33\n12.18%\n91\n33.58%\n23\n8.49%\n\n\n\n\nA paired samples t-test revealed no differences in the ratings of behaviours from time one to time two, Heinz, t(81.979254) = 0.053, p = .958, d = 0.0115954; Cannibal, t(81.8464504) = -0.296, p = .768, d = 0.0644897; Incest, t(81.8102068) = -0.05, p = .960, d = 0.0109172; Trolley, t(81.868551) = -0.164, p = .870, d = 0.0358119.\nA one-way ANOVA revealed significant differences in initial judgements depending on scenario, F(3, ,, , 164) = 20.77, p &lt; .001, partial \\(\\eta\\)2 = .275. Tukey’s post-hoc pairwise comparison revealed that judgements in the Heinz dilemma were significantly more favourable than for each of the other scenarios: Cannibal, p &lt; .001, Incest, p &lt; .001, Trolley, p = .003; while judgements of Cannibal were significantly more harsh than all other scenarios: Heinz, p &lt; .001, Incest, p = .007. Trolley, p &lt; .001; there was no significant difference between judgements of Incest and of Trolley, p = .762.\nA one-way ANOVA revealed the same pattern of differences in revised judgements depending on scenario, F(3, ,, , 164) = 20.19, p &lt; .001, partial \\(\\eta\\)2 = .270. Again, Tukey’s post-hoc pairwise comparison revealed that judgements in the Heinz dilemma were significantly more favourable than for each of the other scenarios: Cannibal, p &lt; .001, Incest, p &lt; .001, Trolley, p = .005; while judgements of Cannibal were significantly more harsh than all other scenarios: Heinz, p &lt; .001, Incest, p = .009. Trolley, p &lt; .001; there was no significant difference between judgements of Incest and of Trolley, p = .685.\n\n\nTables of other Responses\n\n\n(#tab:tab2other)\n\n\nMeans and Standard Deviations for initial and revised confidence ratings, and for self-reported changed mind for each scenario for each study\n\n\n\n\n\n\nM\nSD\nM\nSD\nM\nSD\nM\nSD\n\n\n\n\nStudy 1a\nInitial Confidence\n5.5\n1.3\n5.5\n1.5\n5.4\n1.2\n5.8\n1.7\n\n\n\nRevised Confidence\n5.7\n1.4\n5.4\n1.7\n5.4\n1.5\n6.1\n1.5\n\n\n\nChanged Mind\n2.7\n2\n2.6\n1.9\n2.5\n2.1\n2.5\n2.1\n\n\nStudy 1b\nInitial Confidence\n-\n-\n-\n-\n-\n-\n4.9\n1.6\n\n\n\nRevised Confidence\n-\n-\n-\n-\n-\n-\n5.1\n1.7\n\n\n\nChanged Mind\n-\n-\n-\n-\n-\n-\n2.5\n1.7\n\n\nStudy 2\nInitial Confidence\n5.9\n1.2\n5.3\n1.6\n5.8\n1.4\n5.6\n1.4\n\n\n\nRevised Confidence\n5.8\n1.3\n5.5\n1.5\n5.7\n1.4\n5.7\n1.3\n\n\n\nChanged Mind\n1.7\n1.3\n2.1\n1.5\n2\n1.6\n1.8\n1.4\n\n\nStudy 3\nInitial Confidence\n5.8\n1.6\n5.4\n1.8\n5.6\n1.5\n5.8\n1.6\n\n\n\nRevised Confidence\n5.6\n1.6\n5.5\n1.6\n5.7\n1.4\n6\n1.5\n\n\n\nChanged Mind\n2.1\n1.6\n2.1\n1.5\n2.2\n1.7\n1.7\n1.3\n\n\n\n\n\n\n\n(#tab:tabPostQs)\n\n\nMeans and Standard Deviations for responses to the post-discussion questionnaire for each scenario for each study\n\n\n\n\n\n\nM\nSD\nM\nSD\nM\nSD\nM\nSD\n\n\n\n\nStudy 1a\nConfused\n2.3\n1.6\n3\n2.2\n2.3\n1.6\n2.4\n2.1\n\n\n\nIrritated\n2.3\n1.6\n2.9\n2\n2.5\n2\n3.6\n2.5\n\n\n\nReason-Based\n5.2\n1.8\n5.2\n1.8\n5.6\n1.6\n5.6\n1.8\n\n\n\nGut-Based\n3.5\n2\n3.7\n2\n3.4\n2\n3.7\n2.3\n\n\nStudy 1b\nConfused\n-\n-\n-\n-\n-\n-\n3\n2.1\n\n\n\nIrritated\n-\n-\n-\n-\n-\n-\n3.7\n2.1\n\n\n\nReason-Based\n-\n-\n-\n-\n-\n-\n5.2\n1.6\n\n\n\nGut-Based\n-\n-\n-\n-\n-\n-\n4.3\n1.9\n\n\nStudy 2\nConfused\n1.9\n1.4\n2.4\n1.7\n2\n1.6\n2\n1.4\n\n\n\nIrritated\n2.2\n1.7\n2.3\n1.7\n2.4\n1.9\n2.6\n1.9\n\n\n\nReason-Based\n5.5\n1.7\n5.1\n1.8\n5.1\n1.8\n5.2\n1.9\n\n\n\nGut-Based\n3.5\n2.3\n3.4\n2.1\n3.8\n2.2\n3.4\n2.2\n\n\nStudy 3\nConfused\n2.4\n1.9\n2.8\n2\n2.3\n1.8\n2.1\n1.7\n\n\n\nIrritated\n3.3\n2.3\n3.3\n2.2\n2.3\n1.8\n4\n2.4\n\n\n\nReason-Based\n5.5\n1.5\n5.4\n1.8\n5.5\n1.7\n5.5\n1.8\n\n\n\nGut-Based\n3.9\n2.1\n3.7\n2.2\n3.9\n2.2\n4\n2.4\n\n\n\n\n\n\n\nMeasuring dumbfounding\nParticipants who selected the admission of not having reasons were identified as dumbfounded. Across the four scenarios (Study 1a), 21 participants (50%) provided a dumbfounded response at least once. In Study 1b, 47 participants, (38.2113821%) provided a dumbfounded response for the Cannibal scenario. Table @ref(tab:tab2dumb) shows the number and percentage of participants who selected each response for each scenario across Studies 1a and 1b. Figure @ref(fig:chinesefig) shows this information for Study 1a, while Figure @ref(fig:chinesefig2) additionally includes the responses for Study1b. Crucially for the current study, rates of dumbfounded responding for each scenario in Study 1a were significantly greater than zero, Heinz: z = 2.97, p = .003; Trolley: z = 3.17, p = .001; Incest: z = 2.76, p = .006; Cannibal: z = 3.92, p &lt; .001. Similarly rates of dumbfounded responding in Study 1b were significantly greater than zero for the Cannibal scenario, z = 7.62, p &lt; .001.\n\n\n(#tab:tab2dumb)\n\n\nObserved frequency and percentage of each of the responses: dumbfounded, nothing wrong, and reasons provided for each scenario for each study\n\n\n\n\n\n\nN\npercent\nN\npercent\nN\npercent\nN\npercent\n\n\n\n\nStudy 1a\nNothing wrong\n14\n33.33%\n11\n26.19%\n19\n45.24%\n5\n11.9%\n\n\n\nDumbfounded\n8\n19.05%\n9\n21.43%\n7\n16.67%\n13\n30.95%\n\n\n\nReasons\n20\n47.62%\n22\n52.38%\n16\n38.1%\n24\n57.14%\n\n\nStudy 1b\nNothing wrong\n-\n-\n-\n-\n-\n-\n19\n15.45%\n\n\n\nDumbfounded\n-\n-\n-\n-\n-\n-\n47\n38.21%\n\n\n\nReasons\n-\n-\n-\n-\n-\n-\n57\n46.34%\n\n\nStudy 2\nNothing wrong\n50\n27.14%\n42\n22.8%\n75\n40.71%\n35\n19%\n\n\n\nDumbfounded\n21\n11.4%\n47\n25.51%\n33\n17.91%\n44\n23.88%\n\n\n\nReasons\n112\n60.79%\n95\n51.56%\n77\n41.79%\n106\n57.53%\n\n\nStudy 3\nNothing wrong\n60\n22.14%\n38\n14.02%\n170\n62.73%\n46\n16.97%\n\n\n\nDumbfounded\n33\n12.18%\n56\n20.66%\n22\n8.12%\n45\n16.61%\n\n\n\nReasons\n130\n47.97%\n132\n48.71%\n48\n17.71%\n140\n51.66%\n\n\n\n\n\n\n\n\n\nRates of each type of response for each scenario in the Chinese Sample ( = 42)\n\n\n\n\n\n\n\n\n\nRates of each type of response for each scenario in the Chinese Sample (including additional data on Cannibal scenario)\n\n\n\n\nThere was no significant difference in observed rates of dumbfounded responding depending on which scenario was being discussed, \\(\\chi\\)2(6, N = 271) = 12.34, p = .055. Similarly, there was no influence of type of scenario (reasoning vs intuition) on rates of dumbfounded responding \\(\\chi\\)2(2, N = 271) = 0.31, p = .855.\nWe found clear evidence for dumbfounded responding in our Chinese sample. Interestingly, while the Incest scenario is generally regarded as the most reliable for eliciting moral dumbfounding in Western samples (e.g., Royzman, Kim, and Leeman 2015), Cannibal appeared to be the scenario most likely to elicit dumbfounding in this sample. While this difference in responding to the critical slide is not statistically significant, we did observe significantly harsher judgements for Cannibal than for the other scenarios. The pattern of responding to the critical slide is therefore not surprising. Furthermore, it is possible that the small sample size meant that our study was not sufficiently powered to detect differences in responding to the critical slide. As such, we note that the converging evidence across three measures (initial judgement, revised judgement, and critical slide), point towards issues surrounding death and respect for the dead as being more relevant in this Chinese sample. This is consistent with existing research on the death taboo, and the importance of death in Chinese culture (e.g., Selin and Rakoff 2019; Wu and Lu 2011). This interpretation is further corroborated by analysis of the open-ended responses, with 20 participants (47.62%) providing statements such as “Jennifer eating human flesh is an immoral and uncivilized behaviour”. This suggests that while WEIRD samples appear to be more inclined to moralise, and present as dumbfounded for, the Incest scenario, it appears (from our small and limited sample) that it is the Cannibal scenario that is of greater concern to Chinese participants.\n\n\nIndividual differences (Study 1a)\nA hierarchical linear regression was conducted to test the possible relationship between ICS (Renzhi et al. 2013), and susceptibility to dumbfounding. Susceptibility to dumbfounding was operationalised by creating a new variable representing the number of times each participant provided a dumbfounded response. This measure was included as our outcome variable, and the four sub-scales of ICS were included as predictor variables. The overall model did not significantly predict susceptibility to dumbfounding \\(R^2 = .09\\), \\(F(4, 37) = 0.95\\), \\(p = .448\\).\nWe conducted a series of multinomial logistic regressions to investigate the possible relationship between ICS (Renzhi et al. 2013) and responding to each of the scenarios individually. Response to the critical slide scenario was the dependent variable for each scenario, and the four sub-scales of the ICS were included as predictor variables.\nThe overall model did not significantly predict responses for the Heinz dilemma, \\(\\chi\\)2(8, N = 42) = 10.48, p = .233, the observed power was 0.61; neither did the model significantly predict responses for the Trolley scenario, \\(\\chi\\)2(8, N = 42) = 11.02, p = .201, the observed power was 0.64; the Incest scenario, \\(\\chi\\)2(8, N = 42) = 13.92, p = .084, the observed power was 0.76; nor the Cannibal scenario, \\(\\chi\\)2(8, N = 42) = 6.84, p = .554, the observed power was 0.41.\n\n\nIndividual differences (Study 1b)\nGiven the larger sample size in Study 1b, we additionally tested if ICS (Renzhi et al. 2013) predicted responses to the Cannibal scenario in Study 1b. We conducted a multinomial logistic regression with response to the critical slide as the outcome variable and the four sub-scales of the ICS entered as predictor variables. Overall the model did not significantly predict responses to the critical slide for the Cannibal scenario, \\(\\chi\\)2(8, N = 123) = 10.96, p = .204, the observed power was 0.64."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#method-1",
    "href": "publications/just-wrong-or-just-weird/index.html#method-1",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Method",
    "text": "Method\n\nParticipants and design\nStudy 2 was a frequency based attempted replication of McHugh et al. (2017). The aim of Study 2 was to identify if dumbfounded responding could be evoked in an Indian context.\nA total sample of 188 (116 female, 69 male, 0 other, 3 declined to report their gender; Mage = 22.88, min = 16, max = 39, SD = 2.55) participants took part. The breakdown of participants’ religion is as follows, Hinduism: n = 138, Islam: n = 4, Christianity: n = 7, Sikhism: n = 4, Buddhism: n = 0, Jainism: n = 8, other: n = 10, and 17 participants declined to provide their religion. All participants were of Indian nationality, and 164 indicated that they resided in India at the time of completing the survey. Participants were recruited through snowball sampling.\n\n\nProcedure and materials\nThe procedure for Study 2 was the same as for Study 1, with some minor changes. Given the diversity of languages in India, and the high proficiency of English among Indian nationals, all written materials were presented in English. The survey was programmed and presented using Qualtrics. The demographic information recorded additionally included religion, given the prominence and diversity of religions in Indian society. We also included the meaning in life questionnaire (MLQ: Steger et al. 2008) in Study 2. The entire study lasted twenty to twenty-five minutes."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#results-and-discussion-1",
    "href": "publications/just-wrong-or-just-weird/index.html#results-and-discussion-1",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nJudgements of the scenarios\nThe mean initial ratings for each scenario were as follows: MHeinz = 2.61, SDHeinz = 1.88; MCannibal = 2.1, SDCannibal = 1.52; MIncest = 2.66, SDIncest = 1.84; MTrolley = 2.83, SDTrolley = 1.84. The mean revised ratings for each scenario are as follows: MHeinz = 2.66, SDHeinz = 1.84; MCannibal = 2.15, SDCannibal = 1.5; MIncest = 2.94, SDIncest = 1.94; MTrolley = 2.84, SDTrolley = 1.81. The proportion of wrong, neutral, and ok, judgements for each scenario are displayed in Table @ref(tab:tab2judge).\nA paired samples t-test revealed no differences in the ratings of behaviours from time one to time two, Heinz, t(365.9540429) = -0.26, p = .795, d = 0.0270679; Cannibal, t(367.9546849) = -0.31, p = .757, d = 0.0322612; Incest, t(367.0415483) = -1.403, p = .161, d = 0.1459137; Trolley, t(365.8881139) = -0.057, p = .954, d = 0.0059546.\nA one-way ANOVA revealed significant differences in initial judgements depending on scenario, F(3, ,, , 735) = 5.72, p &lt; .001, partial \\(\\eta\\)2 = .023. Tukey’s post-hoc pairwise comparison revealed that judgements of Cannibal were significantly more harsh than all other scenarios: Heinz, p = .033, Incest, p = .013. Trolley, p &lt; .001; there were no significant differences in the ratings of the other scenarios, Heinz/Incest, p = .988, Heinz/Trolley, p = .631, Incest/Trolley, p = .819.\nA one-way ANOVA revealed the same pattern of differences in revised judgements depending on scenario, F(3, ,, , 733) = 7.17, p &lt; .001, partial \\(\\eta\\)2 = .029. Again, Tukey’s post-hoc pairwise comparison revealed that judgements of Cannibal were significantly more harsh than all other scenarios: Heinz, p = .034, Incest, p &lt; .001. Trolley, p = .001; there were no significant differences in the ratings of the other scenarios, Heinz/Incest, p = .416, Heinz/Trolley, p = .763, Incest/Trolley, p = .944.\n\n\nMeasuring dumbfounding\nParticipants who selected the admission of not having reasons were identified as dumbfounded. Across the four scenarios 90 participants (49.1803279%) provided a dumbfounded response at least once. Table @ref(tab:tab2dumb) and Figure @ref(fig:indiafig) show the number and percentage of participants who selected each response for each scenario. Rates of dumbfounded responding for each scenario in Study 2 were significantly greater than zero, Heinz: z = 4.6, p &lt; .001; Trolley: z = 7.35, p &lt; .001; Incest: z = 6.03, p &lt; .001; Cannibal: z = 7.08, p &lt; .001, thus providing evidence for moral dumbfounding in our Indian sample. To test fot the possibility of order effects, we conducted a chi-square test for independence which revealed no significant differences in responses to the critical slide depending on when the scenario was presented, \\(\\chi\\)2(6, N = 183) = 8, p = .238.\n\n\n\n\n\nRates of each type of response for each scenario in the Indian Sample ( = 181)\n\n\n\n\nA chi-square test for independence revealed significant differences in responses to the critical slide depending on which scenario was being discussed, \\(\\chi\\)2(6, N = 183) = 36.86, p &lt; .001. Table @ref(tab:tabchisq1) shows the observed counts, expected counts and standardised residuals for each response for each scenario. For Heinz, people were significantly better at providing reasons, and significantly less likely to present as dumbfounded; while people were significantly more likely to be dumbfounded by Trolley than expected; for Incest, people were significantly less likely to provide reasons, and significantly more likely to select “There is nothing wrong” than expected; finally for Cannibal significantly fewer than expected selected “There is nothing wrong”.\nThe observed variability was not related to the type of scenario (intuition vs reasoning), with no relationship between type of scenario and response to the critical slide being observed, \\(\\chi\\)2(2, N = 183) = 3.63, p = .163.\n\n\n(#tab:tabchisq1)\n\n\nObserved counts, expected counts, and standardised residuals for each response to the critical slide depending on Scenario\n\n\n\n\nResponse\n\nHeinz\nTrolley\nIncest\nCannibal\n\n\n\n\nNothing Wrong\nObserved count\n50\n42\n75\n35\n\n\n\nExpected count\n50\n50\n51\n51\n\n\n\nStandardised residuals\n-0.03\n-1.61\n4.63**\n-2.99*\n\n\nDumbfounded\nObserved count\n21\n47\n33\n44\n\n\n\nExpected count\n36\n36\n36\n36\n\n\n\nStandardised residuals\n-3.22*\n2.31*\n-0.73\n1.62\n\n\nReason\nObserved count\n112\n95\n77\n106\n\n\n\nExpected count\n97\n97\n98\n98\n\n\n\nStandardised residuals\n2.59*\n-0.4\n-3.56**\n1.38\n\n\n\n\nNote. * = sig. at &lt; .05; ** = sig. at &lt; .001\n\n \n\nStudy 2 provided evidence that dumbfounded responding can be elicited in an Indian sample. Interestingly, the Cannibal appeared to be of more concern to the participants in this sample than the Incest scenario. Indeed, the proportion of participants selecting “there is nothing wrong” for the Incest scenario was significantly higher (75 participants; 40.7055631%) than for the other scenarios. This also appears to be higher than reported in previous studies involving WEIRD samples, however there is notable fluctuation in the selecting of this response for the Incest scenario, ranging from 16.7% (McHugh et al. 2017, Study 3a) to 32.4% (McHugh et al. 2020, Study 2). Regarding the Cannibal scenario, it appears the relative importance of death observed in Study 1 is similarly present in our Study 2 sample, pointing towards potentially important cultural differences that should be considered in future studies.\n\n\nIndividual differences\nA hierarchical linear regression was conducted to test the possible relationship between ICS (Renzhi et al. 2013), MLQ (Steger et al. 2008), and susceptibility to dumbfounding. As in Study 1a, we created a new variable by calculating the number of times each participant provided a dumbfounded response, and used this variable as a measure of participants’ susceptibility to dumbfounding. This measure was our outcome variable, and the four sub-scales of ICS, along with both sub-scales of the MLQ, were included as predictor variables. The overall model was a significant predictor of susceptibility to dumbfounding \\(R^2 = .08\\), \\(F(6, 176) = 2.42\\), \\(p = .029\\), with Vertical Individualism as the only variable making a significant contribution to the model, \\(b = -0.02\\), 95% CI \\([-0.05, 0.00]\\), \\(t(176) = -2.03\\), \\(p = .044\\), see Table @ref(tab:regressiontable).\n\n\n(#tab:regressiontable)\n\n\nStudy 2: Predictors of susceptibility to moral dumbfounding\n\n\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(t\\)\n\\(\\mathit{df}\\)\n\\(p\\)\n\n\n\n\nIntercept\n1.62\n[0.51, 2.72]\n2.88\n176\n.004\n\n\nVC\n0.02\n[0.00, 0.05]\n1.96\n176\n.052\n\n\nHC\n0.00\n[-0.03, 0.03]\n-0.14\n176\n.886\n\n\nVI\n-0.02\n[-0.05, 0.00]\n-2.03\n176\n.044\n\n\nHI\n-0.03\n[-0.06, 0.00]\n-1.75\n176\n.082\n\n\nMLQ Presence\n-0.01\n[-0.04, 0.01]\n-1.21\n176\n.229\n\n\nMLQ Search\n0.01\n[-0.02, 0.04]\n0.92\n176\n.361\n\n\n\n\nWe conducted a series of logistic regressions to investigate the possible relationship between ICS (Renzhi et al. 2013) and responding to each of the scenarios individually. Response to the critical slide scenario was the dependent variable for each scenario, and the four sub-scales of the ICS were included as predictor variables.\nThe overall model did not significantly predict responses for the Heinz dilemma, \\(\\chi\\)2(12, N = 183) = 13.68, p = .322, the observed power was 0.67; neither did the model significantly predict responses for the Trolley scenario, \\(\\chi\\)2(12, N = 183) = 14.43, p = .274, the observed power was 0.7.\nInterestingly, the overall model significantly predicted responses for the Incest scenario, \\(\\chi\\)2(12, N = 183) = 26.33, p = .010, the observed power was 0.95. The overall model explained between 10.68% (Cox and Snell R square) and 14.43% (Nadelkerke R squared) of the variance in responses to the critical slide. The only significant predictors in the model were Horizontal Individualism, and Vertical Collectivism. As HI increased, participants were significantly more likely to select “there is nothing wrong” than to provide reasons for their judgement, Wald = 7.44, p = .006, odds ratio = 1.1097603, 95% CI [1.0297624, 1.1959729]. As VC increased, participants were significantly more likely to present as dumbfounded than to provide reasons, Wald = 5.01, p = .025, odds ratio = 0.9151576, 95% CI [0.8467967, 1.0036718].\nThe overall model also significantly predicted responses for the Cannibal scenario, \\(\\chi\\)2(12, N = 183) = 24.63, p = .017, the observed power was 0.94. The overall model explained between 7.16% (Cox and Snell R square) and 11.49% (Nadelkerke R squared) of the variance in responses to the critical slide. Meaning in Life: Presence (Steger et al. 2008) was the only significant predictor in the model, as Meaning in Life: Presence, increased, participants were significantly more likely provide reasons than to present as dumbfounded, Wald = 5.81, p = .016, odds ratio = 0.9258257, 95% CI [0.8695836, 0.9857055]."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#method-2",
    "href": "publications/just-wrong-or-just-weird/index.html#method-2",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Method",
    "text": "Method\n\nParticipants and design\nStudy 3 was a frequency based attempted replication of McHugh et al. (2017). The aim of Study 3 was to identify if dumbfounded responding could be evoked in a mixed sample of participants from a selection of non-WEIRD countries, primarily North Africa and the Middle East.\nAn initial sample of four-hundred-and-sixty-three participants were recruited for Study 3. Some participants did not provide full responses for all four scenarios (the total number of participants who completed the Critial Slide for all four scenarios was n = 203. In removing participants with missing data, we retained all participants who completed the Critical Slide for at least one scenario. Following this, we were left with a total sample of N = 282 (171 female, 103 male, 3 other, 5 declined to report their gender; Mage = 27.71, min = 18, max = 68, SD = 12.46).\n\n\n(#tab:tabcountries)\n\n\nParticipants by Country\n\n\n\n\nCountry\nFrequency\n\n\n\n\n1\n1\n\n\nAlgeria\n2\n\n\nBahrain\n5\n\n\nBangladesh\n2\n\n\nEgypt\n25\n\n\nindia\n1\n\n\nIndia\n25\n\n\nindiam\n1\n\n\nIndiSan\n1\n\n\nIran\n2\n\n\nIraq\n13\n\n\nIsrael\n1\n\n\nJordan\n10\n\n\nKuwait\n5\n\n\nLebanon\n34\n\n\nLibya\n14\n\n\nMorocco\n1\n\n\nOman\n1\n\n\nPakistan\n8\n\n\nPalestine\n14\n\n\nPhilippines\n13\n\n\nSaudi Arabia\n1\n\n\nSouth Africa\n1\n\n\nSri Lanka\n4\n\n\nSri Lankan\n1\n\n\nSudan\n33\n\n\nSyria\n30\n\n\nUAE\n21\n\n\nYemen\n1\n\n\n\n\nOur target sample was participants from non-WEIRD countries. As such we removed 11 participants who reported being from the UK (n = 3), USA (n = 1), Canada (n = 2), Germany (n = 1), Portugal (n = 1), Netherlands (n = 1), and participants who did not provide a country of origin (n = 2). This left a total sample of N = 271 (165 female, 98 male, 3 other, 5 declined to report their gender; Mage = 27.75, min = 18, max = 68, SD = 12.32). The breakdown of participants’ nationalities is displayed in Table @ref(tab:tabcountries). The breakdown of participants’ religions is as follows, Islam: n = 176, Christianity: n = 49, Hinduism: n = 10, other: n = 21, and 15 participants declined to provide their religion.\n\n\nProcedure and materials\nThe procedure for Study 3 was largely the same as Study 2, with some key changes. Data collection was conducted in collaboration with the Middlesex University Dubai, and participants were recruited through opportunity and snowball sampling by undergraduate students in the University. Given the potentially sensitive and offensive nature of some of the traditional dumbfounding scenarios (Incest and Cannibal), we replaced these scenarios with scenarios less likely to cause offence: Promise and Dog (see Appendix A).\nThe survey was programmed and presented using Qualtrics. The demographic information recorded additionally included participants’ nationality. We also included a filter question in an attempt to limit participation to participants from non-WEIRD countries. As in Study 2, we also included the meaning in life questionnaire (MLQ: Steger et al. 2008) and ICS (Renzhi et al. 2013). The entire study lasted twenty to twenty-five minutes."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#results-and-discussion-2",
    "href": "publications/just-wrong-or-just-weird/index.html#results-and-discussion-2",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nJudgements of the scenarios\nThe mean initial ratings for each scenario were as follows: MHeinz = 2.68, SDHeinz = 1.78; MDog = 2.15, SDDog = 1.76; MPromise = 3.88, SDPromise = 1.65; MTrolley = 2.61, SDTrolley = 1.88. The mean revised ratings for each scenario are as follows: MHeinz = 2.84, SDHeinz = 1.74; MDog = 2.21, SDDog = 1.79; MPromise = 4.37, SDPromise = 1.65; MTrolley = 2.86, SDTrolley = 1.87. The proportion of wrong, neutral, and ok, judgements for each scenario are displayed in Table @ref(tab:tab2judge).\nA paired samples t-test revealed no differences in the ratings of behaviours from time one to time two for Heinz, t(447.9965543) = -0.979, p = .328, d = 0.0922583; Dog, t(454.4625702) = -0.367, p = .714, d = 0.0342957; or Trolley, t(452.9942137) = -1.456, p = .146, d = 0.1365021. In contrast, participants revised ratings of Promise (M = 4.37, SD = 1.65) were significantly more favourable than their initial ratings M = 3.88, SD = 1.65, t(478.8640634) = -3.276, p = .001, d = 0.2987195\nA one-way ANOVA revealed significant differences in initial judgements depending on scenario, F(3, ,, , 927) = 41.37, p &lt; .001, partial \\(\\eta\\)2 = .118. Tukey’s post-hoc pairwise comparison revealed that judgements of Promise were significantly more favourable than all other scenarios: Heinz, p &lt; .001, Dog, p &lt; .001. Trolley, p &lt; .001; Heinz was rated significantly more favourably than Dog, = .009 there were no significant differences in the ratings of the other scenarios, Heinz/Trolley, p = .976, Dog/Trolley, p = .030.\nA one-way ANOVA revealed a similar pattern of differences in revised judgements depending on scenario, F(3, ,, , 908) = 63.46, p &lt; .001, partial \\(\\eta\\)2 = .173. Again, Tukey’s post-hoc pairwise comparison revealed that judgements of Promise were significantly more favourable than all other scenarios: Heinz, p &lt; .001, Dog, p &lt; .001. Trolley, p &lt; .001; and judgements of Dog were significantly more harsh than both Heinz, p = .001 and Trolley, p &lt; .001; all there were no significant differences in ratings of Heinz and Trolley, p = .999.\n\n\nMeasuring dumbfounding\nParticipants who selected the admission of not having reasons were identified as dumbfounded. Across the four scenarios 109 participants (40.2214022%) provided a dumbfounded response at least once. Table @ref(tab:tab2dumb) and Figure @ref(fig:study3fig) show the number and percentage of participants who selected each response for each scenario. Rates of dumbfounded responding for each scenario in Study 2 were significantly greater than zero, Heinz: z = 5.68, p &lt; .001; Trolley: z = 7.36, p &lt; .001; Promise: z = 4.81, p &lt; .001; Dog: z = 6.68, p &lt; .001, thus providing evidence for moral dumbfounding in our MENA sample. To test for the possibility of order effects, we conducted a chi-square test for independence which revealed no significant differences in responses to the critical slide depending on when the scenario was presented, \\(\\chi\\)2(6, N = 271) = 3.92, p = .687.\n\n\n\n\n\nRates of each type of response for each scenario in the Mixed Sample\n\n\n\n\nA chi-square test for independence revealed significant differences in responses to the critical slide depending on which scenario was being discussed, \\(\\chi\\)2(6, N = 271) = 205.55, p &lt; .001. Table @ref(tab:tabchisq1b) shows the observed counts, expected counts and standardised residuals for each response for each scenario. For Heinz, Dog, and Trolley, people were significantly more likely to provide reasons than to select “there is nothing wrong”. In contrast, for Promise participants were more likely to select “there is nothing wrong” than to present as dumbfounded, or to present as dumbfounded or provide reasons. We note that this finding may have been confounded by the responses to Promise, however the result holds when Promise is excluded from the analysis, \\(\\chi\\)2(4, N = 271) = 11.48, p &lt; .001. Study 3 provided further evidence that dumbfounded responding can be elicited in a non-WEIRD sample. Furthermore, the use of alternative scenarios provide evidence that moral dumbfounding can be elicited by a broader range of scenarios than normally demonstrated in the existing literature.\n\n\n(#tab:tabchisq1b)\n\n\nObserved counts, expected counts, and standardised residuals for each response to the critical slide depending on Scenario\n\n\n\n\nResponse\n\nDog\nHeinz\nPromise\nTrolley\n\n\n\n\nNothing Wrong\nObserved count\n46\n60\n170\n38\n\n\n\nExpected count\n79\n76\n82\n77\n\n\n\nStandardised residuals\n-5.27**\n-2.61*\n13.95**\n-6.32**\n\n\nDumbfounded\nObserved count\n45\n33\n22\n56\n\n\n\nExpected count\n39\n38\n41\n38\n\n\n\nStandardised residuals\n1.18\n-0.99\n-3.74**\n3.61**\n\n\nReason\nObserved count\n140\n130\n48\n132\n\n\n\nExpected count\n113\n109\n117\n111\n\n\n\nStandardised residuals\n4.11**\n3.22*\n-10.42**\n3.29*\n\n\n\n\nNote. * = sig. at &lt; .05; ** = sig. at &lt; .001\n\n \n\n\n\nIndividual differences\nA hierarchical linear regression was conducted to test the possible relationship between ICS (Renzhi et al. 2013), MLQ (Steger et al. 2008), and susceptibility to dumbfounding. As in Studies 1 and 2, susceptibility to dumbfounding was operationalized by calculating the number of times a participant provided a dumbfounded response. With this measure as the outcome variable, we included the four sub-scales of ICS, along with both sub-scales of the MLQ, as predictor variables in a multinomial logistic regression model. The overall model was not a significant predictor of susceptibility to dumbfounding \\(R^2 = .06\\), \\(F(6, 179) = 2.03\\), \\(p = .064\\) in Study 3.\nWe conducted a series of logistic regressions to investigate the possible relationship between ICS (Renzhi et al. 2013) and MLQ (Steger et al. 2008), and responding to each of the scenarios individually. Response to the critical slide scenario was the dependent variable for each scenario, and the four sub-scales of the ICS, along with the two sub-scales of the MLQ were included as predictor variables.\nThe overall model predicted responses for the Heinz dilemma, \\(\\chi\\)2(12, N = 205) = 22.7, p = .030, the observed power was 0.91. neither did the model significantly predict responses for the Trolley scenario, \\(\\chi\\)2(12, N = 205) = 13.45, p = .337, the observed power was 0.66. The overall model explained between 6.94% (Cox and Snell R square) and 10.25% (Nadelkerke R squared) of the variance in responses to the critical slide. The only significant predictors in the model were Vertical Individualism, and Meaning in Life: Search. As VI increased, participants were significantly more likely to select “there is nothing wrong” than to provide reasons for their judgement, Wald = 6.3, p = .377, odds ratio = 0.9733865, 95% CI [1.0151915, 1.1302198]. As Meaning in Life: Search increased, participants were significantly more likely to present as dumbfounded than to provide reasons, Wald = 5.23, p = .638, odds ratio = 1.0216129, 95% CI [1.0116615, 1.1621541].\nThe overall models did not significantly predict responses for any of the other scenarios: Trolley, \\(\\chi\\)2(12, N = 195) = 13.45, p = .337, the observed power was 0.66; Promise, \\(\\chi\\)2(12, N = 195) = 18.27, p = .108, the observed power was 0.83; Dog, \\(\\chi\\)2(12, N = 195) = 9.43, p = .666, the observed power was 0.48."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#individual-differences-and-dumbfounded-responding",
    "href": "publications/just-wrong-or-just-weird/index.html#individual-differences-and-dumbfounded-responding",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Individual Differences and Dumbfounded Responding",
    "text": "Individual Differences and Dumbfounded Responding\nIn addition to testing for the existence of moral dumbfounding in non-WEIRD samples, we investigated the possible relationship between dumbfounded responding and ICS (Renzhi et al. 2013: Studies 1 and 2) and MLQ (Steger et al. 2008: Study 2). Study 1 revealed no significant relationship between ICS and (a) overall susceptibility to dumbfounding, or (b) responses for each scenario individually.\nIn Study 2 we found that Vertical Individualism (Renzhi et al. 2013) was related to susceptibility to dumbfounded responding, with those scoring high in Vertical Individualism, being less likely to present as dumbfounded. It is possible that this observed relationship emerged as a result of participants’ relative motivations to do well in the task of providing reasons. Previous research (McHugh et al. 2017) provides suggestive evidence that a dumbfounded response is aversive, that people are motivated to appear consistent. This consistency can be successfully achieved by providing a reason for a judgement, or by revising a judgement and selecting “there is nothing wrong”. In contrast, providing a dumbfounded response may be seen as failing to present as consistent. The items in the Vertical Individualism sub-scale appear to provide a measure of people’s motivations to do well in relation to others (e.g., “It is important that I do my job better than others”; “When another person does better than I do, I get tense and aroused”). However, it is possible that this sub-scale additionally provides an indication of people’s motivations for success (e.g., “Winning is everything”). As such people who are more motivated to “succeed” in general, may be more motivated to avoid the perceived failure associated with a dumbfounded response. This interpretation is merely speculative, and follow up studies should investigate this in more detail.\nIn Study 2 we also found evidence that responses to specific scenarios were related to to the individual difference variables measured. Responses to the Incest scenario were predicted by Horizontal Individualism, and Vertical Collectivism (Renzhi et al. 2013). It is possible that this relationship emerges as a result of the content of the scenario rather than providing an insight into the cognitive processes involved in moral dumbfounding. Participants scoring higher in Horizontal Individualism were more likely to select “There is nothing wrong” than to provide a reason for their judgement. It appears that HI is linked with the valence of participants’ judgements of Incest rather than whether or not it leads to dumbfounding. A key consideration in the Incest scenario is the importance of individual autonomy. Similarly, the items in the HI sub-scale appear to relate to individual autonomy (e.g., “I rely on myself most of the time; I rarely rely on others,” “I often do ‘my own thing’,” Renzhi et al. 2013). As such it is not surprising that participants who score highly on HI, place higher value on individual autonomy when considering the Incest scenario.\nFurthermore, three of the four Vertical Collectivism (Renzhi et al. 2013) items specifically relate to the importance of the family. thus providing an indirect measure of the degree to which people value the family unit. Participants who scored higher in VC were more likely to present as dumbfounded than to provide reasons for their judgement of the Incest scenario. It is likely that participants who score higher in VC regard the family as particularly important, and the Incest scenario presents an affront to the family. As such, these participants may perceive the actions of Julile and Mark as a threat to something that they value, but may not be able to articulate this as a reason for their judgement (e.g., it is too abstract, or they do not think it is an “acceptable” reason).\nFinally, Meaning in Life: Presence (Steger et al. 2008), was related to responses to the Cannibal scenario, such that participants who scored higher in Meaning in Life: Presence were more likely to provide reasons for their judgements then to present as dumbfounded by Cannibal. Again this is likely due to the specific content, rather than informing the cognitive processes involved. It could be argued that the Cannibal scenario involves considerations about the value of life. Participants who score higher in Meaning in Life: Presence, have given consideration to the meaning (and by extension, value) of life (e.g., “I understand my life’s meaning”). It seems that this reflection life’s meaning may provide people with the necessary justifications/arguments/resources to articulate why they disapprove of Cannibal.\nStudy 3 did not find any relationship between susceptibility to dumbfounded responding and either ICS (Renzhi et al. 2013) or MLQ (Steger et al. 2008). We did find that Vertical Individualilsm predicted selecting “There is nothing wrong” for the Heinz dilemma. The content of the scenario may provide an explanation for this observed relationship. Participants were judging the behaviour of the Druggist, who charged an extremely high price for the drug he developed. The druggest’s behaviour is consistent with individualistic values, and it is not surprising that participants who score higher on this individualism measure endorse the behaviour of the druggist. Study 3 also found that participants who scored higher in Meaning in Life: Search were more likely to be dumbfounded than to provide reasons for their judgement."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#limitations-and-future-directions",
    "href": "publications/just-wrong-or-just-weird/index.html#limitations-and-future-directions",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Limitations and Future directions",
    "text": "Limitations and Future directions\nA key limitation in the current studies is the sample make up. Participants in Study 1 were recruited through a Chinese university, participants in Study 2 were university graduates, who also were proficient in English, and participants in Study 3 were also proficient in English, and recruited through snowball sampling in a University setting. This means that none of our samples are representative of their respective population. Furthermore, the samples involved either university graduates, or those currently studying in a university setting, and as such this high level of education is a significant challenge to our stated aim of recruiting from non-WEIRD samples.\nThe moral foreign language effect (Cipolletti, McFarlane, and Weissglass 2016) means that the use of an English language survey in Studies 2 and 3 is another potential limitation. Previous research has shown that people appear to make more utilitarian judgements when moral scenarios are presented in another language (Costa et al. 2014). In the case of the Intuition scenarios, this could potentially lead to a higher number of participants selecting “There is nothing wrong”, rather than presenting as dumbfounded. Despite this potential confound, dumbfounded responding was observed for all scenarios in Studies 2 and 3.\nOur studies were not intended as a systematic investigation of cultural differences in evaluation of specific moral content (there are other research programmes dedicated to this, e.g., Haidt and Joseph 2008; Shweder et al. 1997; Narvaez 2016). Here, we applied existing methods in three novel contexts, to assess whether or not moral dumbfounding can be elicited in these under-studied contexts. We found evidence for moral dumbfounding in a Chinese sample, an Indian sample and a mixed sample. Our results provided some evidence for cultural variation (e.g., the relative importance of the death taboo), that may inform the development of future research programmes."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#footnotes",
    "href": "publications/just-wrong-or-just-weird/index.html#footnotes",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne scenario (Incest) depicted an act of consensual sibling incest, using contraception; another (Cannibal) described an act of cannibalism involving a laboratory cadaver due to cremated.↩︎\nHeinz could not afford drugs priced at 10 times the cost price, and steals drugs to save his wife’s life.↩︎\nParticipants do not articulate or consistently apply harm-based reasons/principles, nor do they articulate norm-based reasons/principles with sufficient consistency to provide evidence that these reasons are guiding their judgements in the dumbfounding paradigm (see McHugh et al. 2020).↩︎\nWe note that, although not statistically significant, the Cannibal scenario appeared to elicit more dumbfounded responding in the Chinese sample↩︎"
  },
  {
    "objectID": "publications/moral-identity-and-covid-19/index.html",
    "href": "publications/moral-identity-and-covid-19/index.html",
    "title": "Moral Identity Predicts Adherence to COVID-19 Mitigation Procedures Depending on Political Ideology:",
    "section": "",
    "text": "Reducing the spread of infectious viruses (e.g., COVID-19) can depend on societal compliance with effective mitigations. Identifying factors that influence adherence can inform public policy. In many cases, public health messaging has become highly moralized, focusing on the need to act for the greater good. In such contexts, a person’s moral identity may influence behavior and serve to increase compliance through different mechanisms: if a person sees compliance as the right thing to do (internalization) and/or if a person perceives compliance as something others will notice as the right thing to do (symbolization). We argue that in societies that are more politically polarized, people’s political ideology may interact with their moral identity to predict compliance. We hypothesized that where polarization is high (e.g., USA), moral identity should positively predict compliance for liberals to a greater extent than for conservatives. However, this effect would not occur where polarization is low (e.g., New Zealand). Moral identity, political ideology, and support for three different COVID-19 mitigation measures were assessed in both nations (N = 1,980). Results show that while moral identity can influence compliance, the political context of the nation must also be taken into account.\n\n\nPDF\n\n\nSource Document\n\n\n\n\n\nBibliography\n\n\n\n  McHugh, C., Griffin, S. M., McGrath, M. J., Rhee, J. J., Maher, P. J., McCashin, D., & Roth, J. (2023). Moral Identity Predicts Adherence to COVID-19 Mitigation Procedures Depending on Political Ideology: A Comparison Between the USA and New Zealand. Political Psychology, 44(2), 337–360. https://doi.org/10.1111/pops.12838"
  },
  {
    "objectID": "publications/moral-identity-and-covid-19/index.html#abstract",
    "href": "publications/moral-identity-and-covid-19/index.html#abstract",
    "title": "Moral Identity Predicts Adherence to COVID-19 Mitigation Procedures Depending on Political Ideology:",
    "section": "",
    "text": "Reducing the spread of infectious viruses (e.g., COVID-19) can depend on societal compliance with effective mitigations. Identifying factors that influence adherence can inform public policy. In many cases, public health messaging has become highly moralized, focusing on the need to act for the greater good. In such contexts, a person’s moral identity may influence behavior and serve to increase compliance through different mechanisms: if a person sees compliance as the right thing to do (internalization) and/or if a person perceives compliance as something others will notice as the right thing to do (symbolization). We argue that in societies that are more politically polarized, people’s political ideology may interact with their moral identity to predict compliance. We hypothesized that where polarization is high (e.g., USA), moral identity should positively predict compliance for liberals to a greater extent than for conservatives. However, this effect would not occur where polarization is low (e.g., New Zealand). Moral identity, political ideology, and support for three different COVID-19 mitigation measures were assessed in both nations (N = 1,980). Results show that while moral identity can influence compliance, the political context of the nation must also be taken into account.\n\n\nPDF\n\n\nSource Document\n\n\n\n\n\nBibliography\n\n\n\n  McHugh, C., Griffin, S. M., McGrath, M. J., Rhee, J. J., Maher, P. J., McCashin, D., & Roth, J. (2023). Moral Identity Predicts Adherence to COVID-19 Mitigation Procedures Depending on Political Ideology: A Comparison Between the USA and New Zealand. Political Psychology, 44(2), 337–360. https://doi.org/10.1111/pops.12838"
  },
  {
    "objectID": "publications/moral-identity-and-covid-19/index.html#highlights",
    "href": "publications/moral-identity-and-covid-19/index.html#highlights",
    "title": "Moral Identity Predicts Adherence to COVID-19 Mitigation Procedures Depending on Political Ideology:",
    "section": "Highlights",
    "text": "Highlights\n\nPeople’s moral identity is related to their attitudes toward, and adherence to, COVID-19 mitigation measures.\nIn a context where polarization is high (USA), this relationship is moderated by political ideology; the relationship between moral identity and support for COVID-19 mitigation measures is observed for liberals, but it is less strong (or even reversed) for conservatives.\nIn a context where polarization is lower (New Zealand), this relationship between moral identity and support for COVID-19 mitigation measures is not moderated by political ideology.\nThe results point to the relevance of the national sociopolitical context in shaping how public health messaging advice is received and acted upon, and a consideration of this context may be important in future messaging on societal issues."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html",
    "href": "publications/moral-judgment-as-categorization/index.html",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "",
    "text": "Observed variability and complexity of judgments of ‘right’ and ‘wrong’ cannot be readily accounted for within extant approaches to understanding moral judgment. In response to this challenge, we present a novel perspective on categorization in moral judgment. Moral judgment as categorization (MJAC) incorporates principles of category formation research while addressing key challenges of existing approaches to moral judgment. People develop skills in making context-relevant categorizations. They learn that various objects (events, behaviors, people, etc.) can be categorized as morally ‘right’ or ‘wrong’. Repetition and rehearsal results in reliable, habitualized categorizations. According to this skill formation account of moral categorization, the learning and the habitualization of the forming of moral categories, occurs within goal-directed activity that is sensitive to various contextual influences. By allowing for the complexity of moral judgments, MJAC offers greater explanatory power than existing approaches while also providing opportunities for a diverse range of new research questions."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#type-token-interpretation",
    "href": "publications/moral-judgment-as-categorization/index.html#type-token-interpretation",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Type-Token Interpretation",
    "text": "Type-Token Interpretation\nBarsalou (2003; 1999) proposes that the learning and maintaining of categorizations occurs through the process of type-token interpretation, defined as the binding of specific tokens (category members) to general types (category). For the category THINGS TO PACK INTO A SUITCASE (Barsalou 2003; Barsalou, Solomon, and Wu 1999), this entails identifying a given item (token) as something that you pack or do not pack into a suitcase (type). Crucially, this process can be implicit, simply involving treating an item as a member or not a member of a particular category within an appropriate context for action, in this case, packing it or not packing it. Skill in forming the categories emerges from repetition and rehearsal of the type-token interpretation; people become skilled at deploying categories that they encounter frequently."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#context-sensitivity",
    "href": "publications/moral-judgment-as-categorization/index.html#context-sensitivity",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Context Sensitivity",
    "text": "Context Sensitivity\nType-token interpretation occurs every time a given token is encountered such that every categorization of a given token (object/item/event) is subject to contextual influences of the current situation. This results in dynamic and complex categories, without necessary and sufficient conditions, and without even stable best exemplars or prototypes. The properties of an object relevant to that particular context become salient, and the categorization process is accented by the details of the particular circumstances in which the actions are being taken. Stable or recurring properties (both object and contextual) can be learned, and their identification or recognition become a part of the subsequent engagement in the relevant goal-directed activity and the enactment of different relevant type-token interpretations of objects. This is dependent on the experience and learning history of the individual and not inherent in the categories themselves, however, which is what gives rise to the complex, dynamic aspects of concepts central to Barsalou’s approach.\nConsider a study by Barsalou (1982). Participants were presented with a series of sentences involving particular items. For example: “The basketball was used when the boat sank”; or “The basketball was well worn from much use” (Barsalou 1982; see also Barsalou 2003). Following each sentence, participants were asked to verify whether particular properties were true for the item; for example whether or not “floats” is true for “basketball” following reading either of the above sentences. The fact that basketballs float is relevant to the first sentence, and thus this property is inferred from reading this sentence. In the second sentence, this property (while still true for basketball) is irrelevant and does not become salient by reading the sentence. Thus, while what is true for basketball does not change depending in the situation, the properties that are inferred in a given instance do. This is evident in that participants were faster at verifying “floats” as true for basketball following reading the first sentence than the second (Barsalou 1982, 2003). Other studies have yielded similar results, demonstrating that different sentences cause different properties to become salient depending on these properties’ relevance to the given sentence (Greenspan 1986; Tabossi 1988; Yeh and Barsalou 2006). The contextually relevant inferences made when we encounter category members are not limited to object properties but can also include situational and introspective inferences (e.g., Barsalou and Wiemer-Hastings 2005)."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#habitualization",
    "href": "publications/moral-judgment-as-categorization/index.html#habitualization",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Habitualization",
    "text": "Habitualization\nHighly skilled categorizations become habitualized (automatic/intuitive) to the point where these categorizations provide an illusion of “stable categories”. Typically, these “stable categories” mirror real-world categories or classes and social norms that are frequently and reliably encountered in day-to-day life. This reflects the use of these categories in (a) interacting effectively with the world and (b) communicating with others. Natural kinds and social norms would constitute prototypical classes of such frequently encountered and reliably implemented categories (e.g., Keil, Rozenblit, and Mills 2004). In some cases, categories that can be referenced to natural kinds may take on the causal rules that distinguish natural kinds. For example, fruit is distinct from vegetables in that the agreed scientific classification of fruit (in our culture) is as containing the seeds. This causal rule is not necessarily operationalized in everyday interactions with fruit and vegetables; however, in certain situations, it may be referenced to aid in the classification of ambiguous items.\nMore abstract categories are more difficult to define because there may not be a set of causal rules governing membership to draw on. There is a large body of literature documenting the search for causal rules or identifying characteristics of particular emotion categories, for instance, but no approach has fully answered this question (Griffiths 1997; see also Barrett, Wilson-Mendenhall, and Barsalou 2014; Mesquita, Barrett, and Smith 2010).\nBarsalou and Wiemer-Hastings (2005) directly address this question of abstract concepts, demonstrating that the content of increasingly abstract concepts contains increasingly situational and introspective focus. Consider the possible inferences associated with the categorization of SOFA versus FREEDOM. Various properties of SOFA will remain relatively stable across contexts. However, to make sense, any conceptualization of FREEDOM needs to be embedded in a specific situational (e.g., freedom from oppression) or introspective (e.g., feeling free) context. Inferences regarding FREEDOM are necessarily more context-dependent. This results in greater situational or introspective inferences being made for abstract categories, while concrete categories allow for more object-level inferences.\nThe abstract nature of moral categories means they are similarly rich in situational and introspective inferences. That is, whether a particular behavior is viewed as right or wrong varies depending on the situation and may be categorized as right or wrong in different ways, specific to the context and the goal-directed activity in which the person is engaged. The link of introspection and the abstract nature of moral categories is supported by recent approaches that stress the tight coupling of moral judgments and emotions (e.g., Cameron, Payne, and Doris 2013; Huebner, Dwyer, and Hauser 2009; Royzman et al. 2014; Rozin et al. 1999; Valdesolo and DeSteno 2006).\nAs with the mapping of habitualized categorizations on to real-world natural kinds, moral categories may appear to follow principles or rules, reflecting social norms of society or a specific social group. A behavior that is encountered frequently and consistently identified as MORALLY RIGHT, may emerge as a “good example”, or a Token2 for MORALLY RIGHT. Over time, people develop a range of Tokens for the categories MORALLY RIGHT (and for MORALLY WRONG). Furthermore, similar behaviors may become categorized together, for example, continued identification of “hitting people” as WRONG, and “kicking people” as WRONG may lead a person to form a superordinate category CAUSING HARM TO PEOPLE, which is consistently identified as WRONG. This may then be taken a step further, and “don’t harm people” and “don’t harm animals” may merge to form INFLICTING HARM, which is consistently identified as WRONG.\nThe emergence of habitualized, highly generalized, morally grounded Tokens may form the basis of what we call values. Furthermore, as more and more Tokens are developed and become increasingly generalized, these generalized Tokens become arranged hierarchically in terms of severity. This essentially becomes our “moral code”. There is not necessarily an underlying set of rules (or moral principles) governing this moral code, it is based on a large collection of Tokens, and a process of categorization that is sensitive to context and on-going actions. Some of the generalized Tokens (values) may appear to exhibit sufficient powers of “governance” to constitute rules. However, these are not true rules; as with the mapping of stable categorizations onto natural kinds, it may be possible to construct plausible (and often true) causes for the associations that define many categories, however, the process of categorization remains grounded in type-token interpretation (rather than the rules that can be inferred from referencing observable categories, Barsalou 2003; Barsalou and Wiemer-Hastings 2005). MJAC provides a framework for the emergence of what appears to be relative stability in categorization while simultaneously accounting for the observed variability and context-dependency that pose a challenge to existing theories of moral judgment."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#moral-dumbfounding",
    "href": "publications/moral-judgment-as-categorization/index.html#moral-dumbfounding",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Moral Dumbfounding",
    "text": "Moral Dumbfounding\nThe processes underlying moral judgment, according to MJAC, predict the phenomenon of moral dumbfounding. Moral dumbfounding occurs when people defend a moral judgment even though they cannot provide a reason to support it (Haidt 2001; Haidt, Björklund, and Murphy 2000; McHugh et al. 2017). Typically, moral dumbfounding occurs for harmless taboo behaviors (consensual incest, cannibalism involving a body that is already dead). Consider the learning of taboo behaviors as wrong through type-token interpretation and typical interaction with such behavior. The taboo nature of these topics means that they are consistently identified as morally wrong, without much discussion [the Scottish public petitions committee notably dismissed a call to legalize incest with no discussion at all; see Sim (2016)]. This leads to a high degree of stability in categorizing them as WRONG. However, while other behaviors may be discussed or disputed, generating a deeper knowledge surrounding the rationale for identifying as right or wrong, the taboo nature of these behaviors prevents them from being discussed. This means that a typical encounter with such behavior involves little more than identifying it as wrong, possibly with an expression of disgust, and changing the subject (Sim 2016). Identifying causal rules that govern the behavior’s membership of the category MORALLY WRONG is likely problematic, in that a person would have limited experience at attempting to do so. In this view, type-token interpretation of taboo behaviors logically leads to moral dumbfounding.\nPhenomena similar to moral dumbfounding have been observed in the non-moral domain. While these have not been explicitly identified as “dumbfounding” we suggest that dumbfounding also occurs for categories other than MORALLY WRONG. For example, Boyd and Keil (Boyd 1989, 1991; Keil 1989; see also Griffiths 1997) found that participants struggled to explain their reasons for categorizing an imagined creature as A CAT or NOT A CAT. Descriptions of participants’ responding in such situations bear a striking similarity, whether the target categorization is in the moral domain or not. In discussing their work on the illusion of explanatory depth, Keil, Rozenblit, and Mills (2004) describe the sensation of being “surprised by our inability to explain something” (2004, 277). Similarly, in discussing moral dumbfounding, Haidt describes how people “express surprise at their inability to find supporting reasons” (Haidt 2001, 817). The illusion of explanatory depth and moral dumbfounding are likely phenomena with common underpinnings."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#categorizing-people-versus-categorizing-actions",
    "href": "publications/moral-judgment-as-categorization/index.html#categorizing-people-versus-categorizing-actions",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Categorizing people versus categorizing actions",
    "text": "Categorizing people versus categorizing actions\nIn line with Barsalou and Wiemer-Hastings (2005), we have been describing the cognitive processes in relation to the development of the abstract categories MORALLY WRONG and MORALLY RIGHT. In reality, people do not deal with these abstractions, rather moral categorization is situated in specific contexts, occurring as part of goal-directed behavior. In some situations, we may identify specific actions as morally questionable or morally praiseworthy, while in others, we may identify specific actors as morally questionable or morally praiseworthy. While, the action or actor may belong to the super-ordinate category MORALLY WRONG, or MORALLY RIGHT (or NOT MORALLY RELEVANT), it is likely that in everyday interactions people are more concerned with the subordinate categories in question, for example, BAD/GOOD PERSON or BAD/GOOD ACTION.\nPrevious authors have argued that when people make moral judgments, the primary evaluation is of the character of the person committing the act (e.g., Uhlmann, Pizarro, and Diermeier 2015; Landy and Uhlmann 2018; see also Siegel, Crockett, and Dolan 2017; Siegel et al. 2018). MJAC does not adopt this position, rather we recognize that there are many potential contextual factors that influence whether the target of any given moral categorization is the actor or on the action (or both). The variability relating to the target of moral categorization can influence which super-ordinate category is eventually implicated, that is, whether the final judgment is MORALLY WRONG, or MORALLY RIGHT (or NOT MORALLY RELEVANT); for example, if a corrupt politician helps a neighbor with shopping, even though this action may be categorized as good, the actor is likely to still be categorized as a bad."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#moral-categorization-involving-known-others",
    "href": "publications/moral-judgment-as-categorization/index.html#moral-categorization-involving-known-others",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Moral Categorization Involving Known Others",
    "text": "Moral Categorization Involving Known Others\nMJAC assumes that moral categorization is dynamic and context-dependent. We propose that consideration of the goal-directed nature of moral categorizations provides a key insight into some of the contexts that may affect the target of a given categorization. Consider the following two scenarios:\n\nyou find out that a colleague has been fired for stealing from your employer - they have been bringing home office equipment for their own personal use, and they have been exaggerating their expense claims;\na close friend of yours reveals to you that they have been stealing from their employer - they have been bringing home office equipment for their own personal use, and they have been exaggerating their expense claims.\n\nIt seems intuitive that people should judge (b) differently from (a), and we predict that people will be more lenient in their judgments of (b) than of (a). Despite the historical paucity of research investigating the influence of the relationship between the person making a judgment and the apparent perpetrator Feltz and May (2017), recent findings support this prediction (Forbes 2018; Heiphetz and Craig 2020; Hofmann et al. 2014; Lee and Holyoak 2020; McManus, Kleiman-Weiner, and Young 2020; Weidman et al. 2020). Several studies have demonstrated that people appear to be more lenient in their judgments of people they are close to versus strangers (Forbes 2018; Hofmann et al. 2014; Lee and Holyoak 2020; Weidman et al. 2020). Further evidence that close others are judged differently to strangers has been found by Heiphetz and Craig (2020). They showed that a tendency to dehumanize racists (and sexists) is associated with a greater tendency to view strangers’ ambiguous actions as racially biased (or sexist), but not the ambiguous actions of friends (Heiphetz and Craig 2020). The importance of accounting for possible relationships in moral judgment research is not limited to the relationship between the observer and the relevant actors. Recent work has shown that people are judged more favorably for helping strangers than helping kin, while a failure to help kin is judged more harshly, suggesting a stronger obligation towards kin than towards strangers (McManus, Kleiman-Weiner, and Young 2020).\nA further prediction is that for (b), the target of categorization will be the action rather than the actor. People are motivated to see close others positively (Forbes 2018; Murray, Holmes, and Griffin 1996a, 1996b). If faced with a situation where a close other committed a moral transgression, people would be motivated to avoid making a negative judgment of the person (Ditto, Pizarro, and Tannenbaum 2009; Murray, Holmes, and Griffin 1996a, 1996b; Proulx and Inzlicht 2012). One way to avoid this is to make the target of the categorization the action rather than the actor.3\nIn making the action the target of the categorization rather than the actor, people can reduce the degree to which they view their close others negatively. However, this strategy is implemented in addition to making judgments that are more lenient. Making more lenient judgments about specific transgressions based on the actor introduces context-specific inconsistency in regard to the categorization of that transgression. MJAC predicts that this inconsistency may threaten the long term stability of the categorization. Specifically, we predict that leniency towards close others for a specific behavior should eventually lead to more general leniency towards that behavior. This development of more general leniency should occur independently of deliberate attempts to present as consistent (although it could be accelerated by attempts to be consistent). For instance, an increased tolerance of “locker room talk” by people who would otherwise disapprove of sexism."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#moral-categorization-involving-unknown-others",
    "href": "publications/moral-judgment-as-categorization/index.html#moral-categorization-involving-unknown-others",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Moral Categorization Involving Unknown Others",
    "text": "Moral Categorization Involving Unknown Others\nDrawing on the goal-directed nature of moral categorization, we further predict any prospective relationships between the observer and the actor. Success in social interactions involves successfully predicting the actions of others (Waytz and Young 2018). As such, a key goal of moral categorization is to distinguish “good” from “bad” people (Uhlmann, Pizarro, and Diermeier 2015), by attempting to identify a person’s moral “essence” (e.g., Dunlea and Heiphetz 2020; Heiphetz and Dunlea 2019), or “character” (N. Klein and O’Brien 2016; Siegel, Crockett, and Dolan 2017; Siegel et al. 2018). This enables people to establish relationships or pursue continued interactions with “good” people, and to limit their interactions with “bad” people (or at least treat interactions with “bad” people with caution).\nThus, evaluations of strangers’ actions should show a bias for categorizing the actor rather than the action. Furthermore, this bias should be more pronounced in situations when people anticipate that there may be follow-up interactions with the stranger. Research on reciprocity and repeated interactions with strangers or partners (e.g., Fehr and Gächter 2000, 2003) provides an ideal framework that could be adapted to test this prediction. In conditions where participants are partnered, their initial evaluations should be more focused on their partner’s character than in conditions where participants interact with a new “stranger” for each trial.\nDrawing on the well-established tendency for negative information to be weighted more heavily than positive information (e.g., Kahneman and Tversky 1979; Rozin and Royzman 2001; A. Smith 1759), we predict that people will be more sensitive to negative actions than positive actions. Indeed, this has been shown to be the case. N. Klein and O’Brien (2016) presented participants with vignettes describing changes in patterns of behavior. Participants were asked to indicate how many consecutive instances of the new behavior would need to occur to convince them that the actor’s “moral character had transformed” (N. Klein and O’Brien 2016, 152). Participants perceived negative transformations much quicker than positive transformations, which was true for commencing negative behaviors and ceasing positive behaviors (N. Klein and O’Brien 2016). A general heightened sensitivity to negative information means that people appear to be quicker to categorize an actor as “bad” (vs. “good”).\nThis identification of “bad” actors appears to be present from an early age, such that even pre-verbal infants show a preference for good actors over bad actors (Hamlin, Wynn, and Bloom 2007, 2010; Hamlin and Wynn 2011; cf. Margoni and Surian 2018; Schlingloff, Csibra, and Tatone 2020; Steckler, Woo, and Hamlin 2017). We do not claim that infants in these studies have acquired fully developed categories of MORALLY WRONG and MORALLY RIGHT, and that they assign different actors to these categories. Rather, type-token interpretation predicts that category members should be treated as similar, independently of whether or not a person can describe the category, or even the relationship between the category members.4 Previous research has demonstrated that we implicitly treat similar items as similar even though we may not be able to articulate what makes them similar (e.g., recognising ‘good decks’ from ‘bad decks’ in the Iowa Gambling Task: Bechara and Damasio 2005; Damasio 1994; or implicit identification of abstract patterns, Proulx and Heine 2009; Whitson and Galinsky 2008).\nThese findings should not be interpreted as categorizations of “bad” actors are more stable than categorizations of “good” actors. Indeed, the opposite is the case (Siegel et al. 2018), where beliefs about “bad” agents are more volatile than beliefs about “good” agents. MJAC explains this volatility in the categorization of “bad” agents relative to “good” as emerging due to the relative consistency with which categorizations are made. As noted by Siegel et al., “bad people often behave morally, but good people rarely behave immorally” (2018, 750). The contexts in which actors are categorized as “good” are more consistent than the contexts in which they are categorized as “bad”. This consistency makes the categorization “good” actor a more stable categorization than “bad” actor. This apparent stability categorizing “good” actors relative to “bad” actors can also be seen in research on moral essentialism, people show a greater tendency to attribute essence based on moral goodness than moral badness (Heiphetz 2020; Newman, Bloom, and Knobe 2014).\nThe findings discussed above reflect the goal-directed nature of moral categorization. Specifically, people are motivated to understand and predict others’ actions to guide future interactions (Uhlmann, Pizarro, and Diermeier 2015; Waytz and Young 2018). If we understand that some behaviours are associated with positive experiences and some with negative outcomes, then it is not surprising that we show a preference for people who behave in a more positive way, even from a very young age (Hamlin and Wynn 2011).\nInterestingly, distinguishing between categorizing an action or categorizing an actor has implications for behavior, specifically when the actor in question is the self. In a series of studies by Bryan, Adams, and Monin (2013), participants took part in tasks in which cheating for financial gain (at the expense of the experimenter) was possible. When task instructions discouraging cheating used the term “cheater”, participants’ rates of cheating was significantly lower than when the term used was “cheating”. Committing an action that might fall into the category MORALLY WRONG is less aversive than being categorized as a BAD PERSON."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#beyond-unidimensional-conceptions-of-morality",
    "href": "publications/moral-judgment-as-categorization/index.html#beyond-unidimensional-conceptions-of-morality",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Beyond Unidimensional Conceptions of Morality",
    "text": "Beyond Unidimensional Conceptions of Morality\n\nDual-Process Theories of Moral Judgment\nThe three dual-processing theories of moral judgment that we address here each use some form of one-to-one mapping between a key dimension of moral judgment and the underlying differences in information processing expressed in that dual-processing account. Identification of the moral dimension is usually made through categorization of responses to challenges such as the trolley problem (the moral judgment literature is, unfortunately, replete with vehicular homicides).\nFor instance, Greene’s theory describes the distinction between deontological and consequentialist outcomes to moral judgments as a qualitative difference in processing, where deontological judgments are grounded in implicit, emotional, automatic processing, and consequentialist judgments involve deliberate, controlled processing (Greene 2016). Byrd and Conway’s (2019) softer approach is less dichotomous, such that deontological judgments are viewed as involving relatively more affective processing. For both Crockett’s (2013) and Cushman’s (2013) model-free vs. model-based accounts the logic is similar, though the emphasis is reversed. While for Greene (2016), and Byrd and Conway (2019), the form of processing drives the form of moral judgments, for both Cushman and Crockett, the framing of the moral task drives the kind of processing that is likely to result. Crockett and Cushman both avoid the simple deontological/consequentialist divide but focus instead on evaluating either moral actions or moral outcomes, which give rise to model-free or model-based judgments respectively. As with Greene and Byrd and Conway, however, they hold a stable one-to-one mapping between this dimension of the content of the moral judgment and the underlying processing.\nThe clarity of these mappings is appealing, but we argue here that the complexity and inconsistency of the findings in the existing literature on these relationships are disconfirming for these accounts (e.g., De Neys and Białek 2017; Gamez-Djokic and Molden 2016; Gubbins and Byrne 2014; Körner and Volk 2014; McPhetres et al. 2018; Pizarro and Bloom 2003; Reynolds and Conway 2018). We note that research on categorization also predicts reliably distinguishable patterns of response along the lines of many dual-processes accounts, distinguished by individual learning histories and experience in performing given categorizations in different circumstances. For clarity and consistency, we will refer to this distinction as one between habitual versus deliberative responses, positioned at either end of a continuum (Kruglanski and Gigerenzer 2011).\nWe follow the categorization research in identifying as a key dimension the extent to which specific categorizations (instances of type-token interpretations) are well-rehearsed and thus, become fluent, stable, and habitual within frequently enacted goal-directed activities (Barsalou 1999, 2003, 2017). Less experience with a particular type-token interpretation will result in less consistent deployment of the category and demand more deliberative consideration of the situation and appropriate action.\nTherefore, this key dimension in underlying processing is not predicted by MJAC to map straightforwardly onto any aspect of task content or framing in moral judgment, such as habitual judgments being deontological while deliberative ones are consequentialist. While well-worn deontic exhortations (“It’s wrong to hurt people”, “Thou shalt not kill”, “You shouldn’t hit your sister”) will no doubt develop a strong habitual foundation, within the MJAC framework, consequentialist judgments that are well-practiced will also be supported by habitual responses [associated with quick intuitive or affective reactions to moral judgments as studied by deneys_dual_2017; Gubbins and Byrne (2014); Reynolds and Conway (2018)]. Consequentialist reasoning, likely requiring explicit moral argument to arise, may be somewhat less commonly practiced, but also some deontological situations have novel characteristics that therefore also require deliberation [as illustrated by the likes of Gamez-Djokic and Molden (2016); Körner and Volk (2014); McPhetres et al. (2018); Pizarro and Bloom (2003)).\nThis variation in the relationship between deontological and consequentialist judgments and the ways (habitual vs. deliberative) they get made undermines both Greene’s and Byrd and Conway’s accounts. Neither Cushman (2013) nor Crockett (2013) connect the moral perspective with a specific form of processing. Still, they do map the distinction between action- and outcome-focused judgments onto the distinction between model-free and model-based processing. While this can accommodate such variability in deontological or utilitarian perspectives depending on circumstances, it runs afoul of what is termed the “doctrine of double effect” (Doris 2010; Mikhail 2000). The doctrine of double effect concerns the difference between causing harm as a means to an end being seen as different to causing harm as a side-effect of achieving the same ends, even when the actions taken are the same (e.g., Mikhail 2000; see also R. A. Klein et al. 2017). It is unclear what about such cases could trigger a difference in processing that would explain differential judgments for model theories. These theories are also challenged by versions of the trolley problem presented in virtual reality environments (Francis et al. 2016), where a usual pattern of responding (preference for inaction over pushing someone onto the track to stop the tram) was reversed. This runs directly counter to the predictions of the action-outcome mapping to form of processing made by these model theories. However, the shift to a more deliberative, calculating mode of thinking is perhaps less surprising for MJAC, given the novelty of the mode of presentation.\nAccording to MJAC, the making of moral judgments is dynamical, and context-dependent, and occurs as part of goal-directed activity; thus we should expect to see this observed variability that poses a challenge to any stable mapping between content and form of processing or judgment outcome. MJAC also assumes that relative stability in moral categorizations emerges as a result of continued and consistent type-token interpretation, such that particular categorizations become habitualized (and hence intuitive). Thus, we should expect a variety of contextual factors affecting people’s moral judgments, not limited to any single key dimension. Constraints on space mitigate against exploring each of these in detail. Still, the sheer range of such factors that have been reported offers compelling evidence that whatever underlies variation in moral judgment is a complex of issues and is not unidimensional in any given situation [the reader is referred to the wealth of literature examining such factors as emotional influences, Cameron, Payne, and Doris (2013); intentionality, evitability, benefit recipient, Christensen et al. (2014); Christensen and Gomila (2012); action-outcome distinction Crockett (2013); cushman_action_2013; trustworthiness and social evaluation Everett, Pizarro, and Crockett (2016); Everett et al. (2018); personal-impersonal distinction, Greene et al. (2001); doctrine of double effect, Mikhail (2000); level of physical contact, Valdesolo and DeSteno (2006); order effects, Wiegmann, Okan, and Nagel (2012)).\n\n\nTheory of Dyadic Morality\nThe theory of dyadic morality (TDM, Gray, Young, and Waytz 2012) that has recently been presented by Gray and colleagues would also seem to be grounded in generic categorization processes [Gray, Waytz, and Young (2012), p. 206; gray_mind_2012, p. 102; Schein and Gray (2018), p. 42]. As such, the approach is not heavily focused on a single processing dimension explaining moral judgment (or the variation therein). While TDM has not been identified with a specific theory of categorization, Gray et al. ((2012, 206) make reference to “prototypes or exemplar sets”, and it is here that the divergence with MJAC becomes clear. Barsalou (2003) summarized a range of findings indicating that neither prototype nor exemplar approaches can adequately explain the dynamic and variable nature of performance in categorization tasks.\nMore problematically, though TDM has been linked to exemplar and prototype theories, its proponents highlight moral situations as those involving a set of necessary and sufficient conditions - those which involve “an intentional agent causing damage to a vulnerable patient” (Schein and Gray 2018, 33), or “an intentional moral agent and a suffering moral patient (Gray, Young, and Waytz 2012, 101). Such appeals to essentialism are at odds with decades of research demonstrating dynamism and context-dependency in categorization (Barsalou 1982, 1987, 2003, 2017; Harman, Mason, and Sinnott-Armstrong 2010; McCloskey and Glucksberg 1978; Mervis and Rosch 1981; Oden 1977; Rosch 1975; Rosch and Mervis 1975; Stich 1993), and returns us to a unidimensional approach to moral judgment, this time identifying the moral character of a situation as the extent to which it involves harm. While intuitively appealing, this does not bear empirical scrutiny.\nProponents of TDM argue that even in ostensibly harmless moral transgressions, people perceive harm (Gray, Schein, and Ward 2014). This perception of harm guides participants’ judgments in moral dumbfounding scenarios (Schein and Gray 2018; Schein 2020). Dumbfounding is displayed when people maintain a moral judgment in the absence of a means of justifying their judgment, usually evoked by vignettes of supposedly “harmless wrongs” such as consensual incest or cannibalism of an already-dead body (Haidt, Björklund, and Murphy 2000; McHugh et al. 2017). Schein and Gray (2018) point to a series of studies by Royzman, Kim, and Leeman (2015), to support their appeal to perceived harm in the “moral dumbfounding” paradigm. royzman_curious_2015 investigating the case of consensual incest, included additional questions that appear to demonstrate that people’s judgments were (at least in part) grounded in perceptions of harm.\nHowever, more recent dumbfounding work fails to support the TDM perspective on this matter (McHugh et al. 2020). In addressing specific methodological limitations of the Royzman, Kim, and Leeman (2015) study, McHugh et al. (2020) found that people do not cite harm as a reason for their judgment. Participants were asked to judge a vignette describing consensual incest, asked to provide reasons for their judgment, and then provided with the questions examining perceptions of harm developed by Royzman, Kim, and Leeman (2015). The responses to the harm-based questions provided one measure of participants’ perceptions of harm, that is, did participants endorse a harm-based reason for their judgment when it was presented to them? Another measure of perceptions of harm was taken by coding the reasons provided for whether or not participants mentioned harm as justifying their judgment. Figure 1 presents a matrix plotting rows of participants’ judgments (wrong vs. not wrong) against columns of their endorsing of harm (left matrix), or whether or not they mentioned harm (right matrix) across three studies (N = 723).5 According to TDM, all participants should be located in either the top left (harm/wrong) or the bottom right (no harm/not wrong) quadrants, the responding of participants in either of the other two quadrants cannot be explained by TDM.\n\n\n\n\n\n\n\n\n\n\nEven in taking the most generous measure of perceptions of harm (left), the responding of 17% of participants (9% plus 8%) cannot be explained by TDM. Taking the stricter (and arguably more accurate, see McHugh et al. 2020) measure of perceptions of harm further reduces the explanatory power of TDM – only 45% of participants responded in line with the predictions of TDM. In addition to evidence for harmless wrongs, the same set of studies had questions explicitly related to the wrongness of behaviors linked with harm and potential harm. While participants were not explicitly asked about their perceptions of harm for boxing or contact team sports, they were presented with a question “How would you rate the behavior of two people who engage in an activity that could potentially result in harmful consequences for either of them?”. Only 50% of participants across two studies (N = 613) rated this as wrong, providing clear evidence for the idea of “wrongless harms” that is rejected by TDM (Schein and Gray 2018).\nSo far, there is nothing uniquely “moral” in moral judgment. The people researchers have studied do not appear to apply any given mode of processing or content in a sufficiently consistent manner to provide stable account of moral judgment. We argue, therefore, that a more successful approach is to explore what the capacity to identify morally right and morally wrong actors, actions and outcomes has in common with people’s capacity to identify categories more generally."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#moral-phenomena-with-domain-general-categorization-explanations",
    "href": "publications/moral-judgment-as-categorization/index.html#moral-phenomena-with-domain-general-categorization-explanations",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Moral Phenomena with Domain General (Categorization) Explanations",
    "text": "Moral Phenomena with Domain General (Categorization) Explanations\nMJAC assumes that moral categorization is a dynamical, context-dependent process, and as such, we predict the same phenomena as have been found within the categorization literature at large. In this section, we briefly outline some evidence for this predicted similarity, though we note that at present, these patterns are more suggestive than conclusive. However, we argue that these patterns should be seen not as noise obscuring an underlying stable moral category but a signal of the complexity of the processes that give rise to that category. We believe that the phenomenon of moral judgment is no more undermined or challenged by this complexity than the cognitive psychology of concepts, and category formation is more generally. These include such phenomena as order effects, language effects, the impact of emotions, and typicality of instance.\n\nOrder effects\nIn morality research, responses to different moral dilemmas have been found to vary depending on the order of presentation (Petrinovich and O’Neill 1996; Wiegmann, Okan, and Nagel 2012). MJAC can explain these in the same way as order effects in non-moral categorization are explained. That is, they occur as a result of priming. The scenario that is presented first causes some features of the second scenario to become more salient. The salience of these features leads to a different judgment than if the initial scenario was not presented. In the case of categorization, the effect of this type of priming is primarily studied concerning reaction times. For example, a study by Barsalou (1982, 2003) showed that reading sentences that made particular features of a given object salient influenced the speed at which participants verified related properties of the given object (see also Tabossi 1988). We predict similar reaction time variability should be observed when participants are primed with relevant properties for making moral categorizations.\nThere is also evidence that priming people with particular concepts can influence their subsequent categorizations. In a study by Higgins, Bargh, and Lombardi (1985), participants completed a task in which they were required to create sentences from a selection of words. Some of the words presented were selected to prime a particular concept, e.g., “bold”, “courageous”, and “brave” primed “Adventurous”; “careless”, “foolhardy”, and “rash” primed “Reckless” (Higgins, Bargh, and Lombardi 1985, 63). Participants were later presented with a description of ambiguous behavior. It was found that the categorizations of these behaviors were influenced by the concept that was primed. A similar study demonstrated the same effect (Srull and Wyer 1979). We predict that this same effect should occur for moral categorizations, for example, participants responses to descriptions of behavior that could be viewed as either “moral” or “self-righteous”, or a behavior that could be viewed as either “immoral” or “crafty” should be subject to the same effect as described by Higgins, Bargh, and Lombardi (1985).\n\n\nLanguage effects\nThough the influence of language on the categories available to a given person has a long and controversial history in psychology, recent research has made it increasingly clear that a given language forms a significant constraint on categorization tasks due to the resources of vocabulary and grammatical structure that it provides (Cubelli et al. 2011; Davidoff 2001). Second language acquisition also impacts how categorizations are formed, as a person learns to deploy new linguistic resources in the service of their goal-directed activities (Athanasopoulos 2007)\nPeople’s moral judgments have been shown to vary depending on whether they read a moral scenario in their first language or in a second language (the ‘foreign language effect,’ e.g., Cipolletti, McFarlane, and Weissglass 2016; Costa et al. 2014; Driver 2020; Geipel, Hadjichristidis, and Surian 2015; Hayakawa et al. 2017). Specifically, people appear to be more willing to endorse action in the Footbridge/Push version of the trolley dilemma when this dilemma is presented a language other than their native language. According to MJAC, deontological judgments become intuitive as a result of consistency across contexts. The changing of the language presents a novel context, which means the inferences associated with the regular context (e.g., emotional inferences) of encountering or this scenario are not as salient. Evidence for this interpretation comes from research investigating people’s reactions to non-moral taboo words in their first language vs. a second language. Harris, Ayçiçeĝi, and Gleason (2003) measured skin conductance of English speakers and Turkish speakers when rating different types of words in their first language and in their second language. It was found that (non-moral) taboo words led to greater arousal when presented in participants’ first language than when presented in a second language (see also, Colbeck and Bowers 2012), suggesting that the emotional inferences associated with the footbridge dilemma are less salient when it is presented in a foreign language.\n\n\nEmotion effects\nEmotion is perhaps the most widely discussed contextual influence on moral judgments (e.g., Cameron, Payne, and Doris 2013; Giner-Sorolla 2018; Huebner, Dwyer, and Hauser 2009; Landy and Goodwin 2015; May 2014; Prinz 2005; Royzman et al. 2014; Rozin et al. 1999; Russell and Giner-Sorolla 2011; Valdesolo and DeSteno 2006). Above, we have outlined how specific emotions may become associated with particular types of judgment; that is, the emergence of relative stability in making specific categorizations is linked with consistency in relevant contextual features, where the relevant contextual features include emotions. In other words, the emotions that may be experienced when a moral categorization is learned (or reinforced/consolidated) are likely to also be present during later categorizations. A corollary of this is that the experience of the specific emotion may provide a contextual cue, reminding people of previous experiences, making a particular categorization more salient (e.g., Barsalou 2003; Barsalou and Wiemer-Hastings 2005; Damasio 1994; Damasio and Damasio 1994; Rosenfield 1988).\nIn line with the prediction that manipulations designed to suppress the salience of these contextual factors (S. M. Smith and Vela 2001), we predict the same type of manipulations should have similar effects on the influences of emotions on moral categorizations. The foreign language (Colbeck and Bowers 2012; Costa et al. 2014; Driver 2020; Geipel, Hadjichristidis, and Surian 2015; Harris, Ayçiçeĝi, and Gleason 2003; Hayakawa et al. 2017) effect described above provides some evidence for this, whereby the salience of the emotional content is reduced by being presented in the second language. Similar effects should be observed using mindset manipulations (Igou 2011; Igou and Bless 2007).\nThe specific contextual influences discussed above provide just a sample of the broader contextual factors known to influence the making of moral judgment. MJAC assumes that moral judgments are dynamical and context-dependent, and as such, it is the approach that is best positioned to understand the diverse contextual influences on moral judgment. It is beyond the scope of the current paper to describe and account for all the known contextual influences on moral judgment (e.g., an incomplete list would include: Bostyn, Sevenhant, and Roets 2018; Christensen et al. 2014; Christensen and Gomila 2012; Costa et al. 2014; Cushman et al. 2012; Everett, Pizarro, and Crockett 2016; Everett et al. 2018; Forbes 2018; Francis et al. 2016; Francis et al. 2017; Lee and Holyoak 2020; Petrinovich and O’Neill 1996; Rozin et al. 1999; Schein 2020; Timmons and Byrne 2019; Uhlmann, Pizarro, and Diermeier 2015; Valdesolo and DeSteno 2006; Vasquez et al. 2001; Vasudev and Hummel 1987). However, MJAC predicts understanding these diverse context effects depends on (a) accounting the learning history (e.g., in the cases of emotional influences and the foreign language effect) and, (b) viewing moral categorization as occurring as part of goal-directed activity (e.g., categorization of actor versus action discussed above). Incorporating both of these considerations into a program of research inevitably leads to attempts to make the study of moral judgment reflective of real-world moral decision making (Bauman et al. 2014; Bostyn, Sevenhant, and Roets 2018; Gilligan 1977, 1993; Hester and Gray 2020; Hofmann et al. 2014; Schein 2020; Watkins 2020).\n\n\nTypicality\nFinally, one of the most salient phenomena within the field of categorization concerns the fact that there are “better” and “worse” examples of any given category (McCloskey and Glucksberg 1978; Oden 1977), for example, a chair is viewed as a more typical member of the category FURNITURE than bookends (McCloskey and Glucksberg 1978). Such judgments are made even for those categories with supposedly logical or sharp boundaries such as geometric figures (Bourne 1982; Feldman 2000).\nMJAC predicts that this same phenomena of typicality should be observed for moral categorizations, e.g., cold-blooded murder versus violence in pursuit of a cause. We further predict that relative typicality should be related to the relative consistency with which category members are identified as members of the given category (and should be independent of perceived severity). This facet of moral judgment has already seen some discussion in the existing moral judgment theoretical literature. Cushman (2013, 282) makes a passing reference – that pushing someone “with your hands” is more typically harmful than pushing someone “with your buttocks”. However, typicality sees more substantial discussion in the context of TDM (Gray and Keeney 2015; Schein and Gray 2018).\nTypicality ratings in moral judgments, as described by TDM, are related to the degree to which a given scenario matches the defined prototype of morality, as an “intentional agent causing damage to a vulnerable patient” (Schein and Gray 2018, 32). An act that more clearly involves harm is rated as more typically wrong than an action where the perceived harm is less. Similarly, if there are evident intentional agent and vulnerable patient, an action is rated as more typically wrong than if the actors are more similar in their intentionality and vulnerability (Gray and Keeney 2015; Schein and Gray 2018).\nThis account of typicality is based on assumptions related to content (agent-patient, harm) and does not inform our understanding of the cognitive processes underlying moral judgments. As such, it cannot clearly distinguish between typicality and severity. Indeed the strong overlap between severity of an act and it’s typicality as an example of moral wrongness is acknowledged: “By definition, more severe acts are more immoral; that is, they are better examples of the category”immorality” (Gray and Keeney 2015, 860).\nWith MJAC, we propose that typicality is related to both frequency and consistency of exposure; that is, behaviors that are frequently encountered and consistently identified as members of a given moral category should emerge as typical category members. Given the consistency with which harm related transgressions are identified as wrong, the emergence of the prototypical template described by Gray and colleagues is not surprising (Gray and Keeney 2015; Schein and Gray 2018). However, we attribute these typicality ratings to the learning history rather than to perceptions of harm and of agents and patients.\nGiven the possible confounding influence of severity on typicality ratings, unpacking this difference in interpretation will prove challenging; however, we believe it will be a worthwhile endeavor. We hypothesize typicality ratings are related to the learning history and not linked to specific content. This predicts differences in typicality ratings when controlling for severity (either by focusing on harmless dilemmas or by keeping the severity of harm constant). This also predicts differences in typicality ratings within populations, through individual differences in moral values (e.g., Graham et al. 2012; Haidt and Joseph 2008), and between populations through cultural variation (e.g., Haidt, Koller, and Dias 1993). Furthermore, this view of typicality of moral categorizations predicts that perceptions of typicality will be context sensitive, that is intra-personal variability should be observed depending on current context, and crucially depending on current goal-directed activity. A professor grading papers would rate straight plagiarism as more typically wrong than plagiarism by omitting references. Whereas when not grading papers, the same professor may be more concerned with the ethics of her colleagues’ precarious contracts and entirely indifferent to the shortcuts students may take in their assignments. Or a sports fan may claim to view cheating as wrong, where different cheating behaviors vary in their typicality (e.g., overt fouling, cynical fouling, feigning injury so that the referee penalizes the other team) however, the same fan may turn a blind eye to these behaviors when committed by members of the team she supports.\nIt is worth noting that this sensitivity to the context of moral judgment implies that the importance of understanding moral judgments in more real-life contexts rather than through the study of abstract decontextualized dilemmas has been well documented (e.g., Bauman et al. 2014; Bostyn, Sevenhant, and Roets 2018; Gilligan 1977, 1993; Hester and Gray 2020; Hofmann et al. 2014; Schein 2020; Watkins 2020). By focusing specifically on context-sensitive categorizations occurring as part of goal-directed activity, MJAC offers a framework for attempting to make the study of moral judgments more reflective of the making of moral judgments in everyday life. Furthermore, in recognizing the broader array of contextual influences on moral categorizations, rather than focusing on specific contextual influences on specific types of judgments, MJAC is uniquely positioned to incorporate known context effects into a coherent parsimonious framework. This would provide opportunities for the combined influences of these contextual factors to be studied relative to each other, with the potential to identify clear boundary conditions to understand how and when specific contextual factors influence moral categorizations more than others."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#summarizing-the-differences-between-mjac-and-existing-approaches",
    "href": "publications/moral-judgment-as-categorization/index.html#summarizing-the-differences-between-mjac-and-existing-approaches",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Summarizing the Differences Between MJAC and Existing Approaches",
    "text": "Summarizing the Differences Between MJAC and Existing Approaches\nAbove, we have outlined how MJAC differs from existing theories in terms of assumptions and explanation. These theories make assumptions based on content, and this results in essentialist theorizing, either implicit or explicit attempts to define an “essence” of morality. In contrast, MJAC rejects essentialism, instead assuming moral categorizations are dynamical, context-dependent, and occurring as part of goal-directed activity. Each of the theories discussed is explicitly or implicitly (e.g., Schein and Gray 2018, 41) based on dual-process assumptions, with related dichotomous assumptions regarding the cognitive mechanisms (where these mechanisms are specified). MJAC does not assume distinct, separable processes, adopting type-token interpretation, occurring as part of goal-directed activity (Barsalou 2003, 2017), as the mechanism that underlies moral categorization. These differences in assumptions underlie the differences in the explanation discussed above. These differences are summarized in Table 1.\nTable 1: Specific points of divergence between MJAC and existing theories\n\n\n\n\n\n\n\n\n\n\n\n\nGreene’s Dual-process theory\n“Soft” dual-process theory\nModel-based accounts\nTDM\nMJAC\n\n\n\n\nAssumptions:\n\n\n\n\n\n\n\nContent\nDeontology-utilitarianism / personal-impersonal\nDeontology-utilitarianism\nAction-outcome\nHarm-based, dyadic\nDynamical Context-dependent Goal-directed\n\n\nMoral “Essence”\n(Implicit)\n(Not discussed)\n(Implicit)\nExplicit\nRejected\n\n\nProcesses\nDual-processes\nDual-processes\nDual-processes\n(implicitly dual-process)\nContinuum\n\n\nMechanisms\nIntuition (emotion) / cognition\nEmotion / cognition\nModel-based / model-free\nCategorization (unspecified)\nType-token interpretation\n\n\nPhenomena Explained:\n\n\n\n\n\n\n\nDumbfounding (harmless wrongs)\n(Not discussed)\n(Not discussed)\nExplained\nDenied\nExplained: learning history\n\n\nWrongless harms\n(Not discussed)\n(Not discussed)\n(Not discussed)\nDenied\nExplained: learning history\n\n\nTypicality\n(Not discussed)\n(Not discussed)\n(Not discussed)\nMatching of “prototype”\nContext-dependent\n\n\nContextual influences\nSpecific: Personal-impersonal\nSpecific: Emotion / cognition\n\nSpecific: Action-outcome\nSpecific: Harm-based"
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#effects-not-directly-predicted-by-mjac",
    "href": "publications/moral-judgment-as-categorization/index.html#effects-not-directly-predicted-by-mjac",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Effects not Directly Predicted by MJAC",
    "text": "Effects not Directly Predicted by MJAC\nDespite predicting a broad range of contextual variability, there remain some influences on moral judgment that are not directly predicted by MJAC. Three such phenomena are the doctrine of double effect, moral luck, and moral conviction. While not directly predicted, these phenomena further illustrate the variability and complexity that theories of moral judgment must account for.\nFirstly, the doctrine of double effect is the name given to the finding that people view causing harm as a means to achieving a goal as worse than causing harm as a side-effect of achieving a goal (Doris 2010; Mikhail 2000). Above, we have presented the doctrine of double effect as a limitation of model-based approaches (Crockett 2013; Cushman 2013); the action-outcome distinction does not adequately explain why people should make a distinction between harm as a means and harm as a side effect (henceforth, means-side effect). Similarly, this means-side effect distinction is not directly predicted by MJAC. Interestingly, it has been found that people apply this distinction even though they cannot reliably articulate it (Cushman, Young, and Hauser 2006; Hauser et al. 2007). This suggests a similarity with moral dumbfounding, and the possibility of a common explanation. In the case of moral dumbfounding, MJAC posits that people implicitly learn (through continued and consistent type-token interpretation) that something is wrong and that learning the categorization occurs independently of learning the reasons for the categorization. Distinguishing side effects from means is much more subtle than distinguishing different types of actions, however, there is no reason for such a distinction not to emerge through the same process of type-token interpretation if others are making the same distinction in their moral judgments (Cushman, Young, and Hauser 2006; Hauser et al. 2007; Mikhail 2000). In this way, while it is not an obvious a priori prediction of MJAC, the doctrine of double effect is not inconsistent with its assumptions.\nThe second known effect that is not directly predicted by MJAC is the phenomenon of moral luck. Moral luck demonstrates that different outcomes can lead to different evaluations of the same behavior (Nagel 1979, 2013; Williams 1982; Wolf 2001; Young, Nichols, and Saxe 2010). Consider the following two scenarios (adapted from Wolf 2001; see also Royzman and Kumar 2004; Williams 1982):\nJo\n\nA truck driver (Jo), needs to make an emergency stop. Jo has neglected to check the brakes of the truck recently. When attempting to stop the truck, Jo loses control and the truck crashes into the ditch.\n\nPat\n\nA truck driver (Pat), needs to make an emergency stop. Pat has neglected to check the brakes of the truck recently. When attempting to stop the truck, Pat loses control and the truck runs over a child.\n\nThe actions of Jo and Pat are the same, however, previous research has shown that in situations like this, people are likely to view Pat as more morally blameworthy than Jo (Walster 1966; Wells and Gavanski 1989; Young, Nichols, and Saxe 2010). People are more harsh in their moral judgments of the same actions when the actions result in negative outcomes. Williams (1982; see Wolf 2001) is attributed with coining the phrase “moral luck” to describe this asymmetry of judgments of actions based on outcomes.\nAs with the trolley problem, and the emergence of typicality, MJAC explains the phenomenon of moral luck with reference to the consistency of previous categorizations. Causing harm to another person is relatively consistently categorized as MORALLY WRONG (Cushman et al. 2012; Schein and Gray 2018; though not with perfect consistency, e.g., Alicke 2012; McHugh et al. 2020). This relative consistency means that encountering an event in which the actions of an agent cause harm is highly likely to be categorized as MORALLY WRONG. The actions described in classic moral luck scenarios are typically ambiguous or minimally problematic. That is, they are not categorized as wrong with the same consistency. This mismatch in the consistency with which the actions vs the outcomes are categorized as wrong that leads to what we observe as moral luck. In effect, the harmful outcome may be viewed as a contextual influence that leads to harsher judgments of actions.\nA third phenomenon that is not directly addressed by MJAC is moral conviction (e.g., Skitka 2010), or zeal in moral positions (e.g., McGregor 2006). While MJAC does not make specific claims about moral conviction, previous research has linked this to identity and identification with particular groups (e.g., Greene 2013; see also Proulx and Inzlicht 2012; Heine, Proulx, and Vohs 2006), and more recently attitude strength has been linked with connectivity (e.g., Dalege et al. 2019) We suggest that the meaning maintenance model provides an ideal framework for understanding zeal in moral categorization. According to the meaning maintenance model (Heine, Proulx, and Vohs 2006), there are four primary domains of meaning: certainty, self-esteem, social relations, and mortality. Where non-moral category knowledge constitutes meaning in the domain of certainty (Heine, Proulx, and Vohs 2006), moral knowledge additionally holds meaning in the social domain (Greene 2013; see also Proulx and Inzlicht 2012; Heine, Proulx, and Vohs 2006). We hypothesize that it is this spanning of both the certainty and the social domains of meaning that leads to moral zeal.\nWhen we apply this insight to the broader framework of MJAC, it appears that some contexts (namely social/group contexts) matter more in the development of robust moral categories. We hypothesize that robustness in moral categorization is related to the consistency of categorization across multiple (social) contexts. Consider the categorization of sexist jokes as MORALLY WRONG. Some groups would endorse this categorization, and there are groups who would disagree. The degree to which a person will be motivated to defend this categorization will be related to the social groups they are members of, and the consistency across these groups. Someone who agrees with this categorization but spends a lot of time tolerating “locker room talk” will be less zealous than someone who socializes with people who openly identify as feminists."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#footnotes",
    "href": "publications/moral-judgment-as-categorization/index.html#footnotes",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe use the term “skill” to refer to the type of automaticity that is developed through practice/rehearsal. This does not imply any objectivist moral truth that can be accessed by moral “experts”, we refer only to the cognitive processes involved.↩︎\nWhere Token denotes a token that may be viewed as ‘prototypical’ for a given type.↩︎\nRelatedly, for favourable judgements, we predict the opposite effect. That is, if for morally praiseworthy actions are being performed by a close other, the target of the categorisation is more likely to be the actor than the action, helping to maintain a positive view of the close other (Forbes 2018; Murray, Holmes, and Griffin 1996a, 1996b).↩︎\nRecall the discussion regarding the ad-hoc category THINGS TO PACK INTO A SUITCASE (Barsalou 1991, 2003).↩︎\nThese figures are not reported in McHugh et al. (2020), however see McHugh et al. (2018) for full data sets.↩︎"
  },
  {
    "objectID": "publications/national-identity/index.html",
    "href": "publications/national-identity/index.html",
    "title": "National Identity Predicts Public Health Support during a Global Pandemic",
    "section": "",
    "text": "Changing collective behaviour and supporting non-pharmaceutical interventions is an important component in mitigating virus transmission during a pandemic. In a large international collaboration (Study 1, N = 49,968 across 67 countries), we investigated self-reported factors associated with public health behaviours (e.g., spatial distancing and stricter hygiene) and endorsed public policy interventions (e.g., closing bars and restaurants) during the early stage of the COVID-19 pandemic (April-May 2020). Respondents who reported identifying more strongly with their nation consistently reported greater engagement in public health behaviours and support for public health policies. Results were similar for representative and non-representative national samples. Study 2 (N = 42 countries) conceptually replicated the central finding using aggregate indices of national identity (obtained using the World Values Survey) and a measure of actual behaviour change during the pandemic (obtained from Google mobility reports). Higher levels of national identification prior to the pandemic predicted lower mobility during the early stage of the pandemic (r = −0.40). We discuss the potential implications of links between national identity, leadership, and public health for managing COVID-19 and future pandemics.\n\n\nPDF\n\n\nSource Document\n\n\n\n\n\nBibliography\n\n\n\n  Van Bavel, J. J., Cichocka, A., Capraro, V., Sjåstad, H., Nezlek, J. B., Pavlović, T., Alfano, M., Gelfand, M. J., Azevedo, F., Birtel, M. D., Cislak, A., Lockwood, P. L., Ross, R. M., Abts, K., Agadullina, E., Aruta, J. J. B., Besharati, S. N., Bor, A., Choma, B. L., … Boggio, P. S. (2022). National identity predicts public health support during a global pandemic. Nature Communications, 13(1), Article 1. https://doi.org/10.1038/s41467-021-27668-9"
  },
  {
    "objectID": "publications/national-identity/index.html#abstract",
    "href": "publications/national-identity/index.html#abstract",
    "title": "National Identity Predicts Public Health Support during a Global Pandemic",
    "section": "",
    "text": "Changing collective behaviour and supporting non-pharmaceutical interventions is an important component in mitigating virus transmission during a pandemic. In a large international collaboration (Study 1, N = 49,968 across 67 countries), we investigated self-reported factors associated with public health behaviours (e.g., spatial distancing and stricter hygiene) and endorsed public policy interventions (e.g., closing bars and restaurants) during the early stage of the COVID-19 pandemic (April-May 2020). Respondents who reported identifying more strongly with their nation consistently reported greater engagement in public health behaviours and support for public health policies. Results were similar for representative and non-representative national samples. Study 2 (N = 42 countries) conceptually replicated the central finding using aggregate indices of national identity (obtained using the World Values Survey) and a measure of actual behaviour change during the pandemic (obtained from Google mobility reports). Higher levels of national identification prior to the pandemic predicted lower mobility during the early stage of the pandemic (r = −0.40). We discuss the potential implications of links between national identity, leadership, and public health for managing COVID-19 and future pandemics.\n\n\nPDF\n\n\nSource Document\n\n\n\n\n\nBibliography\n\n\n\n  Van Bavel, J. J., Cichocka, A., Capraro, V., Sjåstad, H., Nezlek, J. B., Pavlović, T., Alfano, M., Gelfand, M. J., Azevedo, F., Birtel, M. D., Cislak, A., Lockwood, P. L., Ross, R. M., Abts, K., Agadullina, E., Aruta, J. J. B., Besharati, S. N., Bor, A., Choma, B. L., … Boggio, P. S. (2022). National identity predicts public health support during a global pandemic. Nature Communications, 13(1), Article 1. https://doi.org/10.1038/s41467-021-27668-9"
  },
  {
    "objectID": "publications/pandemic-threat/index.html",
    "href": "publications/pandemic-threat/index.html",
    "title": "Pandemic threat and group cohesion:",
    "section": "",
    "text": "Authoritarianism emerges in times of societal threat, in part driven by desires for group-based security. As such, we propose that the threat caused by the COVID-19 pandemic was associated with increased authoritarian tendencies and that this can be partially explained by increased national identification. We tested this hypothesis by collecting cross-sectional data from three different countries in April 2020. In Study 1, data from Ireland (N = 1276) showed that pandemic threat predicted increased national identification, which in turn predicted authoritarianism. In Study 2, we replicated this indirect effect in a representative UK sample (N = 506). In Study 3, we used an alternative measure of authoritarianism and conceptually replicated this effect among USA citizens (N = 429). In this US sample, the association between threat and authoritarian tendencies was stronger among progressives compared to conservatives. Findings are discussed and linked to group-based models of authoritarianism.\n\n\nPDF\n\n\nSource Document\n\n\n\n\n\nBibliography\n\n\n\n  Maher, P. J., Roth, J., Griffin, S., Foran, A. M., Jay, S., McHugh, C., Ryan, M., Bradshaw, D., Quayle, M., & Muldoon, O. T. (2022). Pandemic threat and group cohesion: National identification in the wake of COVID-19 is associated with authoritarianism. The Journal of Social Psychology, 0(0), 1–17. https://doi.org/10.1080/00224545.2021.2024122"
  },
  {
    "objectID": "publications/pandemic-threat/index.html#abstract",
    "href": "publications/pandemic-threat/index.html#abstract",
    "title": "Pandemic threat and group cohesion:",
    "section": "",
    "text": "Authoritarianism emerges in times of societal threat, in part driven by desires for group-based security. As such, we propose that the threat caused by the COVID-19 pandemic was associated with increased authoritarian tendencies and that this can be partially explained by increased national identification. We tested this hypothesis by collecting cross-sectional data from three different countries in April 2020. In Study 1, data from Ireland (N = 1276) showed that pandemic threat predicted increased national identification, which in turn predicted authoritarianism. In Study 2, we replicated this indirect effect in a representative UK sample (N = 506). In Study 3, we used an alternative measure of authoritarianism and conceptually replicated this effect among USA citizens (N = 429). In this US sample, the association between threat and authoritarian tendencies was stronger among progressives compared to conservatives. Findings are discussed and linked to group-based models of authoritarianism.\n\n\nPDF\n\n\nSource Document\n\n\n\n\n\nBibliography\n\n\n\n  Maher, P. J., Roth, J., Griffin, S., Foran, A. M., Jay, S., McHugh, C., Ryan, M., Bradshaw, D., Quayle, M., & Muldoon, O. T. (2022). Pandemic threat and group cohesion: National identification in the wake of COVID-19 is associated with authoritarianism. The Journal of Social Psychology, 0(0), 1–17. https://doi.org/10.1080/00224545.2021.2024122"
  },
  {
    "objectID": "publications/predicting-attitudinal/index.html",
    "href": "publications/predicting-attitudinal/index.html",
    "title": "Predicting attitudinal and behavioral responses to COVID-19 pandemic using machine learning",
    "section": "",
    "text": "At the beginning of 2020, COVID-19 became a global problem. Despite all the efforts to emphasize the relevance of preventive measures, not everyone adhered to them. Thus, learning more about the characteristics determining attitudinal and behavioral responses to the pandemic is crucial to improving future interventions. In this study, we applied machine learning on the multinational data collected by the International Collaboration on the Social and Moral Psychology of COVID-19 (N = 51,404) to test the predictive efficacy of constructs from social, moral, cognitive, and personality psychology, as well as socio-demographic factors, in the attitudinal and behavioral responses to the pandemic. The results point to several valuable insights. Internalized moral identity provided the most consistent predictive contribution—individuals perceiving moral traits as central to their self-concept reported higher adherence to preventive measures. Similar results were found for morality as cooperation, symbolized moral identity, self-control, open-mindedness, and collective narcissism, while the inverse relationship was evident for the endorsement of conspiracy theories. However, we also found a non-neglible variability in the explained variance and predictive contributions with respect to macro-level factors such as the pandemic stage or cultural region. Overall, the results underscore the importance of morality-related and contextual factors in understanding adherence to public health recommendations during the pandemic.\n\n\nPDF\n\n\nSource Document\n\n\n\n\n\nBibliography\n\n\n\n  Pavlović, T., Azevedo, F., De, K., Riaño-Moreno, J. C., Maglić, M., Gkinopoulos, T., Donnelly-Kehoe, P. A., Payán-Gómez, C., Huang, G., Kantorowicz, J., Birtel, M. D., Schönegger, P., Capraro, V., Santamaría-García, H., Yucel, M., Ibanez, A., Rathje, S., Wetter, E., Stanojević, D., … Van Bavel, J. J. (2022). Predicting attitudinal and behavioral responses to COVID-19 pandemic using machine learning. PNAS Nexus, 1(3), pgac093. https://doi.org/10.1093/pnasnexus/pgac093"
  },
  {
    "objectID": "publications/predicting-attitudinal/index.html#abstract",
    "href": "publications/predicting-attitudinal/index.html#abstract",
    "title": "Predicting attitudinal and behavioral responses to COVID-19 pandemic using machine learning",
    "section": "",
    "text": "At the beginning of 2020, COVID-19 became a global problem. Despite all the efforts to emphasize the relevance of preventive measures, not everyone adhered to them. Thus, learning more about the characteristics determining attitudinal and behavioral responses to the pandemic is crucial to improving future interventions. In this study, we applied machine learning on the multinational data collected by the International Collaboration on the Social and Moral Psychology of COVID-19 (N = 51,404) to test the predictive efficacy of constructs from social, moral, cognitive, and personality psychology, as well as socio-demographic factors, in the attitudinal and behavioral responses to the pandemic. The results point to several valuable insights. Internalized moral identity provided the most consistent predictive contribution—individuals perceiving moral traits as central to their self-concept reported higher adherence to preventive measures. Similar results were found for morality as cooperation, symbolized moral identity, self-control, open-mindedness, and collective narcissism, while the inverse relationship was evident for the endorsement of conspiracy theories. However, we also found a non-neglible variability in the explained variance and predictive contributions with respect to macro-level factors such as the pandemic stage or cultural region. Overall, the results underscore the importance of morality-related and contextual factors in understanding adherence to public health recommendations during the pandemic.\n\n\nPDF\n\n\nSource Document\n\n\n\n\n\nBibliography\n\n\n\n  Pavlović, T., Azevedo, F., De, K., Riaño-Moreno, J. C., Maglić, M., Gkinopoulos, T., Donnelly-Kehoe, P. A., Payán-Gómez, C., Huang, G., Kantorowicz, J., Birtel, M. D., Schönegger, P., Capraro, V., Santamaría-García, H., Yucel, M., Ibanez, A., Rathje, S., Wetter, E., Stanojević, D., … Van Bavel, J. J. (2022). Predicting attitudinal and behavioral responses to COVID-19 pandemic using machine learning. PNAS Nexus, 1(3), pgac093. https://doi.org/10.1093/pnasnexus/pgac093"
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html",
    "href": "publications/reasons-or-rationalizations/index.html",
    "title": "Reasons or Rationalisations:",
    "section": "",
    "text": "Moral dumbfounding occurs when people maintain a moral judgment even though they cannot provide reasons for it. Recently, questions have been raised about whether dumbfounding is a real phenomenon. Two reasons have been proposed as guiding the judgments of dumbfounded participants: harm-based reasons (believing an action may cause harm) or norm-based reasons (breaking a moral norm is inherently wrong). Participants who endorsed either reason were excluded from analysis, and instances of moral dumbfounding seemingly reduced to non-significance. We argue that endorsing a reason is not sufficient evidence that a judgment is grounded in that reason. Stronger evidence should additionally account for (a) articulating a given reason, and (b) consistently applying the reason in different situations. Building on this, we develop revised exclusion criteria across 2 studies. Study 1 included an open-ended response option immediately after the presentation of a moral scenario. Responses were coded for mention of harm-based or norm-based reasons. Participants were excluded from analysis if they both articulated and endorsed a given reason. Using these revised criteria for exclusion, we found evidence for dumbfounding, as measured by the selecting of an admission of not having reasons. Study 2 included a further three questions relating to harm-based reasons specifically, assessing the consistency with which people apply harm-based reasons across differing contexts. As predicted, few participants consistently applied, articulated, and endorsed harm-based reasons, and evidence for dumbfounding was found."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#the-influence-of-moral-dumbfounding",
    "href": "publications/reasons-or-rationalizations/index.html#the-influence-of-moral-dumbfounding",
    "title": "Reasons or Rationalisations:",
    "section": "1.1 | The Influence of Moral Dumbfounding",
    "text": "1.1 | The Influence of Moral Dumbfounding\nThe discovery of moral dumbfounding (Haidt, Björklund, and Murphy 2000; see also Haidt, Koller, and Dias 1993) coincided with, and arguably contributed to, some of the key developments in moral psychology over the past two decades. It had a clear influence on the development of Haidt’s social intuitionist model of moral judgment (SIM, Haidt 2001), and by extension may be seen as contributing to the growth of intuitionist theories of moral judgment that followed (e.g., Cushman, Young, and Greene 2010; Haidt 2001; Prinz 2005).\nHaidt proposed the SIM in opposition to the perceived dominance of rationalist approaches (Kohlberg 1969, 1971; Narvaez 2005; Topolski et al. 2013). According to rationalist approaches our moral judgments are grounded in reason, informed by discernible moral principles (Fine 2006; Kennett and Fine 2009; Kohlberg 1971, 1969; Royzman, Kim, and Leeman 2015); Haidt (2001). Moral dumbfounding is presented by Haidt (2001) and by Prinz (2005) as evidence against this rationalist perspective, in that, if moral judgments were grounded in reason, people would be able to provide reasons for their judgments (and moral dumbfounding would not occur). Intuitionist theorists propose that moral judgments are grounded in an emotional or intuitive automatic response rather than slow deliberate reasoning (Cameron, Payne, and Doris 2013; Haidt 2001; Prinz 2005). In recent years the joint role of reason/deliberation and intuition in the making of moral judgments has been emphasised in dual-process theories (Crockett 2013; Cushman, Young, and Greene 2010; Cushman 2013b; Greene 2008; Brand 2016). The dumbfounding paradigm may be useful in developing and extending these theories; developing an understanding of moral dumbfounding and the processes that lead to it, may inform the further development of theories of moral judgment, leading to a greater understanding of the processes that underlie moral judgment more generally.\nThe influence of dumbfounding may be observed in everyday discourse, particularly in relation to highly sensitive and divisive social issues. Real-world interactions differ from a laboratory study designed to elicit a dumbfounded response, and as such, in the absence of explicit and consistent refuting of arguments, it is unlikely that people in everyday life would admit to not having reasons for their moral judgments. Despite this, it is not uncommon to hear unsupported declarations/tautological statements as arguments in support of a position with no further justification (e.g., Mustonen et al. 2017; Stepniak 1995). Similarly, moral positions are often justified by appealing to emotions (e.g., Mustonen et al. 2017; Stepniak 1995; see also Rozin et al. 2008, 1999). This type of appeal to emotion has previously been discussed as similar/equivalent to dumbfounding (see Prinz 2005, 101; see also Haidt and Hersh 2001). These responses may not clearly demonstrate dumbfounding, however they illustrate the way in which discussions of reasons for moral positions are occasionally absent from the public debate.\nThat people may defend a judgment in the absence of articulated reasons, and maintain it even in the knowledge of their own inconsistencies poses a challenge for the type of rational debate that is supposed to form the basis of public discourse and inform the development of public policy. The study of moral dumbfounding, as an extreme case, may lead to a better understanding of the underlying cognitive processes that lead to these types of problematic practices that have no place in public debate. Identifying these processes and explaining moral dumbfounding is beyond the scope of the current research. Here, in light of recent critiques, here we test whether or not dumbfounding is a real phenomenon, worthy of further study."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#challenging-the-dumbfounding-paradigm",
    "href": "publications/reasons-or-rationalizations/index.html#challenging-the-dumbfounding-paradigm",
    "title": "Reasons or Rationalisations:",
    "section": "1.2 | Challenging the Dumbfounding Paradigm",
    "text": "1.2 | Challenging the Dumbfounding Paradigm\nA key concern regarding the dumbfounding paradigm is that the eliciting scenarios have been artificially construed to remove potentially harmful consequences to the point that they become unrealistic or otherwise not credible (e.g., Jacobson 2012). It could be argued that studying such idiosyncratic scenarios does little to inform our understanding of everyday moral decision making; similar criticisms have been made regarding the widely used trolley-type sacrificial dilemmas (e.g., Bauman et al. 2014; Bostyn, Sevenhant, and Roets 2018). However, responses to hypothetical trolley dilemmas have been found to predict behaviour in a money burning game with real pay-off consequences (Dickinson and Masclet 2018), and the study of trolley-type dilemmas arguably contributed to key theoretical advancements of the past two decades (e.g., Plunkett and Greene 2019; see also Greene 2008; Christensen and Gomila 2012; Christensen et al. 2014; Greene et al. 2001). If moral dumbfounding is a real phenomenon it may prove a useful paradigm to further advance theories of moral judgment, and examine the mechanisms and cognitive processes that underlie the making of moral judgments (e.g., the relative roles of emotion versus deliberation). It may be possible to identify specific contextual features that may lead people to change their mind rather than provide a dumbfounded response (or vice-versa). Experimental manipulations that may increase dumbfounded responding (e.g., cognitive load) or reduce dumbfounded responding (e.g., distancing) could be investigated. There may also be individual difference variables that predict susceptibility to dumbfounding.\nIn defending the claim that moral judgments are not caused by reasoning, Haidt (2001) presents moral dumbfounding as a demonstration of inconsistency between judgment and reasons available. The implicit alternative to this argument is that the absence of reasons would lead a moral judgment to change or to be revised; i.e., the presence or absence of reasons can cause a judgment to change. Haidt does not clearly distinguish between reasoning as a cause versus reasons as a cause of judgments (e.g., 2001, 822). Despite being inconsistent with approaches beyond the moral domain (e.g., Mercier 2016; Mercier and Sperber 2017, 2011; Todd and Gigerenzer 2012; Johnson-Laird 2006), this ambiguity can still be seen in discussions of moral judgment (and moral dumbfounding), such that, for the rationalist perspective (see Haidt 2001), reasons appear to play a causal role, (e.g., Jacobson 2012, 17; Triskiel 2016, 93; Flanagan, Sarkissian, and Wong 2008, 7). Furthermore, this assumption is implicit in challenges to the dumbfounding narrative, whereby these challenges attempt to demonstrate that people do have “warrantable reasons” for their judgments (Royzman, Kim, and Leeman 2015, 309). Here we identify and address methodological limitations of one example of this type of challenge to the dumbfounding paradigm (Royzman, Kim, and Leeman 2015).\nGray, Schein, and Ward (2014) argue that people’s moral judgments are grounded in harm-based reasons, suggesting that when judging moral scenarios, people implicitly perceive harm even in scenarios that are construed as objectively harmless. If people perceive harm in the scenarios, then, even when the experimenter claims that they are harm free, this perception of harm still serves as a reason to condemn the behavior. They conducted a series of experiments demonstrating that people do implicitly perceive harm in supposedly victim-less scenarios; e.g., “masturbating to a picture of one’s dead sister, watching animals have sex to become sexually aroused, having sex with a corpse, covering a Bible with feces” (Gray, Schein, and Ward 2014, 1063). This suggests that in studies of moral dumbfounding people may also be making judgments based on an implicit perception of harm.\nJacobson (2012) makes specific reference to the scenarios used in the study of moral dumbfounding, and presents a number of plausible reasons why a person may condemn the actions of the characters in these scenarios. In the case of the Incest scenario, he suggests that the behavior of Julie and Mark was risky, “reckless and licentious” (Jacobson 2012, 25). Jacobson also discusses another scenario, Cannibal, that has been used in studies of moral dumbfounding. This scenario describes an act of cannibalism by a researcher in a pathology lab (Jennifer) on a cadaver from the lab. Jacobson argues that if Jennifer’s behavior became known, people would be less willing to donate their bodies to the lab. In addition to providing reasons that may explain the judgments of participants, Jacobson suggests that when participants appear to be dumbfounded they have simply given up on the argument and conceded to the experimenter who is in a position of authority. While this claim is not directly tested empirically by Jacobson, it has been studied by Royzman, Kim, and Leeman (2015), as discussed in the following section."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#evidence-for-judgments-based-on-reasons-or-principles",
    "href": "publications/reasons-or-rationalizations/index.html#evidence-for-judgments-based-on-reasons-or-principles",
    "title": "Reasons or Rationalisations:",
    "section": "1.3 | Evidence for Judgments Based on Reasons or Principles",
    "text": "1.3 | Evidence for Judgments Based on Reasons or Principles\nA recent series of studies by Royzman, Kim, and Leeman (2015), investigating the Incest scenario specifically, aimed to identify if participants presenting as dumbfounded genuinely had no reasons to support their judgments. In line with Jacobson (2012), they claim that dumbfounding occurs as a result of social pressure to adhere to conversational norms, arguing that dumbfounded participants do have reasons for their judgments and that these reasons are incorrectly dismissed as invalid by the experimenter. They argue that dumbfounded responding occurs as a result of social pressure to avoid appearing “uncooperative” (Royzman, Kim, and Leeman 2015, 299), “inattentive” or “stubborn” (2015, 300). In addition to this claim, Royzman, Kim, and Leeman (2015) identify two justifying principles that may be guiding participants’ judgments: the harm principle and the norm principle. They argue that when excluding from analysis participants who endorse either of these principles, incidences of dumbfounding are negligible.\nIn identifying the harm principle, Royzman, Kim, and Leeman (2015) draw on the work of Gray, Schein, and Ward (2014). They hypothesised that participants may not believe the scenario to be harm free even in the face of repeated assurances from the experimenter that it is harm free. If a participant does not believe that an act is truly harm free then this provides them with a perfectly valid reason to judge it as morally wrong (Gray, Schein, and Ward 2014; Royzman, Kim, and Leeman 2015). They devised two questions which served as a “credulity check” (Royzman, Kim, and Leeman 2015, 309), to assess whether or not participants believed that the Incest scenario was harm-free. The questions read as follows: (i) “Having read the story and considering the arguments presented, are you able to believe that Julie and Mark’s having sex with each other will not negatively affect the quality of their relationship or how they feel about each other later on?”; (ii) “Having read the story and considering the arguments presented, are you able to believe that Julie and Mark’s having sex with each other will have no bad consequences for them personally and/or for those close to them?” (Royzman, Kim, and Leeman 2015, 302–3). If participants responded “No” to either of these questions, their judgments were attributed to harm-based reasons, and therefore they could not be identified as dumbfounded.\nThe second principle identified by Royzman, Kim, and Leeman (2015) is the norm principle. They argue that if people believe that committing a particular act is wrong, regardless of the circumstances, then, for these people, this belief may be sufficient to serve as a reason to condemn the behavior of the characters in the scenario. Royzman, Kim, and Leeman (2015) presented participants with two statements: (a) “violating an established moral norm just for fun or personal enjoyment is wrong only in situations where someone is harmed as a result, but is acceptable otherwise”; (b) “violating an established moral norm just for fun or personal enjoyment is inherently wrong even in situations where no one is harmed as a result” (Royzman et al., 2015, p. 305). If participants endorsed (b) over (a) they reasoned that a judgment could be legitimately defended using a normative statement. They suggest that the “unsupported declarations” (Haidt, Björklund, and Murphy 2000, 12) identified by Haidt, Björklund, and Murphy (2000) are statements of a normative position, and that, rather than being a viewed as a dumbfounded response, they may be viewed as reasons for judgments.\nRoyzman, Kim, and Leeman (2015) used the credulity check to assess if participants’ judgments could be attributed to the harm principle, while attributing judgments to the norm principle was based on the norm statements. Royzman, Kim, and Leeman (2015) use the phrase “fully convergent” to describe participants who, in their view, are eligible for analysis (Royzman, Kim, and Leeman 2015, 306). According to Royzman, Kim, and Leeman (2015), a participant is fully convergent if their judgment cannot be attributed to either the harm principle or the norm principle. Using these stricter criteria for dumbfounding, Royzman, Kim, and Leeman (2015) initially identified 4 participants, from a sample of 53, who presented as dumbfounded. Each of these participants was then interviewed and the inconsistencies in their responses pointed out to them. During these interviews 2 participants changed their judgment of the behavior and 1 participant changed her position on the normative statements. This left just 1 fully convergent, dumbfounded participant. This participant did not resolve the inconsistency in his responses to the questions, and, following post-experiment interviews, Royzman and colleagues found dumbfounding to occur once in a sample of 53. This was found to be not significantly greater than 0 (Royzman, Kim, and Leeman 2015, 309), supporting the claim that moral dumbfounding is “highly irregular” or even “non-existent” (Royzman, Kim, and Leeman 2015, 300; see also Guglielmo 2018)."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#reasons-or-rationalisations",
    "href": "publications/reasons-or-rationalizations/index.html#reasons-or-rationalisations",
    "title": "Reasons or Rationalisations:",
    "section": "1.4 | Reasons or Rationalisations",
    "text": "1.4 | Reasons or Rationalisations\nThe studies conducted by Royzman, Kim, and Leeman (2015) introduce an additional level of methodological rigor to the study of moral dumbfounding. They clearly demonstrate that people will endorse a reason for a judgment if it is available to them. This undermines the dumbfounding narrative, that people defend a judgment in the absence of reasons, and poses a strong challenge to the existence of moral dumbfounding.\nWe (McHugh et al. 2017) have previously outlined some limitations with the conclusions presented by Royzman, Kim, and Leeman (2015). Firstly, Royzman, Kim, and Leeman (2015) suggest that people who present as morally dumbfounded do so in an attempt to avoid appearing “stubborn” or “inattentive” (2015, 310). However, Royzman, Kim, and Leeman (2015) also employ the original Haidt, Björklund, and Murphy (2000) definition of moral dumbfounding, which defines moral dumbfounding as “the stubborn and puzzled maintenance of a judgment without supporting reasons”” (Haidt, Björklund, and Murphy 2000, 2; see also Haidt and Björklund 2008, 197; Haidt and Hersh 2001, 194). This means that according to Royzman, Kim, and Leeman (2015), people who present as dumbfounded, paradoxically present as stubborn in an attempt to avoid appearing stubborn.\nSecondly, the means by which Royzman, Kim, and Leeman (2015) arrive at their estimate of 1 instance of moral dumbfounding out of a sample of 53 is problematic for the claim that moral dumbfounding occurs as a result of social pressure. They present their estimate of 1/53 as not significantly greater than 0/53 (z = 1, p = .315).1 However their original estimate of instances of moral dumbfounding was 4/53, which is significantly greater than 0/53 (z = 2.04, p = .041). These participants were invited back into the lab and the “inconsistencies” in their “responses were pointed out directly” to them (Royzman, Kim, and Leeman 2015, 308). Furthermore they were then “advised to carefully review and, if appropriate, revise” their responses (Royzman, Kim, and Leeman 2015, 308). This procedure subjected participants to social pressure to appear consistent in their responding. This illustrates that dumbfounded responding can be influenced by social pressure, however it does not support the stronger claim (by Royzman, Kim, and Leeman 2015) that dumbfounded responding can be attributed to social pressure (McHugh et al. 2017). The role of social pressure in eliminating instances of dumbfounded responding is not acknowledged by Royzman, Kim, and Leeman (2015).\nFinally, demonstrating that people endorse principles that are consistent with their judgments does not provide evidence that these principles are guiding their judgments. In relying on participants’ endorsing of a given principle to attribute their judgment to that principle, Royzman, Kim, and Leeman (2015) may have falsely excluded some participants from analysis. Consider the following scenario to illustrate this point:\n\nTwo friends (John and Pat) are bored one afternoon and trying to think of something to do. John suggests they go for a swim. Pat declines stating that it’s too much effort – to get changed, and then to get dried and then washed and dried again after; he says he’d rather do something that requires less effort. John agrees and adds “Oh yeah, and there’s that surfing competition on today so the place will be mobbed”. To which Pat replies “Yeah exactly!” (McHugh et al. 2017, 20)\n\nIt is clear from reading this scenario that even though he endorsed it to support or to rationalise his decision, the surfing competition was not the reason for John’s decision not to go to the beach. It would be incorrect to attribute his decision to this reason. The studies conducted by Royzman, Kim, and Leeman (2015) do not guard against the possibility of this type of false attribution, and it is likely that some participants were incorrectly excluded from analysis on this basis. This possibility of false exclusion presents a key limitation Royzman, Kim, and Leeman (2015) that casts doubt on their findings.\nWe suggest that attributing people’s judgments to principles requires stronger evidence than endorsing alone. We propose two measures that may be useful in establishing whether or not a given principle may truly be identified as a reason for the judgments made by participants. Firstly, participants should be given the opportunity to provide the reason(s) that they based their judgment on, and the reasons provided should inform decisions of inclusion or exclusion.2 Attributing participants’ judgments to particular reasons/principles should account for both the endorsing and the articulating of the reason/principle. Secondly, if a principle is guiding the judgments of participants, this principle should be applied consistently across different contexts. We predict that when these two measures are applied evidence for dumbfounding will be found."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#the-current-studies",
    "href": "publications/reasons-or-rationalizations/index.html#the-current-studies",
    "title": "Reasons or Rationalisations:",
    "section": "1.5 | The Current Studies",
    "text": "1.5 | The Current Studies\nThe aim of the current studies was to investigate whether or not people’s moral judgments can be attributed to moral principles based on their endorsing of these principles. Specifically, aim to address the concerns raised by McHugh et al. (2017) and test the claim by Royzman, Kim, and Leeman (2015) that participants’ judgments in the Incest scenario can be attributed to the harm principle or the norm principle. Firstly, the degree to which participants articulate either the harm principle or the norm principle as informing their judgment is examined (Study 1). Secondly, the consistency with which participants apply the harm principle across differing contexts is additionally assessed (Studies 2 and 3). We hypothesise that by developing more rigorous exclusion criteria the rates of false exclusion of participants would be reduced and that evidence for moral dumbfounding would be found, posing a challenge to the type of rationalist perspective described by Haidt (2001). The failure to identify dumbfounded responding would serve as support for these alternative perspectives (e.g., Gray, Schein, and Ward 2014; Jacobson 2012; Royzman, Kim, and Leeman 2015; Sneddon 2007; Wielenberg 2014; Guglielmo 2018) and pose a challenge to SIM as described by Haidt (2001). Given that the exclusion criteria used by Royzman, Kim, and Leeman (2015) were developed for the Incest dilemma, the studies reported here similarly focus on the Incest dilemma specifically."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#method",
    "href": "publications/reasons-or-rationalizations/index.html#method",
    "title": "Reasons or Rationalisations:",
    "section": "2.1 | Method",
    "text": "2.1 | Method\n\n2.1.1 | Participants and design\nStudy 1 was a frequency based extension of Royzman, Kim, and Leeman (2015). A combined sample of 110 (60 female, 49 male, 1 other; Mage = 32.44, min = 18, max = 69, SD = 11.28) took part. Fifty-eight (25 female, 32 male, 1 other; Mage = 38.47, min = 19, max = 69, SD = 12.34) were recruited through MTurk.3 Participation was voluntary and participants were paid 0.50 US dollars for their participation. Participants were recruited from English speaking countries or from countries where residents generally have a high level of English (e.g., The Netherlands, Denmark, Sweden). Fifty-two (35 female, 17 male; Mage = 25.71, min = 18, max = 38, SD = 3.8) were recruited through direct electronic correspondence. Participants in this sample were undergraduate students, postgraduate students, and alumni from Mary Immaculate College (MIC), and University of Limerick (UL). Participation was voluntary and participants did not receive a reward for their participation. Previous research on moral dumbfounding found responses from an MTurk sample and a College sample are largely comparable (see McHugh et al. 2017 Study 3a and 3b).\n\n\n2.1.2 | Procedure and materials\nData were collected using an online questionnaire generated using Questback (Unipark 2013). The questionnaire opened with the information sheet and consent form. The main questionnaire was only accessible once consent had been provided. Following the consent form, participants were presented with questions relating to basic demographics. Participants were then presented with two statements to assess if participants’ judgments may be grounded in the norm principle. These were taken directly from Royzman et al. (2015): (i) “violating an established moral norm just for fun or personal enjoyment is wrong only in situations where someone is harmed as a result, but is acceptable otherwise.”; (ii) “violating an established moral norm just for fun or personal enjoyment is inherently wrong even in situations where no one is harmed as a result.”. Participants read both statements and were asked to select the statement they “identify with the most”. The order of these statements was randomised. Participants who selected (ii) were then asked to elaborate on their position through an open-ended response question. The purpose of these statements was to assess participants’ own prior beliefs regarding moral judgment and justifications (see Royzman, Kim, and Leeman 2015, 331). In order to prevent the potentially confounding influence of a salient example moral scenario, these statements were presented before the moral judgment task.\nParticipants were then presented with the Incest vignette (Appendix A) from the original moral dumbfounding study (Haidt, Björklund, and Murphy 2000). They were asked to rate on a seven-point Likert scale how right or wrong they would rate the behavior of Julie and Mark (where, 1 = Morally wrong; 4 = Neutral; 7 = Morally right). They were asked to provide a reason for their judgment through open-ended response, and, rated their confidence in their judgment. Participants were then presented with a series of prepared counter-arguments designed to refute commonly used justifications for rating the behavior as “wrong” (Appendix B).\nDumbfounding was measured using a “critical slide” (developed by McHugh et al. 2017). The critical slide is a page in an online or computer based questionnaire specifically designed to measure dumbfounded responding. It contains a statement defending the behavior and a question as to how the behavior could be wrong (“Julie and Mark’s behavior did not harm anyone, how can there be anything wrong with what they did?”). There are three possible answer options: (a) “There is nothing wrong”; (b) an admission of not having reasons (“It’s wrong but I can’t think of a reason”); and finally a judgment with accompanying justification (c) “It’s wrong and I can provide a valid reason”. The order of these response options is randomised. Participants who select (c) are prompted on a following slide to type a reason. In line with McHugh et al. (2017), the selecting of option (b), the admission of not having reasons, was taken to be a dumbfounded response.\nFollowing the critical slide, participants rated the behavior, and rated their confidence in their judgment again. They also indicated, on a 7-point Likert scale, how much they changed their mind. A post-discussion questionnaire containing self-report reaction to the scenario across various dimensions (confidence, confusion, irritation, etc.) taken from Haidt, Björklund, and Murphy (2000) was administered after these revised judgments had been made (Appendix C).\nTwo targeted questions were taken directly from Royzman, Kim, and Leeman (2015) to assess whether or not participants’ judgments may be grounded in the harm principle: (i) “Having read the story and considering the arguments presented, are you able to believe that Julie and Mark’s having sex with each other will not negatively affect the quality of their relationship or how they feel about each other later on?”; (ii) “Having read the story and considering the arguments presented, are you able to believe that Julie and Mark’s having sex with each other will have no bad consequences for them personally and/or for those close to them?”. Participants responded “Yes” or “No” to each of these statements. The order of these questions was randomised.\nTwo other measures were also taken for exploratory purposes: Meaning in Life questionnaire [MLQ; Steger et al. (2008)]. This ten item scale is made up of two five item sub scales: presence (e.g., “I understand my life’s meaning.”) and search (e.g., “I am looking for something that makes my life feel meaningful.”). Responses were recorded using a 7-point Likert scale ranging from 1 (strongly disagree) to 7 (strongly agree); and CRSi7 a seven item scale taken from The Centrality of Religiosity Scale (Huber and Huber 2012). Participants responded to questions relating to the frequency with which they engage in religious or spiritual activity (e.g., “How often do you think about religious issues?”). Responses were recorded using a 5-point Likert scale ranging from 1 (never) to 5 (very often). The seven item inter-religious version of the scale was selected because some non-religious activities (such as meditation) may also have a bearing on a person’s ability to reason about moral issues."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#results-and-discussion",
    "href": "publications/reasons-or-rationalizations/index.html#results-and-discussion",
    "title": "Reasons or Rationalisations:",
    "section": "2.2 | Results and Discussion",
    "text": "2.2 | Results and Discussion\nEighty-seven of the total sample (N = 110; 79.09%) initially rated the behavior of Julie and Mark as wrong; no difference in initial rating between the MTurk sample (M = 1.98, SD = 1.52), and the MIC sample, (M = 2.1, SD = 1.39), t(107.94) = -0.409, p = .683, d = 0.0777881. Eighty-six of the total sample, (N = 110; 78.18%) rated the behavior as wrong after viewing the counter-arguments and the critical slide; no difference in revised rating between the MTurk sample, (M = 2, SD = 1.53), and the MIC sample, (M = 2.33, SD = 1.54), t(106.55) = -1.113, p = .268, d = 0.2125744. A paired samples t-test revealed a significant difference in rating of behavior from time one, initial rating, (M = 2.04, SD = 1.45), to time two, revised rating, (M = 2.15, SD = 1.54), t(109) = -2.384, p = .019, d = 0.078971. This result may be due to changes in the severity of the judgments as opposed to changing the judgment. Further analysis revealed that only eight (7.27%) participants changed their judgment: two participants changed their judgment from “wrong” to “neutral”; one participant changed their judgment from “right” to “neutral”; four changed their judgment from “neutral” to “right”; and one participant changed their judgment from “neutral” to “wrong”. A chi-square test for independence revealed no significant association between time of judgment and valence of judgment made, χ2(2, N = 220) = 0.731, p = .694, V = 0.0576344. This rate of changing judgments is lower than the 12% reported in Haidt, Björklund, and Murphy (2000), however, as noted above, social pressure appears to influence responses in the dumbfounding paradigm. It is likely that the lower rates of changing judgments can be attributed to the reduced social pressure in a computerized task.\nTen participants (9%) indicated that they had encountered the scenario before. When asked to elaborate, participants provided anecdotes, or referred to previous readings (either fiction or philosophy). Two participants (2%) indicated that they had encountered it in a previous survey. The low numbers mean that any potential influence of previous experience on the results is negligible and these participants were not excluded from the analyses.\n\n2.2.1 | Measuring dumbfounding\nParticipants who selected the admission of not having reasons on the critical slide were identified as dumbfounded. Rates of of each response to the critical slide are for the entire sample (N = 110) are displayed in Figure 1. Twenty participants (18.18%) were initially identified as dumbfounded.4 The exclusion criteria developed by Royzman, Kim, and Leeman (2015) were applied, all participants who endorsed either the harm principle or the norm principle were excluded from analysis. This left a sample of 14 participants who were eligible for analysis. None of these 14 selected the dumbfounded response.\nThe purpose of the Study 1 was to assess if participants could articulate the principles identified by Royzman, Kim, and Leeman (2015), independently of the targeted statements/questions, as these may serve as a prompt. A revised measure of convergence is developed here. A participant’s endorsement of either principle should lead to their exclusion from analysis, only if the participant also articulated this principle when given the opportunity. The open-ended responses were analysed and coded for any mention of either the harm principle or the norm principle. Participants were only excluded from analysis if they both endorsed and articulated either principle. For the purposes of consistency with Royzman, Kim, and Leeman (2015), unsupported declarations and tautological responses (identified as dumbfounded responses by McHugh et al. 2017) were coded as an articulation of the norm principle here.5 As predicted, the number of participants who both articulated and endorsed either principle was much lower than the number of participants who only endorsed either principle. Fifty two participants were eligible for analysis according to the revised exclusion criteria. Eight of these participants (15.3846154%) selected the dumbfounded response, providing some evidence for moral dumbfounding. Figure 1 shows the responses to the critical slide for the entire sample and for participants eligible for analysis according to each measure of convergence.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\nWarning: Use of `test$sample_perc` is discouraged.\nℹ Use `sample_perc` instead.\n\n\n\n\n\nResponses to critical slide for the entire sample, and for each measure of convergence: (i) endorsing only, and (ii), endorsing and articulating; percentages of full sample displayed within plot, percentages of relevant sample displayed in parenthesis below the count.\n\n\n\n\n\n\n2.2.2 | Consistency between endorsed principles and expressed judgments\nThe exclusion criteria developed by Royzman, Kim, and Leeman (2015) (endorsing only), led to a large proportion of participants who selected “There is nothing wrong” to be excluded from analysis (12 participants; 54.55% of the 22 participants who selected this option). Both the harm principle and the norm principle provide legitimate reasons for participants to judge the behavior as wrong (Royzman, Kim, and Leeman 2015). It follows that if a participant endorsed either principle, they would also judge the behavior as wrong. It is surprising then that, 12 of the 22 participants who selected “There is nothing wrong” on the critical slide, also endorsed either the harm principle or the norm principle. The endorsing of these principles meant that these participants were excluded from analysis on the grounds they had a legitimate reason to rate the behavior as wrong. However, these participants did not rate the behavior as wrong. This demonstrates an inconsistency between the endorsing of the principles through targeted questions and statements and the apparent use of these principles as reasons guiding the participants’ judgments. The endorsing only measure of convergence, using the targeted questions and statements developed by Royzman, Kim, and Leeman (2015) led to participants being falsely excluded from analysis.\nAccording to the revised criteria for exclusion, in which participants are only excluded from analysis if they were also able to articulate the principle that they endorsed, only one of the 22 participants (4.5454545%) who selected “There is nothing wrong” was excluded from analysis. The revised measure of convergence developed in Study 1 shows a reduced incidence of false exclusion of participants who selected “There is nothing wrong”. This suggests that accounting for both the articulating and the endorsing of principles provides more accurate (though still not quite perfect) exclusion criteria.\nThe aim of Study 1 was to extend previous research by Royzman, Kim, and Leeman (2015). They excluded participants from analysis based on their endorsing of either the harm principle or the norm principle through targeted questions/statements. Using these criteria for exclusion, they found minimal dumbfounded responding (1 participant from a sample of 53 (Royzman, Kim, and Leeman 2015, 309)). It was hypothesised that their exclusion criteria were too broad, and that participants’ endorsing of either principle does imply that participants can articulate the given principle. Revised criteria for exclusion were developed which accounted for both the endorsing and the articulation of either the harm principle or the norm principle. Our initial analysis replicated the findings of Royzman, Kim, and Leeman (2015).\nFurther analysis, using the revised measure of convergence demonstrated considerably more consistency in the exclusion/inclusion of participants who selected “There is nothing wrong”. These revised criteria identified eight (7.27% of the total sample of N = 110) participants as dumbfounded. Study 1 demonstrated inconsistency in the endorsing and articulation of the harm principle and the norm principle, and provided evidence for moral dumbfounding, however rates of dumbfounded responding were low, with the majority of participants (68; 61.8181818%) providing reasons for their judgments. A second study was devised to assess the consistency in the application of the harm principle across differing contexts, along with the endorsing, and articulation of the each principle."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#method-1",
    "href": "publications/reasons-or-rationalizations/index.html#method-1",
    "title": "Reasons or Rationalisations:",
    "section": "3.1 | Method",
    "text": "3.1 | Method\n\n3.1.1 | Participants and design\nStudy 2 was a frequency-based extension of Study 1. The aim was to investigate the prevalence of moral dumbfounding when controlling for (a) the consistency with which people articulate and endorse the norm principle and the harm principle, and (b) the consistency with which people apply the norm principle principle. A combined sample of 111 (67 female, 44 male; Mage = 34.23, min = 19, max = 74, SD = 11.42) took part.\nSixty-one (36 female, 25 male; Mage = 39.08, min = 20, max = 74, SD = 12.25) were recruited through MTurk. Participation was voluntary and participants were paid 0.50 US dollars for their participation. Participants were recruited from English speaking countries or from countries where residents generally have a high level of English (e.g., The Netherlands, Denmark, Sweden). Fifty (31 female, 19 male; Mage = 28.32, min = 19, max = 48, SD = 6.65) were recruited through direct electronic correspondence. Participants in this sample were undergraduate students, postgraduate students, and alumni from Mary Immaculate College (MIC), and University of Limerick (UL). Participation was voluntary and participants were not reimbursed for their participation.\n\n\n3.1.2 | Procedure and materials\nData were collected using an online questionnaire generated using Questback (Unipark 2013). The questionnaire in Study 2 was the same as that presented in Study 1, with the inclusion of three additional targeted questions which aimed to assess the consistency with which participants generalise and apply the harm principle. The questions were: (a) “How would you rate the behavior of two people who engage in an activity that could potentially result in harmful consequences for either of them?”; (b) “Do you think boxing is wrong?”; (c) “Do you think playing contact team sports (e.g. rugby; ice-hockey; American football) is wrong?”. Responses to (a) were recorded on a 7-point Likert scale (where, 1 = Morally wrong; 4 = Neutral; 7 = Morally right). Responses to (b) and (c) were recorded using a binary “Yes/No” option. These questions were presented sequentially, in randomised order. The randomised sequence was grouped as Block A. Similarly all slides and questions directly relating the moral scenario were grouped as Block B. Block B also included the targeted questions relating to the endorsing of the harm principle. The order of presentation of these blocks was randomised.\nAs with Study 1, the questionnaire opened with the information sheet, and the main body of the questionnaire could not be accessed until participants consented to continue. Once consent was given participants were asked a number of questions relating to basic demographics. They were then presented with the two targeted statements relating to the norm principle (in randomised order) and asked to select the statement they “identify with the most”. Participants were then presented with either Block A (containing the targeted questions relating to the application of the harm principle) or Block B (containing the moral scenario, related questions, and targeted questions relating to the endorsing of the harm principle). Following this participants were presented with the second block. As in Study 1, the questionnaire ended with the MLQ (Steger et al. 2008); and CRSi7 (Huber and Huber 2012)."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#results-and-discussion-1",
    "href": "publications/reasons-or-rationalizations/index.html#results-and-discussion-1",
    "title": "Reasons or Rationalisations:",
    "section": "3.2 | Results and Discussion",
    "text": "3.2 | Results and Discussion\nSeventy-nine of the total sample (N = 111; 71.17%) initially rated the behavior of Julie and Mark as wrong. An independent samples t-test revealed no difference in initial rating between the MTurk sample (M = 2.08, SD = 1.48), and the MIC sample, (M = 2.68, SD = 1.83), t(93.31) = 1.864, p = .066, d = 0.3632298. Sixty seven of the total sample, (N = 111; 60.36%) rated the behavior as wrong after viewing the counter-arguments and the critical slide. An independent samples t-test revealed a significant difference in revised rating between the MTurk sample, (M = 2.31, SD = 1.53), and the MIC sample, (M = 3, SD = 1.84), t(95.4) = 2.112, p = .037, d = 0.4102093. A paired samples t-test revealed a significant difference in rating of behavior from time one, initial rating, (M = 2.35, SD = 1.67), to time two, revised rating, (M = 2.62, SD = 1.54), t(110) = -3.474, p &lt; .001, d = 0.1602983. Further analysis revealed that although 15 participants changed their judgment, only two participants changed fully the valence of their judgment, changing their judgment from “wrong” to “right”. Of the other changes in judgment, ten participants changed their judgment from “wrong” to “neutral”; two participants changed their judgment from “right” to “neutral”; and one changed their judgment from “neutral” to “right”. A chi-square test for independence revealed no significant association between time of judgment and valence of judgment made, χ2(2, N = 222) = 3.3988504, p = .183, V = 0.1237341.\nEighteen participants (16%) indicated that they had encountered the scenario before. As in Study 1, when asked to elaborate, participants provided anecdotes, or referred to previous readings/TV (either fiction or philosophy), 8 participants (7%) indicated that they had encountered it in a previous survey. The number of participants indicating previous experience with the scenario was higher than in Study 1 and as such the possibility that it may have confounded the results was investigated. An independent samples t-test revealed no difference in judgment between participants who had previously seen the scenario, (M = 2.83, SD = 1.86), and participants who had not previously seen the scenario, (M = 2.26, SD = 1.62), t(22.31) = 1.228, p = .232, d = 0.3465786. Furthermore, a chi-squared test for independence revealed no significant association between previous experience with the scenario and response to the critical slide, χ2(2, N = 111) = 3.16, p = .206, V = 0.1686532. These participants were not excluded from the analyses.\n\n3.2.1 | Testing for order effects\nThe order of the blocks had no influence on the any of the responses of interest (see supplementary materials for details of analysis). Of the questions relating to the application of the harm principle, there were differences in responding to general question only (“How would you rate the behavior of two people who engage in an activity that could potentially result in harmful consequences for either of them?”). This question was more abstract than the two questions it appeared with, in which participants were asked to judge a named behavior (boxing or contact team sports). The description in the general question could apply to either of the named behaviors. Participants who responded to this question first rated the behavior as more wrong than participants who responded to it after reading one or both of the named behaviors. It seems likely that the named behaviors provided an example of a situation in which the behavior described in the general question may be acceptable, leading participants to respond more favorably to the general question.\n\n\n3.2.2 | Measuring dumbfounding\nAs in Study 1, participants who selected the admission of not having reasons on the critical slide were identified as dumbfounded. Rates of each response to the critical slide are for the entire sample (N = 111) are displayed in Figure 1. Twenty one participants (18.92%) were initially identified as dumbfounded.6 The exclusion criteria developed by Royzman et al. (2015; the endorsing of either principle) were applied, and this left a sample of 20 who were eligible for analysis. Two of these fully convergent participants selected the dumbfounded response. We then applied the revised criteria for exclusion (both articulating and endorsing either principle) developed in Study 1, and the number of participants eligible for analysis increased to 61. Of these, nine (14.75%) selected the dumbfounded response. Again this also led to a reduction in false exclusions, three 3 of the 36 (8.33) participants who selected “There is nothing wrong” were excluded by this measure.\nThe responses to the three targeted questions relating the application of the harm principle were analysed together. Only one participant was consistent in their application of the harm principle across all three targeted questions and this meant that only one participant was consistent in the application, articulation, and, endorsing of the harm principle (as measured by the open-ended responses and the targeted questions taken from Royzman, Kim, and Leeman (2015)). This was combined with the exclusion criteria developed in Study 1 leaving a sample of 73 participants who were eligible for analysis. Ten (9.01% of the total sample) of these participants selected the dumbfounded response. The responses to the critical slide across all measures of convergence used are displayed in Figure 2.\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\nWarning: Use of `test$sample_perc` is discouraged.\nℹ Use `sample_perc` instead.\n\n\n\n\n\nResponses to critical slide for the entire sample, and for each measure of convergence: (i) endorsing only, (ii) endorsing and articulating, and (iii), endorsing, articulating, and applying; percentages of full sample displayed within plot, percentages of relevant sample displayed in parenthesis below the count.\n\n\n\n\n\n\n3.2.3 | Consistency between endorsed principles and expressed judgments\nAs in Study 1, the initial criteria for exclusion (endorsing only) excluded a large proportion of the participants who selected “There is nothing wrong”; 20 of the 36 participants (55.56%) who selected “There is nothing wrong” were excluded. When articulation of the principles was accounted for, only three (8.33%) of these 36 participants were excluded. This is higher than in Study 1 (one participant, 4.55% of those who selected “There is nothing wrong”), however in reducing the obvious false exclusion of participants who selected “There is nothing wrong” it remains an improvement on the original criteria. This suggests that accounting for participants’ ability to articulate the principles endorsed provides a more accurate criteria for exclusion than accounting only for the endorsing of a given principle. Furthermore, when the applying of the harm principle was also accounted for, only one of the 36 participants who selected “There is nothing wrong” was excluded. The criteria for convergence developed here lead to greater consistency between a participant’s eligibility for analysis and their judgment made than the original criteria described by Royzman, Kim, and Leeman (2015).\nStudy 2 investigated the consistency with which people apply, articulate, and endorse the harm principle. Only one participant consistently applied, articulated, and endorsed the harm principle. As such, the harm principle as a basis for exclusion from analysis becomes practically redundant, and it seems unlikely that there is a generalised harm principle that underlies moral judgments (though does not rule out the possibility of more focused, content specific harm principles). The endorsing and articulation of the norm principle resulted in the exclusion of 37 participants. The degree to which the articulation or the endorsing of the norm principle may render participants ineligible for consideration as dumbfounded is unclear, this is discussed in more detail below. However, even if participants are excluded from analysis based on the norm principle, dumbfounded responding is still observed, with ten participants (13.7% of sample eligible for analysis; 9.01% of the total sample) selecting the admission of having no reason on the critical slide. As in Study 1, rates of observed dumbfounding are low, and providing reasons appears to be the preferred response, with more participants (54; 48.6486486%) providing reasons than selecting either of the other responses to the critical slide."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#method-2",
    "href": "publications/reasons-or-rationalizations/index.html#method-2",
    "title": "Reasons or Rationalisations:",
    "section": "4.1 | Method",
    "text": "4.1 | Method\n\n4.1.1 | Participants and design\nStudy 3 was a frequency-based replication of Study 2. The aim was to investigate the prevalence of moral dumbfounding when controlling for (a) the consistency with which people articulate and endorse the norm principle and the harm principle, and (b) the consistency with which people apply the norm principle principle. A total sample of 502 (287 female, 212 male; Mage = 39.05, min = 18, max = 81, SD = 12.46) took part. All participants were recruited through MTurk. Participation was voluntary and participants were paid 0.50 US dollars for their participation. Participants were recruited from English speaking countries or from countries where residents generally have a high level of English (e.g., The Netherlands, Denmark, Sweden).\n\n\n4.1.2 | Procedure and materials\nThe materials and procedure were identical to Study 2."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#results-and-discussion-2",
    "href": "publications/reasons-or-rationalizations/index.html#results-and-discussion-2",
    "title": "Reasons or Rationalisations:",
    "section": "4.2 | Results and Discussion",
    "text": "4.2 | Results and Discussion\nThree-hundred-and-seventy-nine of the total sample (N = 502; 75.5%) rated the behavior of Julie and Mark as wrong initially; and 357 participants, (N = 502; 71.12%) rated the behavior as wrong after viewing the counter-arguments and the critical slide. A paired samples t-test revealed a significant difference in rating of behavior from time one, initial rating, (M = 2.21, SD = 1.72), to time two, revised rating, (M = 2.38, SD = 1.79), t(501) = -4.736, p &lt; .001, d = 0.0954637. However a chi-square test for independence revealed no significant association between time of judgment and valence of judgment made, χ2(2, N = 1004) = 3.5866855, p = .166, V = 0.0845269.7\n\n4.2.1 | Testing for order effects\nAs in Study 2, the order of the blocks did influence on the any of the responses of interest, and the general harm question was the only question relating to the application of the harm principle that varied significantly with order (see supplementary materials for details of analysis). Again, it is likely that encountering a behaviour where harm may be acceptable (through the content of the other two questions), led participants to respond to the general question more favourably.\n\n\n4.2.2 | Measuring dumbfounding\nParticipants who selected the admission of not having reasons on the critical slide were identified as dumbfounded. This option was selected by 88 participants (17.53% of the entire sample N = 502).8\nThe exclusion criteria developed by Royzman et al. (2015; the endorsing of either principle) were applied, and this left a sample of 84 who were eligible for analysis. Of these, 9 participants selected the dumbfounded response.\nWe then applied the exclusion criteria developed in Study 1 (both articulating and endorsing either principle), and the number of participants eligible for analysis increased to 294. Of these, 52 (17.69%) selected the dumbfounded response.\nFinally, the exclusion criteria developed in Study 2 were applied, leaving a sample of 345 participants who were eligible for analysis; Sixty nine of whom (13.75% of the total sample) selected the dumbfounded response. The responses to the critical slide for the entire sample, and for each measure of convergence used are displayed in Figure @ref(fig:S3reasonsfig2).\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\nWarning: Use of `test$sample_perc` is discouraged.\nℹ Use `sample_perc` instead.\n\n\n\n\n\nResponses to critical slide for the entire sample, and for each measure of convergence: (i) endorsing only, (ii) endorsing and articulating, and (iii), endorsing, articulating, and applying; percentages of full sample displayed within plot, percentages of relevant sample displayed in parenthesis below the count.\n\n\n\n\n\n\n4.2.3 | Consistency between endorsed principles and expressed judgments\nAs in Studies 1 and 2, the exclusion criteria developed here resulted in fewer false exclusions. In the current study, the exclusion criteria developed by Royzman et al. (2015, endorsing only), led to 66 of the 125 participants who selected “There is nothing wrong” being excluded from analysis (52.8%). Conversely, applying the exclusion criteria developed in Study 1 resulted in seven of these 125 participants being excluded (5.6%); and the exclusion criteria from Study 2 resulted in six of these 125 participants being excluded (4.8%).\nFurther analysis, using the revised measure of convergence demonstrated considerably more consistency in the exclusion/inclusion of participants who selected “There is nothing wrong”. These revised criteria identified sixty-nine (20% of the total eligible sample of N = 345) participants as dumbfounded. Study 1 provided evidence for moral dumbfounding and demonstrated inconsistency in the endorsing and articulation of the harm principle and the norm principle, a second study was devised to assess the consistency in the application of the harm principle across differing contexts, along with the endorsing, and articulation of the each principle. Study 3 replicated the findings of both Studies 1 and 2 with a larger sample. By applying our revised exclusion criteria, we found clear evidence for the existence of moral dumbfounding, though observed rates of dumbfounding were low, with the majority of participants (157; 45.5072464%) providing reasons.\nThe analyses of the individual difference variables are reported in the Supplementary Materials (Appendix D)."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#the-norm-principle-and-unsupported-declarations",
    "href": "publications/reasons-or-rationalizations/index.html#the-norm-principle-and-unsupported-declarations",
    "title": "Reasons or Rationalisations:",
    "section": "5.1 | The Norm Principle and Unsupported Declarations",
    "text": "5.1 | The Norm Principle and Unsupported Declarations\nIn all three studies, unsupported declarations were coded as an articulation of the norm principle, and therefore not taken as dumbfounded responses. However, in previous work, we identified parallels between the providing of unsupported declarations and the providing of admissions of not having reasons [similar proportion of time spent (a) smiling/laughing, (b) in silence; see McHugh et al. (2017)]. There is also a strong theoretical case for the inclusion of unsupported declarations as dumbfounded responses. Propositional beliefs/deontological judgments may be viewed as habitual/model-free intuitions (e.g., Crockett 2013; Cushman 2013a). The reasons for these judgments are independent of the intuition. Stating the content of the intuition, is not the same as providing a reason for the intuition. Royzman, Kim, and Leeman (2015) argue that endorsing the propositional belief is sufficient evidence of that belief playing an influential role in relevant judgments, however, this is holding participants to a different standard. There is a difference between having a reason for an intuition/propositional belief and claiming the direct basis for a judgment is an associated propositional belief. In view of this, it is possible that by not including unsupported declarations or tautological reasons as dumbfounded responses, the rates of dumbfounding reported here are not representative of the phenomenon, providing instead an overly conservative estimate. However, even according to this stricter measure adopted here, evidence for dumbfounding was found."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#consistency-between-endorsed-principles-and-expressed-judgments-3",
    "href": "publications/reasons-or-rationalizations/index.html#consistency-between-endorsed-principles-and-expressed-judgments-3",
    "title": "Reasons or Rationalisations:",
    "section": "5.2 | Consistency Between Endorsed Principles and Expressed Judgments",
    "text": "5.2 | Consistency Between Endorsed Principles and Expressed Judgments\nThe most convincing evidence that the exclusion criteria developed in these studies are more accurate than the criteria proposed by Royzman, Kim, and Leeman (2015) is the greater consistency between valence of judgment and eligibility for analysis. Participants’ eligibility for analysis is determined by whether or not their judgment can be attributed to either the harm principle or the norm principle. If a participant’s judgment can be attributed to a given principle, this participant is deemed to have a reason for their judgment and they cannot be identified as dumbfounded (rendering them ineligible for analysis). In order for a judgment to legitimately be attributed to a particular principle, it is necessary that the valence of the judgment is consistent with what is predicted by the application of that principle. In the case of both principles, applying either the harm principle or the norm principle (as described by Royzman, Kim, and Leeman 2015) results in the behavior being judged as wrong. This means that the judgments of participants who selected “There is nothing wrong” cannot be attributed to either principle. Any participants who are excluded from analysis but selected “There is nothing wrong”, are clearly identifiable as being falsely excluded from analysis such that this may be used as a measure of the relative accuracy of the different exclusion criteria employed.\nAccording to Royzman, Kim, and Leeman (2015), a participant’s judgment can be attributed to a given principle if they endorse this principle. However, in each of the studies reported here, excluding participants based on the endorsing of a principle resulted in over half of the participants who selected “There is nothing wrong” to be falsely excluded from analysis; participants’ judgments were incorrectly attributed to either the harm principle or the norm principle (12 of the 22 participants who selected “There is nothing wrong” in Study 1 were falsely excluded 54.55%; 20 of the 36 participants who selected “There is nothing wrong” in Study 2 were falsely excluded 55.56%; and 66 of the 125 participants who selected “There is nothing wrong” in Study 3 were falsely excluded 52.8%). This suggests that the endorsing of a principle is a flawed indicator of the degree to which the principle is guiding participants’ judgments.\nWe made two changes to the exclusion criteria that aimed to reduce the numbers of participants being falsely excluded from analysis. We hypothesised that providing participants with an opportunity to articulate the reasons for their judgment would more accurately identify the principles that guided participants’ judgments than their endorsing of particular principles. This was found to be the case; in Study 1, only one of the 22 participants who selected “There is nothing wrong” was falsely excluded from analysis; in Study 2 only three of the 36 participants who selected “There is nothing wrong” were falsely excluded from analysis; and in Study 3 seven of the 125 participants who selected “There is nothing wrong” were falsely excluded from analysis. Taking participants’ articulating of the reasons for their judgments into account reduced measurable rate of false exclusion from 54.55% to 4.55% in Study 1; 55.56% to 8.33% in Study 2; and 52.8% to 5.6% in Study 3. Furthermore, in Studies 2 and 3, with specific reference to the harm principle, we hypothesised that assessing the degree to which people’s judgments could be attributed to the harm principle would be related to whether or not they apply the harm principle across different contexts. Again this was found to be the case, as evidenced by a further reduction in the measurable rate of false exclusion from 8.33% (3/36) to 2.78% (1/36) in Study 2, and from 5.6% (7/125) to 4.8% (6/125) in Study 3."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#implications",
    "href": "publications/reasons-or-rationalizations/index.html#implications",
    "title": "Reasons or Rationalisations:",
    "section": "5.3 | Implications",
    "text": "5.3 | Implications\nThe existence of moral dumbfounding and the associated support for intuitionist theories of moral judgment (e.g. Cushman, Young, and Greene 2010; Haidt 2001; Hauser, Young, and Cushman 2008; Prinz 2005; see also Crockett 2013; Cushman 2013a; Greene 2008, 2013) has been questioned in recent years. The majority of these challenges are theoretical (e.g., Jacobson 2012; Sneddon 2007; Wielenberg 2014). The work of Gray, Schein, and Ward (2014), appeared to give some empirical weight to these challenges, while Royzman, Kim, and Leeman (2015) extended these challenges to the dumbfounding paradigm specifically. We conducted three studies addressing specific methodological limitations associated with the work by Royzman, Kim, and Leeman (2015). Their criteria for exclusion were found to be overly liberal, as evidenced by the high rates of false exclusion of participants who selected “There is nothing wrong”. and evidence for dumbfounding was found. Adopting the more rigorous exclusion criteria developed here led to a reduction in the false exclusion of participants. In using these criteria, evidence for dumbfounding was found, and the explanation of dumbfounded responding proposed by Royzman, Kim, and Leeman (2015) was not supported.\nOur findings provide further evidence that the distinction between implicit and explicit cognition (e.g., Bonner and Newell 2010; Evans 2003, 2006, 2008; Evans and Over 2013; Reber 1989) extends to the moral domain. It has long been known that people have poor introspective awareness of how judgments are made (e.g., Nisbett and Wilson 1977) and it appears that in some cases this may also be true for moral judgments."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#limitations-and-future-directions",
    "href": "publications/reasons-or-rationalizations/index.html#limitations-and-future-directions",
    "title": "Reasons or Rationalisations:",
    "section": "5.4 | Limitations and Future Directions",
    "text": "5.4 | Limitations and Future Directions\nThe research we present here consists of three studies with a combined sample of N = 723, from MTurk (N = 621) and third level institutions (N = 102). Follow-up studies should investigate the phenomenon with larger and more diverse samples. Such follow-up work may inform investigations into the influence of cultural and societal norms on the prevalence of moral dumbfounding. Previous work by Haidt and Hersh (2001) provides suggestive evidence that political orientation may influence a person’s susceptibility to moral dumbfounding; furthermore, there is some evidence to indicate that cultural and socio-economic factors may also play a role (Haidt, Koller, and Dias 1993). Future research should draw on the methods developed here and by both McHugh et al. (2017) and Royzman, Kim, and Leeman (2015) to investigate these influences further.\nThe procedures we used were very similar across both studies. They were also very similar to those used by McHugh et al. (2017) and by Royzman, Kim, and Leeman (2015). A more rigourous test of moral dumbfounding should employ a variety of methods. We recommend that future research develops a broader selection of “dumbfounding scenarios”, and investigate the feasibility of alternative procedures that may elicit dumbfounding.\nThe role of social pressure and conversational norms in the emergence of moral dumbfounding is not well understood. The studies described here were conducted using online surveys and therefore there was no immediate social pressure on participants to either appear consistent or to conform to conversational norms. Furthermore, the argument proposed by Royzman, Kim, and Leeman (2015), that participants’ judgment are grounded in reasons (harm-based/norm-based) and that they drop these reasons in response to social pressure is not supported by the evidence presented here; harm-based/norm based reasons were not consistently articulated or applied by participants in these studies. It is apparent then that dumbfounded responding cannot be attributed to social pressure alone. The processes by which we make moral judgments also give rise to moral dumbfounding. This means that isolating the underlying mechanisms that give rise to moral dumbfounding may contribute to our overall understanding of the making of moral judgments."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#study-2-test-for-order-effects",
    "href": "publications/reasons-or-rationalizations/index.html#study-2-test-for-order-effects",
    "title": "Reasons or Rationalisations:",
    "section": "Study 2: Test for Order Effects",
    "text": "Study 2: Test for Order Effects\nRecall that the questions were blocked for randomisation. Tests for effects of the order of the blocks revealed no difference in initial rating, t(106.87) = -1.64, p = .104, d = 0.2949823; no difference in responding to the critical slide, \\(\\chi\\)2(2, N = 111) = 4.76, p = .093, V = 0.21; and no difference in response to the generic potential harm question (“How would you rate the behavior of two people who engage in an activity that could potentially result in harmful consequences for either of them?”), t(85.4) = -1.02, p = .312, d = 0.1999462. A chi-squared test for independence revealed no significant association between order of blocks and judgments of boxing, \\(\\chi\\)2(1, N = 111) = 2.86, p = .091, V = 0.16, or the question regarding contact team sports, \\(\\chi\\)2(1, N = 111) = 0.19, p = .660, V = 0.04.\nThe order of the questions regarding the application of the harm principle was also randomised. A one-way ANOVA revealed a significant difference in responses to the question “How would you rate the behavior of two people who engage in an activity that could potentially result in harmful consequences for either of them?” (1 = Extremely wrong; 4 = Neutral; 7 = Extremely right) depending on when it was presented F(2, ,, , 109) = 4.757 p = .010, partial \\(\\eta\\)2 = .080. Tukey’s post-hoc pairwise revealed that, when this question was responded to first, participants ratings were significantly lower (M = 2.8, SD = 1.43) than when it was responded to second (M = 3.57, SD = 1.21), p = .040, or third (M = 3.67, SD = 1.31) , p = .014; and there was no difference in responding to this question second (M = 3.57, SD = 1.21) or third (M = 3.67, SD = 1.31), p = .932.\nA chi-squared test for independence revealed no significant association between order these questions and responses to the question “Do you think boxing is wrong?”, \\(\\chi\\)2(2, N = 111) = 4.88, p = .087, V = 0.21. Similarly, a chi-squared test for independence revealed a significant association between order these questions and responses to the question “Do you think playing contact team sports (e.g. rugby; ice-hockey; American football) is wrong?”, \\(\\chi\\)2(2, N = 111) = 1.79, p = .409, V = 0.13."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#study-3-test-for-order-effects",
    "href": "publications/reasons-or-rationalizations/index.html#study-3-test-for-order-effects",
    "title": "Reasons or Rationalisations:",
    "section": "Study 3: Test for Order Effects",
    "text": "Study 3: Test for Order Effects\nAs in Study 2, the questions were blocked for randomisation. Tests for effects of the order of the blocks revealed no difference in initial rating, t(465.55) = 1.76, p = .079, d = 0.1591698; no difference in responding to the critical slide, \\(\\chi\\)2(2, N = 502) = 1.12, p = .570, V = 0.05; no difference in responses to the generic potential harm question, t(443.45) = 0.99, p = .322, d = 0.0903016. no association with judgments of boxing, \\(\\chi\\)2(1, N = 502) = 1.03, p = .310, V = 0.05, or the question regarding contact team sports, \\(\\chi\\)2(1, N = 502) = 1.15, p = .283, V = 0.1, depending on order of blocks.\nRegarding the three questions assessing the application of the harm principle, a one-way ANOVA revealed a significant difference in responses to the generic potential harm question depending on when it was presented F(2, ,, , 499) = 23.512 p &lt; .001, partial \\(\\eta\\)2 = .086. Tukey’s post-hoc pairwise revealed that, when this question was responded to first, participants ratings were significantly lower (M = 2.6, SD = 1.46) than when it was responded to second (M = 3.5, SD = 1.44), p &lt; .001, or third (M = 3.47, SD = 1.2) , p &lt; .001; and there was no difference in responding to this question second (M = 3.5, SD = 1.44) or third (M = 3.47, SD = 1.2), p = .983. As in Study 2, it seems likely that the named behaviours in the other questions provide an example of potential harm that is acceptable, leading to a more favourable response to this more abstract question. There was no significant association between question order and responses to the question “Do you think boxing is wrong?”, \\(\\chi\\)2(2, N = 502) = 1.12, p = .570, V = 0.05; or “Do you think playing contact team sports (e.g. rugby; ice-hockey; American football) is wrong?”, \\(\\chi\\)2(1, N = 502) = 1.03, p = .310, V = 0.05."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#study-3-individual-differences",
    "href": "publications/reasons-or-rationalizations/index.html#study-3-individual-differences",
    "title": "Reasons or Rationalisations:",
    "section": "Study 3: Individual Differences",
    "text": "Study 3: Individual Differences\nA series of logistic regressions were conducted to investigate if dumbfounded responding was related to any of the individual difference variables Religiosity (as measured by CRSi7 Huber and Huber 2012), or Meaning in Life (Presence and Search, measured using MLQ Steger et al. 2008). We first report the results for each variable individually, followed by the combined model.\n\nReligiosity\nThe overall mean Religiosity score was M = 2.57, SD = 1.17. The mean religiosity scores for participants depending on response to the critical slide were as follows: M = 2.84, SD = 1.17 for participants who provided reasons, M = 2.42, SD = 1.11 for participants who were dumbfounded, and M = 2.28, SD = 1.12 for participants who selected “There is nothing wrong”.\nA multinomial logistic regression revealed a statistically significant association between Religiosity and response to the critical slide, \\(\\chi\\)2(2, N = 502) = 17.38, p &lt; .001, The observed power was 0.97. Religiosity explained approximately 2.4% (McFadden R square) of the variance in responses to the critical slide. Participants with higher religiosity scores were significantly more likely to provide reasons than to present as dumbfounded, Wald = 6.14, p = .013, odds ratio = 0.7292316, 95% CI [0.5680613, 0.9361292], or select “There is nothing wrong” Wald = 15.24, p &lt; .001, odds ratio = 0.6511987, 95% CI [0.5250259, 0.807693]. See Figure @ref(fig:adREggplotlogit1).\n\n\n\n\n\nProbability of selecting each response to the critical slide depending on Religiosity\n\n\n\n\n\n\nMeaning in Life (Presence)\nThe overall mean Meaning in Life (Presence) score was M = 4.74, SD = 1.66. The mean Meaning in Life (Presence) scores for participants depending on response to the critical slide were as follows: M = 5.01, SD = 1.67 for participants who provided reasons, M = 4.35, SD = 1.42 for participants who were dumbfounded, and M = 4.62, SD = 1.73 for participants who selected “There is nothing wrong”.\nA multinomial logistic regression revealed a statistically significant association between Meaning in Life (Presence) and response to the critical slide, \\(\\chi\\)2(2, N = 345) = 8.46, p = .015, The observed power was 0.74. Meaning in Life explained approximately 1.17% (McFadden R square) of the variance in responses to the critical slide. Participants with higher MLQ: presence scores were significantly more likely to provide reasons than to present as dumbfounded, Wald = 7.46, p = .006, odds ratio = 0.7876735, 95% CI [0.6637247, 0.9347693]. (Participants with higher MLQ: presence scores were marginally more likely to provide reasons than to select “There is nothing wrong” Wald = 3.77, p = .052, odds ratio = 0.8635496, 95% CI [0.744727, 1.0013305].) See Figure @ref(fig:adMLQPggplotlogit).\n\n\n\n\n\nProbability of selecting each response to the critical slide depending on MLQ: Presence\n\n\n\n\n\n\nMeaning in Life (Search)\nThe overall mean Meaning in Life (Search) score was M = 4.47, SD = 1.73. The mean Meaning in Life (Search) scores for participants depending on response to the critical slide were as follows: M = 4.42, SD = 1.75 for participants who provided reasons, M = 4.55, SD = 1.68 for participants who were dumbfounded, and M = 4.49, SD = 1.73 for participants who selected “There is nothing wrong”.\nA multinomial logistic regression revealed no statistically significant association between Search for Meaning in Life and response to the critical slide, \\(\\chi\\)2(2, N = 345) = 0.3, p = .859, The observed power was 0.07. See Figure @ref(fig:adMLQSggplotlogit).\n\n\n\n\n\nProbability of selecting each response to the critical slide depending on MLQ: Search\n\n\n\n\n\n\n(#tab:adlogittable)\n\n\nMultinomial logistic regression predicting responses to the critical slide where providing reasons is the referent in each case.\n\n\n\n\nVariable\nResponse\n\nS.E.\nWald\n\n\nO.R.\nLower\nUpper\n\n\n\n\nReligiosity\nDumbfounded\n-0.247\n0.141\n3.045\n6\n.081\n0.781\n0.592\n1.031\n\n\n\nNothing wrong\n-0.426\n0.119\n12.899\n6\n&lt;.001**\n0.653\n0.518\n0.824\n\n\nMLQ: Presence\nDumbfounded\n-0.173\n0.095\n3.29\n6\n.070\n0.841\n0.698\n1.014\n\n\n\nNothing wrong\n-0.043\n0.082\n0.275\n6\n.600\n0.958\n0.815\n1.125\n\n\nMLQ: Search\nDumbfounded\n0.054\n0.09\n0.358\n6\n.522\n1.055\n0.885\n1.259\n\n\n\nNothing wrong\n0.074\n0.076\n0.95\n6\n.207\n1.077\n0.928\n1.25\n\n\n\n\nNote. * = sig. at &lt; .05; ** = sig. at &lt; .001\n\n \n\n\n\nIndividual Differences\nWhen analysed together, a multinomial logistic regression revealed a statistically significant association between the three individual difference variables and response to the critical slide, \\(\\chi\\)2(6, N = 345) = 22.15, p = .001, The observed power was 0.99. The model explained approximately 3.07% (McFadden R square) of the variance in responses to the critical slide. Religiosity was the only significant predictor (see Table @ref(tab:adlogittable)). Participants who scored higher in Religiosity were significantly more likely to provide reasons than to select “There is nothing wrong”, Wald = 12.899, p , odds ratio = 0.653, 95% CI [0.518, 0.518]. It seems religiosity was more related to valence of judgement than to ability to provide reasons Wald = 3.045, p , odds ratio = 0.781, 95% CI [0.592, 0.592].\nA linear regression was conducted to assess the relationship between the individual difference variables (Religiosity, Meaning and Life Presence, Meaning in Life Search) and initial judgement. The model significantly predicted valence of judgement, \\(R^2 = .04\\), \\(F(3, 497) = 6.22\\), \\(p &lt; .001\\). Religiosity the only significant predictor, \\(b = -0.21\\), 95% CI \\([-0.35, -0.07]\\), \\(t(497) = -3.01\\), \\(p = .003\\) (MLQ: presence \\(b = -0.08\\), 95% CI \\([-0.18, 0.03]\\), \\(t(497) = -1.44\\), \\(p = .152\\); MLQ: search \\(b = 0.06\\), 95% CI \\([-0.03, 0.15]\\), \\(t(497) = 1.25\\), \\(p = .212\\)). Participants who scored higher in Religiosity were more likely to condemn the actions of Julie and Mark.\n\n\n(#tab:adlogittable2)\n\n\nMultinomial logistic regression predicting responses to the critical slide where providing reasons is the referent in each case.\n\n\n\n\nVariable\nResponse\n\nS.E.\nWald\n\n\nO.R.\nLower\nUpper\n\n\n\n\nReligiosity\nDumbfounded\n-0.236\n0.147\n2.588\n8\n.108\n0.79\n0.593\n1.053\n\n\n\nNothing wrong\n-0.875\n0.23\n14.524\n8\n&lt;.001**\n0.417\n0.266\n0.654\n\n\nMLQ: Presence\nDumbfounded\n-0.155\n0.099\n2.453\n8\n.117\n0.856\n0.705\n1.04\n\n\n\nNothing wrong\n0.031\n0.139\n0.05\n8\n.823\n1.031\n0.786\n1.353\n\n\nMLQ: Search\nDumbfounded\n0.039\n0.092\n0.179\n8\n.672\n1.04\n0.868\n1.245\n\n\n\nNothing wrong\n0.031\n0.136\n0.05\n8\n.823\n1.031\n0.789\n1.347\n\n\nInitial Judgement\nDumbfounded\n0.39\n0.139\n7.821\n8\n.005*\n1.477\n1.124\n1.942\n\n\n\nNothing wrong\n1.98\n0.219\n81.533\n8\n&lt;.001**\n7.241\n4.712\n11.128\n\n\n\n\nNote. * = sig. at &lt; .05; ** = sig. at &lt; .001\n\n \n\nA final multinomial logistic regression was conducted that included Initial Judgement as a predictor variable. The results are shown in Table @ref(tab:adlogittable2). Overall the model was a significant predictor of response to the critical slide, \\(\\chi\\)2(8, N = 345) = 292.33, p &lt; .001, The observed power was 1. The model explained approximately 40.54% (McFadden R square) of the variance in responses to the critical slide. As shown in Table @ref(tab:adlogittable2), Religiosity appeared to be related only to valence of judgement on the critical slide, initial judgement appeared to predict valence of judgement and ability to provide reasons, with more extreme judgements of “wrong” most strongly predicting the providing of reasons. The relative probabilities of selecting each response to the critical slide depending on initial judgement are displayed in Figure @ref(fig:adggplotlogit1).\n\n\n\n\n\nProbability of selecting each response to the critical slide depending on Initial Judgement."
  },
  {
    "objectID": "publications/reasons-or-rationalizations/index.html#footnotes",
    "href": "publications/reasons-or-rationalizations/index.html#footnotes",
    "title": "Reasons or Rationalisations:",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNo explanation for the responding of this participant is offered. Neither can this participant’s response be explained by the theoretical position adopted by Royzman, Kim, and Leeman (2015).↩︎\nParticipants in Royzman, Kim, and Leeman (2015) provided reasons however these reasons did not inform their exclusion criteria.↩︎\nIn order to prevent repeat participation from MTurk workers, this study and all remaining studies conducted on MTurk, were included as part of the same MTurk project as Study 3b from McHugh et al. (2017). In addition, a probe question was included to check if participants had encountered the scenario before. This probe included a follow-up question to determine the nature of participants’ previous experience with the scenario.↩︎\nUnsupported declarations and tautological responses provided in the open-ended responses resulted in an additional six participants presenting as potentially dumbfounded; given that Royzman, Kim, and Leeman (2015) argue that these responses are an articulation of a norm/principle, these participants are not identified as dumbfounded here.↩︎\nBy only identifying participants who explicitly admittied to not having a reason as dumbfounded we also reduced the potential risk of “false inclusions”, where people provide a dumbfounded response through laziness or inattentiveness. While the motivations for selecting various responses cannot be known, previous research has identified the selecting of an admission of not having reasons as a conservative indicator of moral dumbfounding (McHugh et al. 2017, 16).↩︎\nUnsupported declarations and tautological responses provided in the open-ended responses resulted in an additional six participants presenting as potentially dumbfounded; again, these participants are not identified as dumbfounded here.↩︎\nFurther analysis revealed that 42 participants changed their judgment, only seven participants changed fully the valence of their judgment, with five changing their judgment from “wrong” to “right”, and two changing their judgement from “right” to “wrong”. Of the other changes in judgment, twenty two participants changed their judgment from “wrong” to “neutral”; six participants changed their judgment from “right” to “neutral”; and four changed their judgment from “neutral” to “right”.↩︎\nUnsupported declarations and tautological responses provided in the open-ended responses resulted in an additional 50 participants presenting as potentially dumbfounded; again, these participants are not identified as dumbfounded here.↩︎"
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html",
    "href": "publications/searching-for-moral-dumbfounding/index.html",
    "title": "Searching for Moral Dumbfounding:",
    "section": "",
    "text": "Moral dumbfounding is defined as maintaining a moral judgement, without supporting reasons. The most cited demonstration of dumbfounding does not identify a specific measure of dumbfounding and has not been published in peer-review form, or directly replicated. Despite limited empirical examination, dumbfounding has been widely discussed in moral psychology. The present research examines the reliability with which dumbfounding can be elicited, and aims to identify measureable indicators of dumbfounding. Study 1 aimed at establishing the effect that is reported in the literature. Participants read four scenarios and judged the actions described. An Interviewer challenged participants’ stated reasons for judgements. Dumbfounding was evoked, as measured by two indicators, admissions of not having reasons (17%), unsupported declarations (9%) with differences between scenarios. Study 2 measured dumbfounding as the selecting of an unsupported declaration as part of a computerised task. We observed high rates of dumbfounding across all scenarios. Studies 3a (college sample) and 3b (MTurk sample), addressing limitations in Study 2, replaced the unsupported declaration with an admission of having no reason, and included open-ended responses that were coded for unsupported declarations. As predicted, lower rates of dumbfounding were observed (3a 20%; 3b 16%; or 3a 32%; 3b 24% including unsupported declarations in open-ended responses). Two measures provided evidence for dumbfounding across three studies; rates varied with task type (interview/computer task), and with the particular measure being employed (admissions of not having reasons/unsupported declarations). Possible cognitive processes underlying dumbfounding and limitations of methodologies used are discussed as a means to account for this variability."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#method",
    "href": "publications/searching-for-moral-dumbfounding/index.html#method",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Method",
    "text": "Method\n\n\nParticipants and design\n\n\nStudy 1 was a frequency based attempted replication. The aim was to identify if dumbfounded responding could be evoked. All participants were presented with the same four moral vignettes. Results are primarily descriptive. Any further analysis tested for differences in responding depending on the vignette, or type of vignette, presented.\n\n\nA sample of 31 participants (15 female, 16 male) with a mean age of Mage = 28.83 (min = 19, max = 64, SD = 10.99) took part in this study. Participants were undergraduate students, postgraduate students, and alumni from Mary Immaculate College (MIC), and University of Limerick (UL). Participation was voluntary and participants were not reimbursed for their participation.\n\n\n\nProcedure and materials\n\nFour moral judgement vignettes were used (Appendix A). Three of the vignettes (Heinz, Incest, and Cannibal) were taken from Haidt, Björklund, and Murphy (2000). Incest was taken directly from the original study however Cannibal and Heinz were modified slightly, following piloting.\n\n\nThe original version of Cannibal stated that people had “donated their body to science for research”; participants during piloting were able to argue that eating does not constitute “research”. In order to remove this as a possible argument, the modified version stated that bodies had been donated for “the general use of the researchers in the lab” and that the “bodies are normally cremated, however, severed cuts may be disposed of at the discretion of lab researchers”.\n\n\nSimilarly, piloting suggested that participants agreed with the actions of Heinz and condemned the actions of the druggist. The original wording of Heinz suggested that any discussion related to Heinz as opposed to the druggist meaning that, for Heinz, participants would typically be defending an approval of the character’s actions. However, for Incest and Cannibal participants generally condemn the actions of the character and as such are defending a judgement of “morally wrong”. In order to ensure that participants were consistently defending a judgement of “morally wrong” across all scenarios, Heinz was modified to include “The druggist had Heinz arrested and charged”. Any discussion on Heinz then related to the character whose behaviour participants thought was wrong.\n\n\nIn the original study by Haidt, Björklund, and Murphy (2000), Incest and Cannibal are presented as “intuition” stories, and contrasted against a single “reasoning” dilemma: Heinz. In order for a more balanced comparison, a bridge variant of the classic trolley dilemma (Trolley) was included as a second “reasoning” dilemma. In this vignette, participants judge the actions of Paul, who pushes a large man off a bridge to stop a trolley and save five lives. The inclusion of Trolley meant that there were two “reasoning” dilemmas to be contrasted with the two “intuition” stories.\n\n\nSample counter arguments were prepared for each scenario. To ensure that participants were only pushed to defend a judgement of “morally wrong” these counter arguments exclusively defended the potentially questionable behaviour of the characters. A list of prepared counter arguments can be seen in Appendix B. A post-discussion questionnaire, taken from Haidt, Björklund, and Murphy (2000) was administered after discussion of each scenario (Appendix C).\n\n\nTwo other measures were also taken for exploratory purposes.: Firstly, in response to a possible link between meaning and morality (e.g., Bellin 2012; Schnell 2011), the Meaning in Life questionnaire (MLQ; Steger et al. 2008) was included. This ten item scale, is made up of two five item sub scales: presence (e.g., “I understand my life’s meaning”) and search (e.g., “I am looking for something that makes my life feel meaningful”). Responses were recorded using a seven point Likert scale ranging from 1 (strongly disagree) to 7 (strongly agree). Secondly, in line with Haidt’s (2007; see also, Haidt and Hersh 2001) work, describing a link between religious conservatism and moral views, it was hypothesised that incidences of dumbfounding may be moderated by individual differences in religiosity . As such, the seven item CRSi7 scale, taken from The Centrality of Religiosity Scale (Huber and Huber 2012) was also included. Participants responded to questions relating to the frequency with which they engage in religious or spiritual activity (e.g., “How often do you think about religious issues?”). Responses were recorded using a five point Likert scale ranging from 1 (never) to 5 (very often).\n\n\nThe interviews took place in a designated psychology lab in MIC and were recorded on a digital video recording device. Participants were presented with an information sheet and a consent form. The consent form required two signatures: firstly, participants consented to take part in the study (including consent to be video recorded); the second signature related to use of the video for any presentation of the research (with voice distorted and face pixelated). Only two participants opted not to sign the second part.\n\n\nParticipants read brief vignettes describing each scenario, and were subsequently interviewed regarding the protagonists. All four scenarios were discussed in a single interview session, with a brief pause between each discussion for the participant to complete a questionnaire about their judgements, and to read the next scenario. The conversation continued when they were happy to do so. Each of the four moral dilemmas Heinz, Trolley, Cannibal and Incest (Appendix A) were presented in this way and participants asked to judge the behaviour of the characters in the dilemmas. The order of presenting the scenarios was randomised. Judgements made by participants were challenged by the experimenter (“Nobody was harmed, how can there be anything wrong?”; “Do you still think it was wrong? Why?”; “Why do you think it is wrong?”; “Have you got a reason for your judgement?”). The resulting discussion continued until participants could not articulate any further arguments. Participants filled in a brief questionnaire after discussing each dilemma. In this they were asked to rate, on a seven point Likert scale, how right/wrong they thought the behaviour was; how confident they were in their judgement, how confused they were; how irritated they were; how much their judgement had changed; how much their judgement was based on reason; and how much their judgement was based on “gut” feeling. Participants completed a longer questionnaire at the end of the interview. This contained the MLQ (Steger et al. 2008), the Centrality of Religiosity Scale (Huber and Huber 2012), and some questions relating to demographics The entire study lasted approximately 20 to 25 minutes. The videos were analysed using BORIS – Behavioural Observation Research Interactive Software (Friard and Gamba 2015). All statistical analysis was conducted using R (3.4.0, R Core Team 2017b)4; SPSS (IBM Corp 2015) was also used."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion",
    "href": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nThe videos of the interviews were analysed and participants were identified as dumbfounded if they ( a ) admitted to not having reasons for their judgements; or ( b ) resorted to using unsupported declarations (“It’s just wrong!”) as justification for their judgements, and subsequently failed to provide reasons when questioned further. Table 1 shows the initial and revised ratings of the behaviours for each scenario.\n\n\nTable 1\n\n\nRatings of each scenario for each study\n\n\n\n\n\n\nStudy\n\n\nJudgement\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\n\n\n\n\nStudy 1\n\n\nInitial: Wrong\n\n\n27\n\n\n87.1%\n\n\n25\n\n\n80.65%\n\n\n26\n\n\n83.87%\n\n\n23\n\n\n74.19%\n\n\n\n\n\n\nInitial: Neutral\n\n\n0\n\n\n0%\n\n\n0\n\n\n0%\n\n\n0\n\n\n0%\n\n\n0\n\n\n0%\n\n\n\n\n\n\nInitial: OK\n\n\n4\n\n\n12.9%\n\n\n6\n\n\n19.35%\n\n\n5\n\n\n16.13%\n\n\n8\n\n\n25.81%\n\n\n\n\n\n\nRevised: Wrong\n\n\n26\n\n\n83.87%\n\n\n23\n\n\n74.19%\n\n\n20\n\n\n64.52%\n\n\n22\n\n\n70.97%\n\n\n\n\n\n\nRevised: Neutral\n\n\n0\n\n\n0%\n\n\n0\n\n\n0%\n\n\n0\n\n\n0%\n\n\n1\n\n\n3.23%\n\n\n\n\n\n\nReviesd: OK\n\n\n5\n\n\n16.13%\n\n\n8\n\n\n25.81%\n\n\n11\n\n\n35.48%\n\n\n8\n\n\n25.81%\n\n\n\n\nStudy 2\n\n\nInitial: Wrong\n\n\n53\n\n\n73.61%\n\n\n68\n\n\n94.44%\n\n\n63\n\n\n87.5%\n\n\n50\n\n\n69.44%\n\n\n\n\n\n\nInitial: Neutral\n\n\n9\n\n\n12.5%\n\n\n3\n\n\n4.17%\n\n\n3\n\n\n4.17%\n\n\n6\n\n\n8.33%\n\n\n\n\n\n\nInitial: OK\n\n\n10\n\n\n13.89%\n\n\n1\n\n\n1.39%\n\n\n6\n\n\n8.33%\n\n\n16\n\n\n22.22%\n\n\n\n\n\n\nRevised: Wrong\n\n\n51\n\n\n70.83%\n\n\n67\n\n\n93.06%\n\n\n66\n\n\n91.67%\n\n\n48\n\n\n66.67%\n\n\n\n\n\n\nRevised: Neutral\n\n\n7\n\n\n9.72%\n\n\n3\n\n\n4.17%\n\n\n3\n\n\n4.17%\n\n\n9\n\n\n12.5%\n\n\n\n\n\n\nReviesd: OK\n\n\n14\n\n\n19.44%\n\n\n2\n\n\n2.78%\n\n\n3\n\n\n4.17%\n\n\n15\n\n\n20.83%\n\n\n\n\nStudy 3a\n\n\nInitial: Wrong\n\n\n54\n\n\n75%\n\n\n67\n\n\n93.06%\n\n\n61\n\n\n84.72%\n\n\n48\n\n\n66.67%\n\n\n\n\n\n\nInitial: Neutral\n\n\n6\n\n\n8.33%\n\n\n3\n\n\n4.17%\n\n\n7\n\n\n9.72%\n\n\n10\n\n\n13.89%\n\n\n\n\n\n\nInitial: OK\n\n\n12\n\n\n16.67%\n\n\n2\n\n\n2.78%\n\n\n4\n\n\n5.56%\n\n\n14\n\n\n19.44%\n\n\n\n\n\n\nRevised: Wrong\n\n\n53\n\n\n73.61%\n\n\n67\n\n\n93.06%\n\n\n57\n\n\n79.17%\n\n\n43\n\n\n59.72%\n\n\n\n\n\n\nRevised: Neutral\n\n\n11\n\n\n15.28%\n\n\n4\n\n\n5.56%\n\n\n12\n\n\n16.67%\n\n\n15\n\n\n20.83%\n\n\n\n\n\n\nReviesd: OK\n\n\n8\n\n\n11.11%\n\n\n1\n\n\n1.39%\n\n\n3\n\n\n4.17%\n\n\n14\n\n\n19.44%\n\n\n\n\nStudy 3b\n\n\nInitial: Wrong\n\n\n81\n\n\n80.2%\n\n\n85\n\n\n84.16%\n\n\n71\n\n\n70.3%\n\n\n66\n\n\n65.35%\n\n\n\n\n\n\nInitial: Neutral\n\n\n9\n\n\n8.91%\n\n\n13\n\n\n12.87%\n\n\n20\n\n\n19.8%\n\n\n14\n\n\n13.86%\n\n\n\n\n\n\nInitial: OK\n\n\n11\n\n\n10.89%\n\n\n3\n\n\n2.97%\n\n\n10\n\n\n9.9%\n\n\n21\n\n\n20.79%\n\n\n\n\n\n\nRevised: Wrong\n\n\n87\n\n\n86.14%\n\n\n82\n\n\n81.19%\n\n\n73\n\n\n72.28%\n\n\n59\n\n\n58.42%\n\n\n\n\n\n\nRevised: Neutral\n\n\n10\n\n\n9.9%\n\n\n15\n\n\n14.85%\n\n\n19\n\n\n18.81%\n\n\n17\n\n\n16.83%\n\n\n\n\n\n\nReviesd: OK\n\n\n4\n\n\n3.96%\n\n\n4\n\n\n3.96%\n\n\n9\n\n\n8.91%\n\n\n25\n\n\n24.75%\n\n\n\n\n\n\nTwenty two of the 31 participants (70.97%) produced a dumbfounded response (admission of having no reasons; or the use of an unsupported declaration as a justification for a judgement, with a failure to provide any alternative reason when the unsupported declaration was questioned) at least once. Examples of such responses included “It just seems wrong and I cannot explain why, I don’t know”, “because I just think it’s wrong, oh God, I don’t know why, it’s just [pause] wrong”. Table 2 shows the number, and percentage, of participants who displayed dumbfounded responses and non-dumbfounded responses for each dilemma. The rates of each type of dumbfounded response are also displayed. Figure 1 shows the percentage of participants displaying dumbfounded responses for each dilemma. Table 3 shows the responses to the questionnaires presented between dilemmas.\n\n\nTable 2\n\n\nObserved frequency and percentage of each of the responses: dumbfounded, nothing wrong, and reasons provided\n\n\n\n\n\n\n\n\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\n\n\n\n\nStudy 1\n\n\nNothing wrong\n\n\n6\n\n\n19.35%\n\n\n8\n\n\n25.81%\n\n\n11\n\n\n35.48%\n\n\n8\n\n\n25.81%\n\n\n\n\n\n\nDumbfounded\n\n\n0\n\n\n0%\n\n\n11\n\n\n35.48%\n\n\n18\n\n\n58.06%\n\n\n3\n\n\n9.68%\n\n\n\n\n\n\n(admissions)\n\n\n0\n\n\n0%\n\n\n8\n\n\n25.81%\n\n\n10\n\n\n32.26%\n\n\n3\n\n\n9.68%\n\n\n\n\n\n\n(declarations)\n\n\n0\n\n\n0%\n\n\n3\n\n\n9.68%\n\n\n8\n\n\n25.81%\n\n\n0\n\n\n0%\n\n\n\n\n\n\nReasons\n\n\n25\n\n\n80.65%\n\n\n12\n\n\n38.71%\n\n\n2\n\n\n6.45%\n\n\n20\n\n\n64.52%\n\n\n\n\nStudy 2\n\n\nNothing wrong\n\n\n8\n\n\n11.11%\n\n\n4\n\n\n5.56%\n\n\n2\n\n\n2.78%\n\n\n10\n\n\n13.89%\n\n\n\n\n\n\nDumbfounded\n\n\n45\n\n\n62.5%\n\n\n46\n\n\n63.89%\n\n\n54\n\n\n75%\n\n\n45\n\n\n62.5%\n\n\n\n\n\n\nReasons\n\n\n19\n\n\n26.39%\n\n\n22\n\n\n30.56%\n\n\n16\n\n\n22.22%\n\n\n17\n\n\n23.61%\n\n\n\n\nStudy 3a\n\n\nNothing wrong\n\n\n14\n\n\n19.44%\n\n\n4\n\n\n5.56%\n\n\n12\n\n\n16.67%\n\n\n15\n\n\n20.83%\n\n\n\n\n(critical slide)\n\n\nDumbfounded\n\n\n13\n\n\n18.06%\n\n\n14\n\n\n19.44%\n\n\n18\n\n\n25%\n\n\n14\n\n\n19.44%\n\n\n\n\n\n\nReasons\n\n\n45\n\n\n62.5%\n\n\n54\n\n\n75%\n\n\n42\n\n\n58.33%\n\n\n43\n\n\n59.72%\n\n\n\n\nStudy 3a\n\n\nNothing wrong\n\n\n14\n\n\n19.44%\n\n\n4\n\n\n5.56%\n\n\n12\n\n\n16.67%\n\n\n15\n\n\n20.83%\n\n\n\n\n(coded)\n\n\nDumbfounded\n\n\n19\n\n\n26.39%\n\n\n21\n\n\n29.17%\n\n\n31\n\n\n43.06%\n\n\n22\n\n\n30.56%\n\n\n\n\n\n\nReasons\n\n\n39\n\n\n54.17%\n\n\n47\n\n\n65.28%\n\n\n29\n\n\n40.28%\n\n\n35\n\n\n48.61%\n\n\n\n\nStudy 3b\n\n\nNothing wrong\n\n\n21\n\n\n20.79%\n\n\n10\n\n\n9.9%\n\n\n31\n\n\n30.69%\n\n\n24\n\n\n23.76%\n\n\n\n\n(critical slide)\n\n\nDumbfounded\n\n\n12\n\n\n11.88%\n\n\n19\n\n\n18.81%\n\n\n16\n\n\n15.84%\n\n\n16\n\n\n15.84%\n\n\n\n\n\n\nReasons\n\n\n68\n\n\n67.33%\n\n\n72\n\n\n71.29%\n\n\n54\n\n\n53.47%\n\n\n61\n\n\n60.4%\n\n\n\n\nStudy 3b\n\n\nNothing wrong\n\n\n21\n\n\n20.79%\n\n\n10\n\n\n9.9%\n\n\n31\n\n\n30.69%\n\n\n24\n\n\n23.76%\n\n\n\n\n(coded)\n\n\nDumbfounded\n\n\n16\n\n\n15.84%\n\n\n30\n\n\n29.7%\n\n\n28\n\n\n27.72%\n\n\n22\n\n\n21.78%\n\n\n\n\n\n\nReasons\n\n\n64\n\n\n63.37%\n\n\n61\n\n\n60.4%\n\n\n42\n\n\n41.58%\n\n\n55\n\n\n54.46%\n\n\n\n\n\n\n\n\nRates of observed dumbfounding for each scenario across each study.\n\n\n\nTable 3\n\n\nResponses to post-discussion questionnaire questions\n\n\n\n\n\n\nStudy\n\n\nQuestion\n\n\nHeinz\n\n\nCannibal\n\n\nIncest\n\n\nTrolley\n\n\n\n\n\n\nStudy 1\n\n\nChanged mind\n\n\n2.87\n\n\n3.40\n\n\n2.63\n\n\n2.60\n\n\n\n\n\n\nConfidence\n\n\n5.30\n\n\n4.77\n\n\n5.40\n\n\n5.07\n\n\n\n\n\n\nConfused\n\n\n3.00\n\n\n3.67\n\n\n3.33\n\n\n3.70\n\n\n\n\n\n\nIrritated\n\n\n3.00\n\n\n3.33\n\n\n3.13\n\n\n3.37\n\n\n\n\n\n\n‘Gut’\n\n\n5.23\n\n\n5.20\n\n\n4.97\n\n\n5.07\n\n\n\n\n\n\n‘Reason’\n\n\n4.83\n\n\n4.40\n\n\n4.43\n\n\n4.77\n\n\n\n\n\n\nGut minus Reason\n\n\n0.40\n\n\n0.80\n\n\n0.53\n\n\n0.30\n\n\n\n\nStudy 2\n\n\nConfidence\n\n\n6.10\n\n\n5.86\n\n\n5.62\n\n\n5.26\n\n\n\n\n\n\nConfused\n\n\n2.40\n\n\n3.08\n\n\n4.14\n\n\n3.17\n\n\n\n\n\n\nIrritated\n\n\n4.58\n\n\n4.68\n\n\n4.32\n\n\n4.28\n\n\n\n\n\n\n‘Gut’\n\n\n5.29\n\n\n5.54\n\n\n5.82\n\n\n4.96\n\n\n\n\n\n\n‘Reason’\n\n\n4.89\n\n\n5.19\n\n\n4.89\n\n\n4.93\n\n\n\n\n\n\nGut minus Reason\n\n\n0.40\n\n\n0.35\n\n\n0.93\n\n\n0.03\n\n\n\n\nStudy 3a\n\n\nChanged mind\n\n\n2.38\n\n\n1.67\n\n\n2.00\n\n\n2.00\n\n\n\n\n\n\nConfidence\n\n\n5.22\n\n\n5.50\n\n\n5.38\n\n\n4.81\n\n\n\n\n\n\nConfused\n\n\n2.75\n\n\n2.96\n\n\n3.25\n\n\n2.89\n\n\n\n\n\n\nIrritated\n\n\n3.94\n\n\n4.64\n\n\n4.07\n\n\n3.60\n\n\n\n\n\n\n‘Gut’\n\n\n4.78\n\n\n5.44\n\n\n5.44\n\n\n4.92\n\n\n\n\n\n\n‘Reason’\n\n\n5.07\n\n\n5.26\n\n\n5.11\n\n\n5.06\n\n\n\n\n\n\nGut minus Reason\n\n\n-0.29\n\n\n0.18\n\n\n0.33\n\n\n-0.14\n\n\n\n\nStudy 3b\n\n\nChanged mind\n\n\n1.74\n\n\n1.60\n\n\n1.57\n\n\n1.83\n\n\n\n\n\n\nConfidence\n\n\n5.78\n\n\n6.16\n\n\n5.81\n\n\n5.36\n\n\n\n\n\n\nConfused\n\n\n2.06\n\n\n2.07\n\n\n2.12\n\n\n2.22\n\n\n\n\n\n\nIrritated\n\n\n4.42\n\n\n4.01\n\n\n3.56\n\n\n3.39\n\n\n\n\n\n\n‘Gut’\n\n\n4.42\n\n\n4.43\n\n\n4.47\n\n\n4.01\n\n\n\n\n\n\n‘Reason’\n\n\n5.46\n\n\n5.69\n\n\n5.26\n\n\n5.58\n\n\n\n\n\n\nGut minus Reason\n\n\n-1.04\n\n\n-1.27\n\n\n-0.79\n\n\n-1.57\n\n\n\n\n\n\nIn line with the original study (Haidt et al., 2000), the videos were also coded, by the primary researcher, across a range of measures. Haidt, Björklund, and Murphy (2000) report differences, between intuition and reasoning scenarios. They do not, however, report comparisons between participants identified as dumbfounded and participants not identified as dumbfounded. The current research, aiming to identify measurable indicators of dumbfounding, categorised participants as dumbfounded according to the two types of verbal responses (admissions and unsupported declaration) and compared these groups with participants who were not identified as dumbfounded, across a range of measures. There were two stages in this analysis. Firstly, all participants identified as dumbfounded were compared against participants who provided reasons only. Secondly, participants identified as dumbfounded were grouped according to type of dumbfounded response, and participants who did not rate the behaviour as wrong were also included in the analysis.\n\n\nJudgement variables reported by Haidt, Björklund, and Murphy (2000) included the length of time until the first argument, the length of time until the first evaluation, the length of time between the first evaluation and the first argument. The current research reports the same judgement variables.\n\n\nA range of “argument variables” were also reported. Identifying specific objectively verifiable measurable indicators for some of the “argument variables” reported by Haidt, Björklund, and Murphy (2000) was problematic (e.g., “dead-ends”, “argument kept”, “argument dropped”). The current research coded each verbal utterance according to a relevance for forming an argument. As such some of the argument variables reported by Haidt, Björklund, and Murphy (2000) are not reported here in the same way, however, related measures are reported.\n\n\nParalinguistic variables reported by Haidt, Björklund, and Murphy (2000) include frequency (per minute) of: “ums, uhs, hmms”, “turns with laughter”, “turns with face touch”, “doubt faces”, and “turns with pen fiddle”. As with the argument variables, the coding of the non-verbal/paralinguistic responses also varies slightly from what was reported by Haidt, Björklund, and Murphy (2000). We coded for both verbal hesitations (“um/em/uh”) and non-verbal hesitations/stuttering. “Turns” was coded independently of other behaviours as changing position. Laughter was coded for independently of changing position. The coding of hands touching the self was not limited to the face. Participants did not have pens to fiddle with, however we coded for generic fidgeting. The term “doubt faces” presented as problematic to code for rigorously across different individuals. As such, two distinctive and opposing facial expressions were coded for: smiling and frowning.\n\n\nDumbfounded versus reasons\n\nFifty nine cases of participants providing reasons, were compared with 32 cases of dumbfounded responding. There was no difference in time until first judgement between the dumbfounded group, (M = 14.89, SD = 20.41) and the group who provided reasons (M = 15.19, SD = 40.54), p = .969. Similarly, there was no difference in time until first argument between the dumbfounded group, (M = 39.20, SD = 28.90) and the group who provided reasons (M = 30.49, SD = 32.30), F(1, ,, , 81) = 1.42, p = .237, partial ()2 = .017. There was no difference in time from first judgement to time of first argument between the dumbfounded group, (M = 20.60, SD = 36.76) and the group who provided reasons (M = 15.65, SD = 46.42), p = .634.\n\n\nThere was a significant difference in frequency (per minute) of utterances whereby participants were working towards a reason between the dumbfounded group, (M = 1.47, SD = 1.45) and the group who provided reasons (M = 2.70, SD = 1.53), F(1, ,, , 89) = 13.82, p &lt; .001, partial ()2 = .134. There was no difference in frequency (per minute) of irrelevant arguments between the dumbfounded group, (M = 1.03, SD = .74) and the group who provided reasons (M = .86, SD = .77), F(1, ,, , 89) = 1.05, p = .308, partial ()2 = .012. There was a significant difference in frequency (per minute) of expressions of doubt between the dumbfounded group, (M = .63, SD = .65) and the group who provided reasons (M = .31, SD = .58), F(1, ,, , 89) = 5.87, p = .017, partial ()2 = .062.\n\n\nA one-way ANOVA revealed a significant difference in number of times per minute participants laughed between the dumbfounded group, (M = 2.81, SD = 2.84) and the group who provided reasons (M = 1.18, SD = 1.25), F(1, ,, , 89) = 14.35, p &lt; .001, partial ()2 = .139. Similarly, a one-way ANOVA revealed a significant difference relative amount of time spent smiling (as a proportion of the total time spent on the given scenario) between the dumbfounded group, (M = .32, SD = .15) and the group who provided reasons (M = .16, SD = .14), F(1, ,, , 89) = 25.24, p &lt; .001, partial ()2 = .221. Consistent with the results reported by Haidt, Björklund, and Murphy (2000), a series of one-way ANOVAs revealed no differences in verbal hesitations, F(1, ,, , 89) = 2.35, p = .129, partial ()2 = .026, non-verbal hesitations, p = .074, changing posture, p = .485, hands on the self, p = .864, frowning, p = .958, and fidgeting, F(1, ,, , 89) = 1.66, p = .201, partial ()2 = .018. A one-way ANOVA revealed a significant difference relative amount of time spent in silence (as a proportion of the total time spent on the given scenario) between the dumbfounded group, (M = .14, SD = .08) and the group who provided reasons (M = .09, SD = .06), F(1, ,, , 89) = 9.72, p = .002, partial ()2 = .098.\n\n\nFrom the above analysis, it appears that, working towards reasons, expressions of doubt, laughter, smiling, and silence were the only measures that varied significantly depending on whether a person was identified as dumbfounded or provided reasons. Having identified differences between dumbfounded participants and participants providing reasons, the following analysis investigates if there are differences depending the type of dumbfounded response provided. participants who did not rate the behaviour as wrong are also included in the following analysis.\n\n\n\nVariation between different types of dumbfounded responses\n\nFour groups, based on overall reaction to scenarios, were identified: participants who did not rate the behaviour as wrong, participants who provided reasons, participants who provided unsupported declarations, and participants who admitted to not having reasons.\n\n\nA one-way ANOVA revealed a significant difference in relative frequency of utterances whereby participants were working towards a reason depending on overall reaction to scenarios, F(3, ,, , 120) = 7.54, p &lt; .001, partial ()2 = .159. Tukey’s post-hoc pairwise comparison revealed that participants who provided reasons were identified as working towards a reason significantly more frequently (M = 2.70, SD = 1.53) than participants who did not rate the behaviour as wrong (M = 1.76, SD = 1.48), p = .021, and more frequently than participants who provided unsupported declarations as justifications (M = .64, SD = .72), p &lt; .001. There was no difference between participants who admitted to not having reasons (M = 1.90, SD = 1.56) and any of the other groups. A one-way ANOVA revealed no significant difference in relative frequency of expressions of doubt depending on overall reaction to scenarios, F(3, ,, , 120) = 2.17, p = .096, partial ()2 = .051.\n\n\nA one-way ANOVA revealed a significant difference in relative frequency laughter depending on overall reaction to scenarios, F(3, ,, , 120) = 8.27, p &lt; .001, partial ()2 = .171. Tukey’s post-hoc pairwise comparison revealed that participants who admitted to not having reasons laughed significantly more frequently (M = 2.41, SD = 2.00), than participants who provided reasons (M = 1.18, SD = 1.25), p = .039, and more frequently than participants who provided did not rate the behaviour as wrong (M = .97, SD = 1.29), p = .025. Similarly, participants who provided unsupported declarations laughed significantly more frequently (M = 3.57, SD = 4.00), than participants who provided reasons, p &lt; .001, and more frequently than participants who did not rate the behaviour as wrong, p &lt; .001. There was no difference between participants who provided reasons, and participants who did not rate the behaviour as wrong p = .951. Interestingly, there was no difference between participants who admitted to not having reasons and participants who provided unsupported declarations, p = .305.\n\n\nA similar pattern of results was found for time spent smiling. A one-way ANOVA revealed a significant difference in relative time spent smiling depending on overall reaction to scenarios, F(3, ,, , 120) = 9.97, p &lt; .001, partial ()2 = .200. Tukey’s post-hoc pairwise comparison revealed that participants who admitted to not having reasons spent significantly more time smiling (M = .33, SD = .14), than participants who provided reasons (M = .16, SD = .14), p &lt; .001, and more time smiling than participants who provided did not rate the behaviour as wrong (M = .16, SD = .13), p &lt; .001. Participants who provided unsupported declarations spent significantly more time smiling (M = .31, SD = .17), than participants who provided reasons, p = .008, and participants who did not rate the behaviour as wrong, p = .014. There was no difference between participants who provided reasons, and participants who did not rate the behaviour as wrong, p = 1.000. Again, there was no difference between participants who admitted to not having reasons and participants who provided unsupported declarations, p = .996.\n\n\nA one-way ANOVA revealed a significant difference in relative amount of time spent in silence depending on overall reaction to scenarios, F(3, ,, , 120) = 3.31, p = .023, partial ()2 = .076. Mean proportion of interview time spent in silence are as follows: participants providing reasons, M = .09, SD = .06; participants not rating the behaviour as wrong, M = .12, SD = .07; participants admitting to not having reasons, M = .14, SD = .09; and participants providing unsupported declarations, M = .14, SD = .05. Tukey’s post-hoc pairwise comparison did not reveal any significant differences between specific groups.\n\n\n\nFurther analyses\n\nAn exploratory analysis revealed no association between number of times dumbfounded and score on either measures from the MLQ: Presence, r(31) = 0.74, p = .466, or Search, r(31) = 1.38, p = .179, or the Centrality of Religiosity Scale r(31) = 0.35, p = .726. There was no difference in observed rates of dumbfounded responses depending on the order of scenario presentation, χ2(6, N = 124) = 4.01, p = .676. Rates of dumbfounded responses varied depending on which moral dilemma was being discussed, χ2(6, N = 124) = 46.82, p &lt; .001. The highest rate of dumbfounding was recorded for Incest, with 18 of the 31 (58.06%) participants displaying dumbfounded responses. Eleven participants (35.48%) displayed dumbfounded responses for Cannibal and three participants (9.68%) displayed dumbfounded responses for Trolley. The lowest recorded rate of dumbfounded response was for the Heinz dilemma, with no participants resorting to unsupported declarations as justification or admitting to not having reasons for their judgement. This trend is generally consistent with that which emerged in the original study (with the exception of Trolley, which was not used in the original study). Furthermore, rates of dumbfounded responding varied depending on which type of moral scenario was being discussed. Heinz and Trolley, identified as reasoning scenarios, were contrasted against the intuition scenarios Incest and Cannibal. There was significantly more dumbfounded responding for the intuition scenarios (29 instances) than for the reasoning scenarios (3 instances), χ2(2, N = 124) = 38.17, p &lt; .001.\n\n\nThe aim of Study 1 was to examine the replicability of moral dumbfounding as identified by Haidt, Björklund, and Murphy (2000), and identify specific measurable responses that may be indicative of dumbfounding. The overall pattern of responses, and pattern of inter-scenario variability in responding resembled that observed in the original study. As such, Study 1 successfully replicated the findings of the original moral dumbfounding study (Haidt, Björklund, and Murphy 2000). Participants were identified as dumbfounded according to two specific measures, admissions of having no reasons, and unsupported declarations followed by a failure to provide reasons when questioned further. Both of these responses were accompanied by similar increases incidences of laughter, and time spent smiling, when compared to participants providing reasons, and participants not rating the behaviour as wrong. When taken together, these responses were also accompanied by more silence during the interview, when compared with participants who provided reasons. As such, it appears that identifying incidences of dumbfounding according to unsupported declarations or admissions of not having reasons largely capture dumbfounding as described by Haidt, Björklund, and Murphy (2000).\n\n\nStudy 1 provides evidence supporting the view that moral dumbfounding is a genuine phenomenon and can be elicited in an interview setting when participants are pressed to justify their judgements of particular moral scenarios. Two key limitations have been identified as a result of conducting studies in an interview setting. Firstly, conducting video-recorded interviews, and the accompanying analyses, is particularly labour intensive, which leads to a smaller sample size. The aims of the present research were to examine the replicability of dumbfounding, and to identify specific measurable indicators of dumbfounding. A sample size of thirty-one is not sufficient in fulfilling the first aim. Secondly, an interview setting introduces a social context that may influence the responses of participants, in that, participants may feel a social pressure to behave in a particular way (e.g., Royzman, Kim, and Leeman 2015). Alternative methods are required to examine dumbfounding with a larger sample, and whether it still occurs in the absence of the social pressure that is present in an interview setting. Two responses have been identified as indicators of dumbfounding. The degree to which each of these responses can be elicited in a setting other than an interview is investigated in Studies 2 and 3."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#method-1",
    "href": "publications/searching-for-moral-dumbfounding/index.html#method-1",
    "title": "Searching for Moral Dumbfounding:",
    "section": "",
    "text": "Participants and design\n\nStudy 3a was a frequency based, modified replication. The aim was to identify if dumbfounded responding could be evoked. All participants were presented with the same four moral vignettes. Results are primarily descriptive. Further analysis tested for differences in responding depending on the vignette, or type of vignette, presented.\n\n\nA sample of 72 participants (46 female, 26 male; Mage = 21.80, min = 18, max = 46, SD = 3.91) took part in this study. Participants were undergraduate students and postgraduate students from MIC. Participation was voluntary and participants were not reimbursed for their participation.\n\n\n\n\nProcedure and materials\n\nThe materials in this study were almost the same as in Study 2 with a change to the “dumbfounded” response option on the critical slide. Extra questions were included following each of the counter-arguments. On the critical slide, the unsupported declaration option was replaced with an admission of not having reasons (“It’s wrong but I can’t think of a reason”). Following each counter-argument, participants were asked if they (still) thought the behaviour was wrong, and if they had a reason for their judgement. There was also a revision to the question on the post-discussion questionnaire asking if participants had changed their judgements was changed: “did your judgement change?” with a binary “yes/no” response option reverted back to “how much did your judgement change?” with a seven point Likert scale response (as in Study 1). The same four dilemmas Heinz, Incest, Cannibal and Trolley (Appendix A) along with the same prepared counter arguments (Appendix B) as in Study 2 were used in Study 3a. Both the MLQ (Steger et al. 2008); and CRSi7 (Huber and Huber 2012) were also used. This study was conducted in a designated psychology computer lab in MIC and was administered entirely on individual computers using OpenSesame (Mathôt, Schreij, and Theeuwes 2012).\n\n\nParticipants were seated, given instructions, and allowed to begin the computer task. The four vignettes from Study 1 Heinz, Incest, Cannibal and Trolley (Appendix A) along with the same pre-prepared counter arguments (Appendix B) were used. Dumbfounding was measured using the critical slide. The updated critical slide contained a statement defending the behaviour and a question as to how the behaviour could be wrong (e.g., “Julie and Mark’s behaviour did not harm anyone, how can there be anything wrong with what they did?”) with three possible response options: ( a ) “There is nothing wrong”; ( b ) “It’s wrong, but I can’t think of a reason”; ( c ) “It’s wrong and I can provide a valid reason”. The order of these response options was randomised. Participants who selected ( c ) were required to provide a reason. The selecting of option ( b ), the admission of not having reasons, was taken to be a dumbfounded response. When participants had completed all questions relating to all four dilemmas they completed the same longer questionnaire as in Studies 1 and 2 containing the MLQ (Steger et al. 2008), the Centrality of Religiosity Scale (Huber and Huber 2012), and some questions relating to demographics. The entire study lasted approximately fifteen to twenty minutes."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-1",
    "href": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-1",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nParticipants who selected the unsupported declaration on the critical slide were identified as dumbfounded. Table 1 shows the ratings of the behaviours across each scenario. Table 2 shows the number, and percentage, of participants who displayed “dumbfounded” responses (identified as the selecting of an unsupported declaration) and non-dumbfounded responses for each dilemma. Figure 1 shows the percentage of participants displaying dumbfounded responses for each dilemma. Table 3 shows the responses to the questionnaires presented between dilemmas. The open-ended responses provided by participants who selected option ( c ) “It’s wrong and I can provide a valid reason” were analysed and coded, by the primary researcher, and unsupported declarations provided here were also identified as dumbfounded responses. Following this coding, one additional participant was identified as dumbfounded for Trolley. Sixty eight of the 72 participants (94%) selected the unsupported declaration at least once. There was no statistically significant difference in responses to the critical slide depending on the order of scenario presentation, χ2(6, N = 288) = 4.13, p = .659. There was no statistically significant difference in responses to the critical slide depending on scenario presented, χ2(6, N = 288) = 9.00, p = .173. Rates of dumbfounded responding did not vary with type of moral scenario (100 instances for intuition scenarios, 90 instances for reasoning scenarios) being discussed, χ2(2, N = 288) = 6.58, p = .037. Forty five participants (62.5%) selected the unsupported for Heinz. Forty six participants (63.89%) selected (or provided) the unsupported declaration for Cannibal and Trolley. Fifty four participants (75%) selected the unsupported declaration for Incest. There was no association between number of times dumbfounded and score on either measure on the Meaning and Life questionnaire; Presence r(72) = -0.44, p = .662, or Search, r(72) = 1.12, p = .268, or the Centrality of Religiosity Scale r(72) = 1.24, p = .220.\n\n\nThe most striking result from this study was the willingness of participants to select the unsupported declaration in response to a challenge to their judgement. This is inconsistent with what was found in in both Study 1 and in the original study by Haidt, Björklund, and Murphy (2000). In these studies, participants did not readily offer an unsupported declaration as justification for their judgement, rather it was a last resort following extensive cross-examining. The exceptionally high rates of dumbfounding observed in Study 2 do not appear to be representative of the phenomenon more generally. There is, therefore, clearly a difference between offering an unsupported declaration as a justification for a judgement during an interview and selecting an unsupported declaration from a list of possible response options during a computerised task. It is possible that, during the interview, participants experienced a social pressure to successfully justify their judgement. This social pressure may also have made participants were more aware of the illegitimacy of using an unsupported declaration as a justification for their judgement. It is also possible that, seeing it written down as a possible answer legitimises selecting it as a justification for the judgement. The unsupported declaration does not provide an acceptable answer to the question on the critical slide, however, its presence in the list of possible response options may imply to participants that it is an acceptable answer, particularly if they do not put too much thought into it. By selecting the unsupported declaration participants can move quickly along to the next stage in the study without necessarily acknowledging any inconsistency in their reasoning, avoiding potentially dissonant cognitions (e.g., Case et al. 2005; Harmon-Jones and Harmon-Jones 2007; see also Heine, Proulx, and Vohs 2006). Selecting the unsupported declaration may also allow the participant to proceed without expending effort trying to think of reasons for their judgement beyond the intuitive justifications that had already been de-bunked.\n\n\nRates of dumbfounded responding in Study 2 were higher than expected. Possible reasons for this could be ( a ) reduced social pressure to appear to have reasons for judgements; ( b ) a failure of participants to comprehend that the unsupported declaration does not provide a logically justifiable response to the question asked in the critical slide; ( c ) the apparent legitimising of the unsupported declaration by its inclusion in the list of possible response options; or (d) the selecting by participants of an “easy way out” option without thinking about it fully (through carelessness/laziness/eagerness to move on to a less taxing task). It appears that the selecting of unsupported declarations is not an accurate measure of dumbfounding. In Study 1, participants were only identified as dumbfounded based on the providing of an unsupported declaration if they subsequently failed to provide further reasons when the unsupported declaration was questioned. However, in some cases, participants who provided unsupported declarations were not identified as dumbfounded, based on subsequent responses. A follow up analysis of the interview data revealed that 23 participants provided an unsupported declaration and proceeded to provide reasons for at least one of their judgements; a further six participants provided an unsupported declaration and proceeded to revise their judgement at least once. A stricter measure of dumbfounding, one by which participants are required to explicitly acknowledge a state of dumbfoundedness is necessary to address the issues with the selecting of an unsupported declaration that may have led to the unusually high rates of dumbfounding observed in Study 2."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#method-2",
    "href": "publications/searching-for-moral-dumbfounding/index.html#method-2",
    "title": "Searching for Moral Dumbfounding:",
    "section": "",
    "text": "Participants and design\n\nStudy 3b was a frequency based, modified replication. The aim was to identify if dumbfounded responding could be evoked. All participants were presented with the same four moral vignettes. Results are primarily descriptive. Further analysis tested for differences in responding depending on the vignette, or type of vignette, presented.\n\n\nA sample of 101 participants (53 female, 47 male; Mage = 36.58, min = 18, max = 69, SD = 12.45) took part in this study. Participants were recruited online through MTurk (Amazon Web Services Inc. 2016). Participation was voluntary and participants were paid 0.70 US dollars for their participation. Participants were recruited from English speaking countries or from countries where residents generally have a high level of English (e.g., The Netherlands, Denmark, Sweden). Location data for individual participants was not recorded, however, based on other studies, using the same selection criteria, it is likely that 90% of the sample was from the United States.\n\n\n\n\nProcedure and materials\n\nThe materials in this study were almost the same as in Study 3a, however, a different software package was used to present the materials and collect the responses. OpenSesame (Mathôt, Schreij, and Theeuwes 2012) was replaced with Questback (Unipark 2013)in order to facilitate online data collection. This meant that the recording of responses changed from keyboard input to mouse input. It also allowed for multiple questions to be displayed on the screen at the same time. Other than these changes, the materials were the same as in Study 3a.\n\n\nThe computer task in Study 3b was much the same as Study 3a. The four vignettes from Study 1: Heinz, Incest, Cannibal, and Trolley (Appendix A) along with the same pre-prepared counter arguments (Appendix B). Dumbfounding was measured using the critical slide.\n\n\nThe critical slide contained a statement defending the behaviour and a question as to how the behaviour could be wrong, with three possible response options: ( a ) “There is nothing wrong”; ( b ) “It’s wrong but I can’t think of a reason”; ( c ) “It’s wrong and I can provide a valid reason”. Participants who selected ( c ) were required to provide a reason. The order of these response options was randomised. When participants had completed all questions relating to all four dilemmas they completed the same longer questionnaire as in Studies 1 and 2 containing the Meaning and Life questionnaire (Steger et al. 2008), the Centrality of Religiosity Scale (Huber and Huber 2012), and some questions relating to demographics. The entire study lasted approximately fifteen to twenty minutes."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-2",
    "href": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-2",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nParticipants who selected the admission of not having reasons on the critical slide (option b) were identified as dumbfounded. Forty of the 72 participants (56%) selected the admission of not having reasons at least once. Table 1 shows the ratings of the behaviours across each scenario. Table 2 and Figure 1 show the percentage of participants displaying dumbfounded responses for each dilemma. Table 3 shows the responses to the questionnaires presented between dilemmas. Again there was no statistically significant difference in responses to the critical slide depending on the order of scenario presentation, χ2(6, N = 288) = 0.61, p = .996. There was no difference in responses to the critical slide depending on scenario, χ2(6, N = 288) = 9.6, p = .142, , or, type of scenario (32 instances for intuition scenarios, 27 instances for reasoning scenarios), χ2(2, N = 288) = 4.53, p = .104. Thirteen participants (18.06%) selected the admission of having no reasons for Heinz. Fourteen participants (19.44%) selected the admission of not having reasons for Cannibal and Trolley. Eighteen participants (25%) selected the admission of not having reasons for Incest.\n\n\nThe replacing of an unsupported declaration with an admission of having no reasons led to substantially lower rates of dumbfounding than observed in Study 2. As such, it appears that the issues associated with the selecting of an unsupported declaration have been addressed in Study 3a. However, the rates of dumbfounding observed for Incest and Cannibal in Study 3a were considerably lower than those observed in Study 1. This suggests the revised measure may be too strict, measuring only open admissions of not having reasons, but not accounting for a failure to provide reasons. As in the first computerised task, participants who selected “It’s wrong and I can provide a valid reason” were then required to provide a reason. In order to provide a measure of a failure to provide reasons, these responses were analysed and coded, by the primary researcher. Those containing unsupported declarations were taken as evidence for a failure to provide a reason and identified as dumbfounded responses.\n\n\nDuring the coding, another class of dumbfounded response was identified. Participants occasionally provided undefended tautological responses as justification for their judgements, whereby they simply named or described the behaviour in the scenario as justification for their judgement (e.g., “They are related”, “Because it is canibalism” [typographical error in response]). These responses may be viewed as largely equivalent to unsupported declarations (e.g., Mallon and Nichols 2011). In Study 1, they were not identified as dumbfounded responses, because when provided in an interview setting, they were always followed by further questioning. This further questioning could lead to two possible responses: ( a ) a dumbfounded response (unsupported declaration or an admission of not having reasons) or ( b ) an alternative reason. A computerised task does not allow for a follow-up probe to encourage participants to elaborate on such responses. Participants were not placed under time pressure and could articulate and review their typed reason at their own pace. It is reasonable to expect then, that, if participants did have a valid reason for their judgement, they would have provided it along with, or instead of, the undefended tautological response. As such, an undefended tautological reason appears to be evidence of a failure to identify reasons . For this reason, these undefended tautological reasons were also coded as dumbfounded responses, along with the unsupported declarations.\n\n\nTable 2 and Figure 2 show the number and percentage of dumbfounded responses when the coded string responses are included in the analysis. When the coded string responses are included in the analysis, the number of participants displaying a dumbfounded response at least once increased from 40 (56%) to 57 (79%). Observed rates of dumbfounding increased for each scenario when the coded open-ended responses were included, with 19 participants (26.39%) appearing to be dumbfounded by Heinz, 21 (29.17%) by Cannibal, 31 (43.06%) by Incest, and 22 (30.56%) apparently dumbfounded by Trolley. Still, rates of dumbfounded responding did not vary with type of moral scenario (52 instances for intuition scenarios, 41 instances for reasoning scenarios) being discussed, χ2(1, N = 288) = 1.59, p = .208. There was no association between number of times dumbfounded and score on either measure on the Meaning and Life questionnaire; Presence r(72) = 0.82, p = .413, or Search, r(72) = 0.07, p = .945, or the Centrality of Religiosity Scale r(72) = 1.29, p = .201.\n\n\n\n\nRates of observed dumbfounding for each scenario across each study, including coded string responses.\n\n\n\nWhen the coded open-ended responses were included in the analysis, the proportion of participants displaying a dumbfounded response at least once in Study 3a (79%) was much closer to that observed in the interview in Study 1 (74%) than before the open-ended responses were included (56%). The variation in observed rates of dumbfounding between dilemmas that was observed in the interview was not present in the computerised task. As such there remains a difference between the dumbfounding elicited during an interview and that elicited as part of a computerised task. However, it is clear that dumbfounded responses can be elicited as part of a computerised task. The participants in Studies 1, 2, and 3a were all college students (largely from the same institution) and as such, the following study investigated the phenomenon in a more diverse sample."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#method-3",
    "href": "publications/searching-for-moral-dumbfounding/index.html#method-3",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Method",
    "text": "Method\n\n\nParticipants and design\n\nStudy 3b was a frequency based, modified replication. The aim was to identify if dumbfounded responding could be evoked. All participants were presented with the same four moral vignettes. Results are primarily descriptive. Further analysis tested for differences in responding depending on the vignette, or type of vignette, presented.\n\n\nA sample of 101 participants (53 female, 47 male; Mage = 36.58, min = 18, max = 69, SD = 12.45) took part in this study. Participants were recruited online through MTurk (Amazon Web Services Inc. 2016). Participation was voluntary and participants were paid 0.70 US dollars for their participation. Participants were recruited from English speaking countries or from countries where residents generally have a high level of English (e.g., The Netherlands, Denmark, Sweden). Location data for individual participants was not recorded, however, based on other studies, using the same selection criteria, it is likely that 90% of the sample was from the United States.\n\n\n\n\nProcedure and materials\n\nThe materials in this study were almost the same as in Study 3a, however, a different software package was used to present the materials and collect the responses. OpenSesame (Mathôt, Schreij, and Theeuwes 2012) was replaced with Questback (Unipark 2013)in order to facilitate online data collection. This meant that the recording of responses changed from keyboard input to mouse input. It also allowed for multiple questions to be displayed on the screen at the same time. Other than these changes, the materials were the same as in Study 3a.\n\n\nThe computer task in Study 3b was much the same as Study 3a. The four vignettes from Study 1: Heinz, Incest, Cannibal, and Trolley (Appendix A) along with the same pre-prepared counter arguments (Appendix B). Dumbfounding was measured using the critical slide.\n\n\nThe critical slide contained a statement defending the behaviour and a question as to how the behaviour could be wrong, with three possible response options: ( a ) “There is nothing wrong”; ( b ) “It’s wrong but I can’t think of a reason”; ( c ) “It’s wrong and I can provide a valid reason”. Participants who selected ( c ) were required to provide a reason. The order of these response options was randomised. When participants had completed all questions relating to all four dilemmas they completed the same longer questionnaire as in Studies 1 and 2 containing the Meaning and Life questionnaire (Steger et al. 2008), the Centrality of Religiosity Scale (Huber and Huber 2012), and some questions relating to demographics. The entire study lasted approximately fifteen to twenty minutes."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-3",
    "href": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-3",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nParticipants who selected the admission of not having reasons on the critical slide (option b) were identified as dumbfounded. Table1 shows the ratings of the behaviours across each scenario. Table 2 and Figure 1 show the percentage of participants displaying dumbfounded responses for each scenario. Table 3 shows the responses to the questionnaires presented between scenario. On this occasion there was a statistically significant difference in responses to the critical slide depending on the order of scenario presentation, χ2(6, N = 404) = 14.77, p = .022. The observed rates of dumbfounded responses were higher for the third scenario, however they went down again for the fourth scenario along with rates of selecting “nothing wrong”, meaning that the rates of participants providing reasons went up again for the fourth scenario. The higher rates of providing reasons observed for the fourth scenario presented means that this fluctuation is unlikely to be due to experimental fatigue, which was the primary reason for testing for order effects. There was also a difference in responses to the critical slide depending on scenario, χ2(6, N = 404) = 15.18, p = .019 with more people selecting “nothing wrong” for Incest and fewer people selecting “nothing wrong” for Cannibal. When dumbfounded responses are isolated and contrasted against other responses this difference is no longer present, χ2(3, N = 404) = 1.86, p = .602. Forty four participants (44%) selected the admission of not having reasons at least once. Twelve participants (11.88%) selected the admission of having no reasons for Heinz. Sixteen participants (15.84%) selected the admission of not having reasons for Incest and Trolley. Nineteen participants (18.81%) selected the admission of not having reasons for Cannibal.\n\n\nAs in Study 3a, participants who selected option ( c ) “It’s wrong and I can provide a valid reason”, were there then required to provide a reason through open-ended response. These open-ended responses were coded, by the primary researcher, for dumbfounded responses, again, identified as unsupported declarations or as undefended tautological responses. Table 2 and Figure 2 show the rates of observed dumbfounding when the coded open-ended responses were included in the analysis. As expected, the number of participants displaying a dumbfounded response at least once increased, from 44 (44%) to 57 (56%). Observed rates of dumbfounding increased for each scenario when the coded reasons were included with 16 participants (15.84%) appearing to be dumbfounded by Heinz, 30 (29.7%) by Cannibal, 28 (27.72%) by Incest, and 22 (21.78%) apparently dumbfounded by Trolley. Taking these revised rates of dumbfounding there was a no significant difference in rates of dumbfounded responding depending on scenario, χ2(3, N = 404) = 6.56, p = .087. There was however, significantly more dumbfounded responding for the intuition scenarios (58 instances) than for the reasoning scenarios (38 instances), χ2(1, N = 404) = 4.93, p = .026.\n\n\nThere was no association between number of times dumbfounded and score on either measure on the Meaning and Life questionnaire; Presence r(101) = -0.78, p = .436, or Search, r(101) = 0.63, p = .532, or the Centrality of Religiosity Scale r(101) = 0.44, p = .662. This is consistent with Studies 1, 2, and 3a. It appears that susceptibility to dumbfounding is not related to either measure."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#evaluating-each-measure-of-dumbfounding",
    "href": "publications/searching-for-moral-dumbfounding/index.html#evaluating-each-measure-of-dumbfounding",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Evaluating each Measure of Dumbfounding",
    "text": "Evaluating each Measure of Dumbfounding\n\nThe current research identifies moral dumbfounding as a rare demonstration of a separation between intuitions and reasons for these intuitions (e.g., Barsalou 2008, 2009, 2003; Crockett 2013; Cushman 2013). Two ways in which this separation may manifest were identified. Firstly participants may acknowledge that they do not have reasons for their judgements, admitting to not having reasons. Secondly, participants may fail to provide reasons when asked, providing responses that fail to answer the question they were asked. Two such responses were identified, unsupported declarations and tautological responses.\n\n\nMeasuring dumbfounding according to an admission of not having reasons only, in Studies 1, 3a and 3b (N = 204), 100 participants (49%) were identified as dumbfounded at least once. When a failure to provide reasons (taken as the providing of unsupported declarations in Study 1, and, unsupported declarations and tautological responses in Study 3) was included as a dumbfounded response, 136 participants (67%) were identified as dumbfounded at least once. When the selecting of an unsupported declaration (Study 2, N = 72) was included (N = 276), 204 participants, (74%) were identified as dumbfounded at least once.\n\n\nThe disparity in results between Study 2 and the other studies suggests that the selection of an unsupported declaration does not provide a good measure of moral dumbfounding. Participants in Studies 1, 3a, and 3b, recognised the illegitimacy unsupported declarations as justifications for their judgement, with the majority of participants avoided resorting to this type of response at all. The vast majority of participants appeared to be willing to ignore the illegitimacy of the response, with large numbers of participants selecting the unsupported declaration. While Study 2 did not identify a means to measure dumbfounding, these results are interesting, and may provide an insight into the cognitive processes that lead to dumbfounding.\n\n\nProviding an unsupported declaration is clearly different to selecting one from a list of possible responses. One possible explanation, is that dumbfounding is an aversive state, similar to experiencing a threat to meaning (Heine, Proulx, and Vohs 2006; Proulx and Inzlicht 2012), or cognitive dissonance (Cooper 2007; Festinger 1957; Harmon-Jones and Harmon-Jones 2007). The selecting of an unsupported declaration without deliberation allows participants to avoid or minimise the impact of this aversive state and move on. Providing an unsupported declaration involves more deliberation, making the illegitimacy of it more salient, reducing its effectiveness in avoiding the aversive state of dumbfoundedness. Furthermore, the relative attractiveness of these different responses to participants may be linked to social desirability (Chung and Monroe 2003; Latif 2000; Morris and McDonald 2013). Follow-up work could investigate these questions directly.\n\n\nThe explicit acknowledgement of an absence of reasons can be measured systematically by the selection of an admission of having no reasons. This is an unambiguous measure of moral dumbfounding, does not account for participants who fail to provide reasons. Measuring a failure to provide reasons, however, is more problematic. What is termed as a valid reason is subjective. The providing of unsupported declarations and tautological responses has been identified here as an indicator of a failure to provide reasons. This is grounded in discussions of dumbfounding in the wider literature (Haidt 2001; Mallon and Nichols 2011; Prinz 2005), and the theoretical framework adopted here. Evidence for equivalence of unsupported declarations and admissions of not having reasons was also found in Study 1 whereby both measures displayed similar variability in non-verbal behaviours when contrasted against participants who provided reasons, and participants who did not rate the behaviour as wrong. However, caution is advised in taking unsupported declarations as evidence for dumbfounding, particularly given the pattern of responses in Study 2, and that a number of participants in Study 1 who provided an unsupported declaration proceeded to provide reasons, or a revised judgement.\n\n\nThe current research identified two measures of dumbfounding. Limitations are associated with each. Relying on admissions of having no reasons only, provides an overly strict measure whereby a failure to provide reasons is not measured. Taking unsupported declarations (and tautological reasons) as a measure of dumbfounding may provide too broad a measure, risks identifying lazy or inattentive participants as dumbfounded. The providing of a type-written response as part of a computerised task requires effort, and the majority of participants avoid the use of unsupported declarations as justifications for their judgements. This suggests that those who provided unsupported declarations did so because they failed to identify alternative reason. It appears that the most practicable means to measure dumbfounding accurately requires each of the responses: providing/selecting admissions of not having reasons, and the providing of an unsupported declaration, to be accounted for. Participants providing either of these responses may be identified as dumbfounded."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#differences-between-scenarios",
    "href": "publications/searching-for-moral-dumbfounding/index.html#differences-between-scenarios",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Differences between Scenarios",
    "text": "Differences between Scenarios\n\nIn Study 1 we found that rates of dumbfounded responding varied depending on the scenario presented. Study 2 recorded high rates of dumbfounded responses for all scenarios. In Studies 3a and 3b, we observed low rates of dumbfounded responding for all scenarios. In Study 1 and Study 3b, we observed varying rates of dumbfounded responses depending on scenario type. When Studies 3a and 3b are analysed together this variation is still observed, with significantly more dumbfounded responses recorded for the intuition scenarios (110 instances) than for the reasoning scenarios (79 instances), χ2(1, N = 288) = 6.55, p = .010. However, this combined analysis may be skewed in favour of Study 3b, due to the larger sample size, 101 participants; Study 3a had only 72 participants. Further research and continued replication is needed to confirm the reliability of this finding. When the open-ended responses coded as tautological were included in the analysis of Studies 3a and 3b, the rates of dumbfounding appeared to be closer to those observed in Study 1.\n\n\nTable 2 and Figure 1 show the initial observed rates of dumbfounding for each study. Table 2 and Figure 2 show the revised rates of observed dumbfound responding in each study once the open-ended coded responses from Studies 3a and 3b are included. Rates of dumbfounding reported by Haidt, Björklund, and Murphy (2000) are also included for comparison. Study 2 was a primarily a pilot study, and, as discussed, the observed rates of dumbfounding do not appear to be representative of the phenomenon being studied, as such Study 2 is not included in Figure 2."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#differences-between-the-samples",
    "href": "publications/searching-for-moral-dumbfounding/index.html#differences-between-the-samples",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Differences between the Samples",
    "text": "Differences between the Samples\n\nThe trend in observed rates of dumbfounded responses, across the dilemmas, identified by Haidt, Björklund, and Murphy (2000) appears to also be present in Study 1 (Interview). There does not appear to be a difference between scenarios in the computerised tasks. When the open-ended responses are included, the rates of observed dumbfounding for Cannibal appear to be similar across all the studies included in Figure 2 (two interviews and two computerised tasks). The computerised tasks appear to have higher rates of dumbfounding for both Heinz and Trolley than the interviews. There is a large degree of variation in the observed rate of dumbfounding for Incest between the four studies.\n\n\nIncest recorded higher rates of dumbfounding than the other scenarios in both interview studies (Study 1 and Haidt, Björklund, and Murphy 2000) and, to some degree, in Study 3a, the computer task with a college sample. The rate of dumbfounding observed for Incest with the online sample, in Study 3b, is lower than that observed with the college sample in Study 3a and is also slightly lower than that observed for Cannibal in the online sample. This is surprising, in that, the Incest dilemma is the most commonly cited example (e.g., Haidt 2001; Prinz 2005; Royzman, Kim, and Leeman 2015), and, in Studies 1, 2, and 3a, is the most reliable for eliciting dumbfounding, consistently eliciting higher rates than the other dilemmas. Looking at the ratings of the behaviours in each dilemma for each study may provide some clue as to where this variation comes from. The online sample were less inclined to rate the behaviour in Incest as wrong relative to the participants in the other studies. The percentage of participants initially rating Incest as wrong for each study are as follows: Study 1: 83.87%; Study 2: 87.5%; Study 3a: 84.72%; Study 3b: 70.3%. Furthermore, on the critical slide, the proportion of participants who selected “nothing wrong” for Incest for Study 3b (30.69%; 31 participants) was nearly double the proportion that selected “nothing wrong” for Incest for Study 3a (16.67; 12 participants). When these participants are excluded from the analysis of Study 3b (see Table 4 and Figure 3), the percentage of participants appearing to be dumbfounded by Incest (22.86%; 16 participants; or 40%; 28 participants when open-ended responses are included; N = 70) exceeds the percentage of participants appearing to be dumbfounded by Cannibal (20.88%; 19 participants; or 32.97%; 30 participants when open-ended responses are included; N = 91). As such, it appears that the apparent uncharacteristically low rates of observed dumbfounding for Incest in Study 3b, when compared to Cannibal, may be due to the online sample being less inclined to rate the behaviour as morally wrong rather than a difference in this sample’s ability to provide justifications for their judgements to the two scenarios.\n\n\nTable 4\n\n\nPercentage of participants dumbfounded excluding participants who selected nothing wrong\n\n\n\n\n\n\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\n\n\n\n\nStudy 1 (N = 31)\n\n\n0/25\n\n\n0%\n\n\n11/23\n\n\n47.83%\n\n\n18/20\n\n\n90%\n\n\n3/23\n\n\n13.04%\n\n\n\n\nStudy 2 (N = 72)\n\n\n45/64\n\n\n70.31%\n\n\n46/68\n\n\n67.65%\n\n\n54/70\n\n\n77.14%\n\n\n46/62\n\n\n74.19%\n\n\n\n\nStudy 3a (N = 72)\n\n\n19/58\n\n\n32.76%\n\n\n21/68\n\n\n30.88%\n\n\n31/60\n\n\n51.67%\n\n\n22/57\n\n\n38.6%\n\n\n\n\nStudy 3b (N = 101)\n\n\n16/80\n\n\n20%\n\n\n30/91\n\n\n32.97%\n\n\n28/70\n\n\n40%\n\n\n22/77\n\n\n28.57%\n\n\n\n\n\n\n\n\nFigure\n\n\n\nPercentage of dumbfounded responses when “nothing wrong” is excluded."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#intuition-versus-reasoning",
    "href": "publications/searching-for-moral-dumbfounding/index.html#intuition-versus-reasoning",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Intuition versus Reasoning",
    "text": "Intuition versus Reasoning\n\nHaidt, Björklund, and Murphy (2000) attribute the observed trend in dumbfounded responding to differences in type of scenario. They argue that Heinz is a “reasoning” scenario while Cannibal and Incest are “intuition” scenarios. Prinz (2005) suggests that these “intuition” scenarios have an emotional component, specifically that they elicit disgust, which leads to the judgement. Prinz argues that judgements grounded in disgust are more difficult to justify because they are grounded in emotion rather than reason. The variability between scenarios may be evidence for Haidt et al. prediction that judgements on the “intuition” scenarios would be more difficult to justify than the “reasoning” scenarios.\n\n\nStudy 1, the interview, was the only study to produce robust differences between the scenarios.5 The results of the computerised tasks may indicate that there is no difference between the reasoning scenarios and the intuition scenarios. Alternatively, this may have highlighted a difference between an interview and a computerised task that influences the way people make moral judgements.\n\n\nIt is possible that there exists a social influence in an interview setting that changes the way participants respond (e.g., Asch 1956; Sabini 1995; Staub 2013) and, that the interviewer may be seen as a person in authority, demanding justifications for judgements made (e.g., Milgram 1974). This may motivate participants to identify reasons to justify their judgements, leading to the suppression of dumbfounded responses. On the other hand, it may also motivate participants to heed the counter-arguments offered by the experimenter. This may lead to an interaction between scenario difficulty and social pressure to emerge, with the social pressure leading to fewer dumbfounded responses to the easier “reasoning” scenarios, but leading to more dumbfounded responses to the more difficult “intuition” scenarios. It may be the case that the rates of dumbfounding found in the computer tasks provide something of a crude baseline measure of participants’ initial perception of their own ability to justify their judgement of the scenario, having read the scenario and a number of counter-arguments. In the interview, these initial responses to the scenarios are distilled by the discussion with the experimenter to reflect the variation in difficulty between the scenarios."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#implications",
    "href": "publications/searching-for-moral-dumbfounding/index.html#implications",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Implications",
    "text": "Implications\n\nThe existence of moral dumbfounding has informed various theories of moral judgement either directly (e.g., Cushman, Young, and Greene 2010; Haidt 2001; Hauser, Young, and Cushman 2008; Prinz 2005) or indirectly (Crockett 2013; Cushman 2013; Greene 2008, 2013). The original demonstration of moral dumbfounding remains unpublished in peer reviewed form (Haidt, Björklund, and Murphy 2000) and has not been directly replicated. The studies presented here aimed to replicate and extend this original moral dumbfounding study (Haidt, Björklund, and Murphy 2000) and thus, assess the notion that moral dumbfounding is in fact a psychological phenomenon that can be consistently observed. Study 1 successfully replicated the original study. Study 2 piloted the use of a computer task and recorded unexpectedly high rates of dumbfounded responding. Possible reasons for this were identified and addressed in Studies 3a and 3b. Study 3a and 3b recorded more moderate rates of dumbfounding with two different samples. All three studies successfully elicited dumbfounded responding identified as ( a ) admissions of not having reasons; ( b ) use of unsupported declarations as justification of a judgement; or ( c ) use of undefended tautological response as justification for a judgement; however, differences remain between the interview in Study 1 and the computerised task in Studies 3a and 3b. Taking these responses to be indicators of a state of dumbfoundedness, it appears that moral dumbfounding can be evoked in face-to-face and online contexts. As such, the research presented here may be seen as more support for the existence of intuitionist theories of moral judgement (e.g., Cushman, Young, and Greene 2010; Greene 2008; Haidt 2001; Hauser, Young, and Cushman 2008; Prinz 2005) over rationalist theories (e.g., Kohlberg 1971; Topolski et al. 2013)."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#responding-to-criticisms",
    "href": "publications/searching-for-moral-dumbfounding/index.html#responding-to-criticisms",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Responding to Criticisms",
    "text": "Responding to Criticisms\n\nThe present research did not directly address the questions raised by Royzman, Kim, and Leeman (2015). Those researchers suggest that there are two main factors that lead participants to produce responses that appear to be indicative of dumbfounding. Firstly, they argue that dumbfounded responding occurs as a result of social pressure to avoid appearing “uncooperative” (Royzman, Kim, and Leeman 2015, 299), “inattentive” or “stubborn” (Royzman, Kim, and Leeman 2015, 310). However, recall that the original definition of dumbfounding, which Royzman et al., employ, refers to the “stubborn” maintenance of a judgement. This creates a paradoxical situation whereby presenting as stubborn (as part of a dumbfounded response) occurs as a result of an attempt to avoid appearing stubborn. Secondly, they claim that participants’ judgements can be attributed to either norm-based reasons, or reason of potential harm. This claim is tested by presenting participants with questions relating to norm-based reasons and harm-based reasons, and excluding participants from analysis, based on their responses to these questions. They showed that almost all participants who rated the behaviour as wrong also endorsed at least one of these reasons. When controlling for the endorsing of these reasons Royzman et al. report a dumbfounding estimate of 1/53 which they report to be “not significantly greater than 0/53 (z = 1.00, p = .32)” (Royzman, Kim, and Leeman 2015, 309) leading to the conclusion that, when controlling for norm-based reasons or harm-based reasons, moral dumbfounding does not occur. There are three main issues with the way this conclusion is reached.\n\n\nFirstly, the initial estimate of incidences of dumbfounding was 4/53 (7.55%). Based on the same calculations used by Royzman, Kim, and Leeman (2015), this estimate of 4/53 is significantly greater than 0/53, z = 2.0388386, p= .041. These four participants were then interviewed further, during which, the “inconsistencies” in participants’ “responses were pointed out directly” (Royzman, Kim, and Leeman 2015, 308). Following this interview, Royzman et al. were left with a dumbfounding estimate of 1/53 (which they claim is not significantly greater than 0/53).\n\n\nIt is surprising that, having made the claim that dumbfounding arises as a result of social pressure, providing convincing evidence for this claim required a follow up interview, in which participants are exposed to social pressure. Using the same logic employed by Royzman et al. it would not be surprising if participants revised their responses after being “advised to carefully review and, if appropriate, revise” their responses (Royzman, Kim, and Leeman 2015, 308). From this, it appears that incidences of dumbfounding can be reduced by changing the demands of the social situation. In effect, Royzman, Kim, and Leeman (2015) have shown that moral dumbfounding is sensitive to social pressure. Demanding consistency between judgement and the endorsing of principles that may be relevant for a judgement reduces incidences of dumbfounding, whereas demanding consistency between a judgement and information contained in the vignette leads to increased dumbfounding. This is not the same as their claim that moral dumbfounding is caused by social pressure. Furthermore, the role of social pressure in the reduced incidences of dumbfounding observed is not acknowledged.\n\n\nSecondly, following this interview, Royzman, Kim, and Leeman (2015) are still left with one participant who, by their own criteria, can be identified as dumbfounded (Royzman, Kim, and Leeman 2015, 308). No explanation for the responding of this participant is offered, and cannot be explained by the theoretical position adopted in the conclusion. It is argued that one participant from a sample of 53, is not significantly greater than 0/53, z = 1.00, p = .32. Disregarding this estimate of moral dumbfounding as not statistically significant, p = .32, avoids offering an explanation for a response that is inconsistent with the argument made in the paper.\n\n\nThirdly, and most importantly, the current research identifies dumbfounding as a rare demonstration of the separation between intuitions and reasons for these intuitions. Practical challenges to demonstrating this separation have already been identified: ( a ) post-hoc rationalisation and identification of reasons that are consistent with a judgement; ( b ) the possibility that the intuition emerged as a result of a well-rehearsed reasoned response. The work presented by Royzman, Kim, and Leeman (2015) may be viewed as a practical demonstration of this first challenge; helping participants identify reasons that are consistent with their judgement and providing an opportunity them to endorse these reasons. As previously noted, the endorsing of a reason does not imply that the reason contributed to the judgement. This view of moral dumbfounding presents two methodological considerations that need to be addressed before accepting the claim that judgements in the dumbfounding paradigm can be attributed to either norm-based reasons or harm-based reasons. The first relates to participants’ ability to articulate either harm-based or norm-based reasons. The second relates to the consistency with which these reasons guide judgements.\n\n\nFirstly, the final study reported by Royzman, Kim, and Leeman (2015) does not report whether or not participants who endorsed either norm-based reasons or harm-based reasons also articulated the same reason. The mere endorsing of a principle or reason does not provide evidence that this principle guided the making of a judgement. To illustrate this point, consider the following scenario:\n\n\n\nTwo friends (John and Pat) are bored one afternoon and trying to think of something to do. John suggests they go for a swim. Pat declines stating that it’s too much effort - to get changed, and then to get dried and then washed and dried again after; he says he’d rather do something that requires less effort. John agrees and adds “Oh yeah, and there’s that surfing competition on today so the place will be mobbed”. To which Pat replies “Yeah exactly!”.\n\n\n\nWhen John mentioned the surfing competition Pat immediately adopted it as another reason not to go for a swim however it is clear that this reason played no part in Pat’s original judgement. It is possible that in identifying other reasons that are consistent with a particular judgement researchers may falsely attribute the judgement made to these reasons. The studies described by Royzman, Kim, and Leeman (2015) do not sufficiently guard against the possibility of falsely attributing judgements to reasons endorsed, allowing for the possibility that some participants were falsely excluded from analysis. One way to avoid the false exclusion of participants would be to include an open-ended string response option immediately after the presenting of the vignette, in which participants are invited to provide the reason(s) for their judgement. Participants are then only excluded from analysis if they both articulated and endorsed a given principle.\n\n\nSecondly, consider the harm-based reasons, or the application of the harm principle. Royzman, Kim, and Leeman (2015) argue that if participants do not believe that no harm came from the actions of Julie and Mark then concerns of harm may be considered a legitimate reason for judging the behaviour as wrong. Essentially, they have identified the harm principle as “it is wrong for two people to engage in an activity whereby harm may occur”. Royzman, Kim, and Leeman (2015) argue that the application of this principle provides participants with a legitimate reason for their judgements. If this principle is guiding the judgements of participants, then this principle should be applied consistently across differing contexts. Royzman do not demonstrate that the participants in their sample consistently apply this principle across differing contexts (e.g., contact sports/boxing).\n\n\nTwo indicators, measuring dumbfounding by differing standards, have been identified here: admissions of not having reasons, demonstrating an explicit acknowledgement of the absence of reasons; and unsupported declarations, demonstrating a failure to provide reasons when asked. The materials and measures developed here can be used in follow-up work in order address the methodological issues identified in the work of Royzman, Kim, and Leeman (2015) and assess the strength of the concerns they identified in a more rigorous manner."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#limitations-and-future-directions",
    "href": "publications/searching-for-moral-dumbfounding/index.html#limitations-and-future-directions",
    "title": "Searching for Moral Dumbfounding:",
    "section": "Limitations and Future Directions",
    "text": "Limitations and Future Directions\n\nThe current research recorded variability between the different studies that remains unexplained. The interview recorded variation in responses between the different scenarios that was not observed in the computerised tasks. Possible explanations for this difference between computer task and interview have been offered here, however these are merely speculative and should be investigated further.\n\n\nThe studies presented here are exploratory in design. The aim was to identify whether or not the phenomenon of moral dumbfounding could be elicited in a robust fashion. There was no experimental manipulation and analyses were primarily descriptive. These studies raise significant questions about the mechanisms underlying dumbfounded responses to moral judgement tasks, but clearly indicate that such dumbfounded responses can be reliably elicited, and demonstrate interesting variability. Future research is needed to identify specific variables that may moderate dumbfounding; examples may include meaning maintenance and meaning threat (Heine, Proulx, and Vohs 2006; Proulx and Inzlicht 2012), need for closure (Kruglanski and Webster 1996; Kruglanski 2013), or zeal (McGregor et al. 2001; McGregor 2006a, 2006b; McGregor, Nash, and Prentice 2012)."
  },
  {
    "objectID": "publications/solidarity-matters/index.html",
    "href": "publications/solidarity-matters/index.html",
    "title": "Solidarity Matters:",
    "section": "",
    "text": "The effectiveness of measures introduced to minimise the spread of Severe Acute Respiratory Syndrome CoronaVirus 2 (SARS-CoV-2 or COVID-19) depends on compliance from all members of society. The Irish response to COVID-19 has been framed as a collective effort, fostering national solidarity. However, dominant representations of the national community often unreflexively reaffirm the prototypicality of majority group members, implicitly marginalizing minority group members. This may have implications for adherence behaviours. We propose that majority/minority membership of the national community predicts adherence to COVID-19 health advice via prototypicality and national solidarity. In Study 1, we collected data online from Irish residents (N = 1,185) during the first wave of restrictions in Ireland’s response. In Study 2, we collected data from Irish residents (N = 537) during the second wave of restrictions, with more targeted sampling of minority groups. Based on these two studies, there is no difference between minority and majority group members’ adherence behaviours. However, mediation analysis showed that greater adherence to COVID-19 health advice is shown when group members perceive themselves to be prototypical of the Irish national community, and thereby show greater national solidarity. In Study 3, we manipulated an appeal to adhere to restrictions (N = 689) and show that an inclusive solidarity appeal increased reported intentions to adhere to COVID-19 restrictions compared to an exclusive solidarity appeal among minority group members. These findings suggest that appeals to national solidarity in response to COVID-19 will be most successful when they reference the diversity of the nation.\n\n\nPDF\n\n\nSource Document\n\n\n\n\n\nBibliography\n\n\n\n  Foran, A.-M., Roth, J., Jay, S., Griffin, S. M., Maher, P. J., McHugh, C., Bradshaw, D., Ryan, M., Quayle, M., & Muldoon, O. T. (2021). Solidarity Matters: Prototypicality and Minority and Majority Adherence to National COVID-19 Health Advice. International Review of Social Psychology, 34(1), Article 1. https://doi.org/10.5334/irsp.549"
  },
  {
    "objectID": "publications/solidarity-matters/index.html#abstract",
    "href": "publications/solidarity-matters/index.html#abstract",
    "title": "Solidarity Matters:",
    "section": "",
    "text": "The effectiveness of measures introduced to minimise the spread of Severe Acute Respiratory Syndrome CoronaVirus 2 (SARS-CoV-2 or COVID-19) depends on compliance from all members of society. The Irish response to COVID-19 has been framed as a collective effort, fostering national solidarity. However, dominant representations of the national community often unreflexively reaffirm the prototypicality of majority group members, implicitly marginalizing minority group members. This may have implications for adherence behaviours. We propose that majority/minority membership of the national community predicts adherence to COVID-19 health advice via prototypicality and national solidarity. In Study 1, we collected data online from Irish residents (N = 1,185) during the first wave of restrictions in Ireland’s response. In Study 2, we collected data from Irish residents (N = 537) during the second wave of restrictions, with more targeted sampling of minority groups. Based on these two studies, there is no difference between minority and majority group members’ adherence behaviours. However, mediation analysis showed that greater adherence to COVID-19 health advice is shown when group members perceive themselves to be prototypical of the Irish national community, and thereby show greater national solidarity. In Study 3, we manipulated an appeal to adhere to restrictions (N = 689) and show that an inclusive solidarity appeal increased reported intentions to adhere to COVID-19 restrictions compared to an exclusive solidarity appeal among minority group members. These findings suggest that appeals to national solidarity in response to COVID-19 will be most successful when they reference the diversity of the nation.\n\n\nPDF\n\n\nSource Document\n\n\n\n\n\nBibliography\n\n\n\n  Foran, A.-M., Roth, J., Jay, S., Griffin, S. M., Maher, P. J., McHugh, C., Bradshaw, D., Ryan, M., Quayle, M., & Muldoon, O. T. (2021). Solidarity Matters: Prototypicality and Minority and Majority Adherence to National COVID-19 Health Advice. International Review of Social Psychology, 34(1), Article 1. https://doi.org/10.5334/irsp.549"
  },
  {
    "objectID": "publications/the-political-psychology-of-covid19/index.html",
    "href": "publications/the-political-psychology-of-covid19/index.html",
    "title": "The Political Psychology of COVID-19",
    "section": "",
    "text": "The COVID-19 pandemic has given rise to unprecedented and extraordinary conditions. It represents a profound threat to health and political and economic stability globally. It is the pressing issue of the current historical moment and is likely to have far-reaching social and political implications over the next decade. Political psychology can inform our preparedness for the next phase of the pandemic as well as our planning for a post COVID-19 world. We hope that this special issue will play its part in helping us to think how we manage and live with COVID-19 over the coming decade. In this editorial, we review the key themes arising from the contributions to our special issue and, alongside existing knowledge highlight the relevance of political psychology to finding solutions during this time of crisis. The contributions to this special issue and the pandemic raise many classic topics of central interest to political psychology: leadership, solidarity and division, nationalism, equality, racism, and international and intergroup relations. In our editorial, we offer an analysis that highlights three key themes. First, the importance of sociopolitical factors in shaping behavior during this pandemic. Second, the relevance of political leadership and rhetoric to collective efforts to tackle SARS-COV-2. And third, how sociopolitical cohesion and division has become increasingly relevant during this time of threat and crisis.\n\n\nPDF\n\n\nSource Document\n\n\n\n\nBibliography\n\n\n\n  Muldoon, O. T., Liu, J. H., & McHugh, C. (2021). The Political Psychology of COVID-19. Political Psychology, 42(5), 715–728. https://doi.org/10.1111/pops.12775"
  },
  {
    "objectID": "publications/the-political-psychology-of-covid19/index.html#abstract",
    "href": "publications/the-political-psychology-of-covid19/index.html#abstract",
    "title": "The Political Psychology of COVID-19",
    "section": "",
    "text": "The COVID-19 pandemic has given rise to unprecedented and extraordinary conditions. It represents a profound threat to health and political and economic stability globally. It is the pressing issue of the current historical moment and is likely to have far-reaching social and political implications over the next decade. Political psychology can inform our preparedness for the next phase of the pandemic as well as our planning for a post COVID-19 world. We hope that this special issue will play its part in helping us to think how we manage and live with COVID-19 over the coming decade. In this editorial, we review the key themes arising from the contributions to our special issue and, alongside existing knowledge highlight the relevance of political psychology to finding solutions during this time of crisis. The contributions to this special issue and the pandemic raise many classic topics of central interest to political psychology: leadership, solidarity and division, nationalism, equality, racism, and international and intergroup relations. In our editorial, we offer an analysis that highlights three key themes. First, the importance of sociopolitical factors in shaping behavior during this pandemic. Second, the relevance of political leadership and rhetoric to collective efforts to tackle SARS-COV-2. And third, how sociopolitical cohesion and division has become increasingly relevant during this time of threat and crisis.\n\n\nPDF\n\n\nSource Document\n\n\n\n\nBibliography\n\n\n\n  Muldoon, O. T., Liu, J. H., & McHugh, C. (2021). The Political Psychology of COVID-19. Political Psychology, 42(5), 715–728. https://doi.org/10.1111/pops.12775"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Click on any of the Projects below to participate in my current research:\n\n\n  Moral Characters\n  \n    Complete a 3-5 min survey\n  \n  \n\n\n  \n\n  Individual Differences\n  \n    Complete a longer survey\n  \n  \n  \n\n\n\n  Moral Judgements\n  \n    Make a series of moral decisions (20 min)\n  \n  \n  \n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "music.html",
    "href": "music.html",
    "title": "Music",
    "section": "",
    "text": "I have been playing music from a young age and started writing songs as a teenager. Influences range from heavy, multi-instrumental Prog (Yes, Rush, Pink Floyd and Genesis) to more traditional acoustic folk and blues (Neil Young, Eric Bibb and Tom Waits) and everything in between.\nI have released 2 albums to date: Back to the Drawing Board was released in 2012, and It Will Pass… was released in 2019. Both albums are available online (check out Cillian Mc Hugh on Spotify (also on youtube music, and itunes). Have a listen to the new album below. I also have the unreleased Spectrum which included some songs that made it onto Back to the Drawing Board along with some longer songs that reflect my Prog influences. Some material from Spectrum is available on Soundcloud (in particular see this playlist).\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\nAlbum released in 2019\n\n\n\n\n\n\nMar 13, 2019\n\n\nCillian Mc Hugh\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAlbum released in 2011\n\n\n\n\n\n\nSep 12, 2011\n\n\nCillian Mc Hugh\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAlbum released in 2009\n\n\n\n\n\n\nApr 20, 2009\n\n\nCillian Mc Hugh\n\n\n\n\n\n\nNo matching items\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "music.html#albums",
    "href": "music.html#albums",
    "title": "Music",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\nAlbum released in 2019\n\n\n\n\n\n\nMar 13, 2019\n\n\nCillian Mc Hugh\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAlbum released in 2011\n\n\n\n\n\n\nSep 12, 2011\n\n\nCillian Mc Hugh\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAlbum released in 2009\n\n\n\n\n\n\nApr 20, 2009\n\n\nCillian Mc Hugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "music.html#other",
    "href": "music.html#other",
    "title": "Music",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html",
    "href": "music/albums/back-to-the-drawing-board/index.html",
    "title": "Back to the Drawing Board",
    "section": "",
    "text": "Track information for the album Back to the Drawing Board. Preview the album in the Spotify player below or listen to the full album on on youtube music, Spotify, or itunes. There is also a YouTube playlist at the bottom of this page (and available here)"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#rain",
    "href": "music/albums/back-to-the-drawing-board/index.html#rain",
    "title": "Back to the Drawing Board",
    "section": "Rain",
    "text": "Rain\nVocals: Ciara McHugh Backing Vocals: Cillian McHugh Guitar: Cillian McHugh\n\n\n  \n    \n  \n\n\n\n\nThe sky is grey, it’s raining today  Pearls on the grass  It’s always the same, drops of rain  But the tears keep falling down\nTears in your eyes, misty skies  Vision is blurred Where is the pain, you can’t explain Tears in the rain\nWhy do we cry? Love or loss?\nDrops on the glass trace a path Of patterns and maps Trickling down, from your frown It tickles your cheek\nIt’s always the same, drops of rain On the window pane You wipe your eyes but continue to cry And the rain falls"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#bigness",
    "href": "music/albums/back-to-the-drawing-board/index.html#bigness",
    "title": "Back to the Drawing Board",
    "section": "Bigness",
    "text": "Bigness\nVocals: Cillian McHugh Backing Vocals: Ciara McHugh Guitars: Cillian McHugh Organ: Ciara McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nLook up to see, to see how high can we ever reach the sky what’ll i be before i die and should i wonder why\nstart with one, another makes two are the numbers really true it can’t be done, if we only knew to count them all: impossible to do\ntime goes on and on, every day we’re here adds up to pass the year when it’s done and gone you can’t go back to change the past it doesn’t work like that\nthe universe expands that’s what they say getting bigger every day the big bang blew it all away far and wide past the milky way\n  Verse:\n        C       G(1)    C       G(1)    Am      G(2)    Am      G(2)\n  E||---0---|---0---|---0---|---0---|---0---|---0---|---0---|---0---|\n  B||---3---|---1---|---3---|---1---|---1---|---3--1|---1---|---3---|\n  G||---0---|---0---|---0---|---0---|---2---|---0---|---2---|---0---|\n  D||---2---|---0---|---2---|---0---|---2---|---0---|---2---|---0---|\n  A||---3---|---2---|---3---|---2---|---0---|---2---|---0---|---2---|\n  E||-------|-------|-------|-------|-------|-------|-------|-------|\n\n  Chorus 1\n        F    C G1 Am    F     Am G(2)   F     C  Em     Dm    Am G(2)\n  E||---1---|0-0--0-|---1---|-0--0--|---1---|-0--0--|---1---|-0--0--|\n  B||---1---|1-1--1-|---1---|-1--3--|---1---|-1--0--|---3---|-1--3--|\n  G||---2---|0-0--2-|---2---|-2--0--|---2---|-0--0--|---2---|-2--0--|\n  D||---3---|2-0--2-|---3---|-2--0--|---3---|-2--2--|---0---|-2--0--|\n  A||---3---|3-2--0-|---3---|-0--2--|---3---|-3--2--|-------|-0--2--|\n  E||---1---|-------|---1---|-------|---1---|----0--|-------|-------|\n\n  Bridge:\n        Dm    C  G(1)   Dm    Am G(2)   Dm    C  G(1)   Dm    Am G(2)\n  E||---1---|-0--0--|---1---|-0--0--|---1---|-0--0--|---1---|-0--0--|\n  B||---3---|-1--1--|---3---|-1--3--|---3---|-1--1--|---3---|-1--3--|\n  G||---2---|-0--0--|---2---|-2--0--|---2---|-0--0--|---2---|-2--0--|\n  D||---0---|-2--0--|---0---|-2--0--|---0---|-2--0--|---0---|-2--0--|\n  A||-------|-3--2--|-------|-0--2--|-------|-3--2--|-------|-0--2--|\n  E||-------|-------|-------|-------|-------|-------|-------|-------|\n\n  Chorus 2\n        F     C  Em     F     Am G(2)   F    C G1 Am    Dm    Am G(2)\n  E||---1---|-0--0--|---1---|-0--0--|---1---|0-0--0-|---1---|-0--0--|\n  B||---1---|-1--0--|---1---|-1--3--|---1---|1-1--1-|---3---|-1--3--|\n  G||---2---|-0--0--|---2---|-2--0--|---2---|0-0--2-|---2---|-2--0--|\n  D||---3---|-2--2--|---3---|-2--0--|---3---|2-0--2-|---0---|-2--0--|\n  A||---3---|-3--2--|---3---|-0--2--|---3---|3-2--0-|-------|-0--2--|\n  E||---1---|----0--|---1---|-------|---1---|-------|-------|-------|"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#stars",
    "href": "music/albums/back-to-the-drawing-board/index.html#stars",
    "title": "Back to the Drawing Board",
    "section": "Stars",
    "text": "Stars\nVocals: Ciara McHugh, Cillian McHugh Guitars: Cillian McHugh Electric Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nI watch the sun go down: Purple and red through the scattered clouds. Golden light fills the sky.\nDo you remember by the water? Clouds in the way, sky was grey, It didn’t matter.\nI watch the darkness fall and surround us all. Warmth and light Are faded and gone.\nCold bite on a lonely night. Chill in the air as I sit and stare\nStare at the sky for hours trying to count the stars. So far away, so long ago Does it matter?\nStanding alone, under the moon’s glow Empty night and the starlight. Silver streak shines on your cheek as a star winks out."
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#faith",
    "href": "music/albums/back-to-the-drawing-board/index.html#faith",
    "title": "Back to the Drawing Board",
    "section": "Faith",
    "text": "Faith\nVocals: Cillian McHugh Guitars: Cillian McHugh Electric Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nWhat if I told you that everyone was wrong? wrong in what they believe… And what if I told you that everyone was right right in what they believe … but wrong…?…\nWould you call me mad if I told you right was wrong and wrong was right?\nGood and Evil invented by men trying to explain what we cannot comprehend. Don’t search for the answer ’cause it’s right there to be found… and not there… and right there.\nWould you call me mad if I told you there’s no point? and that’s the point… so what’s the point?\nFollow your heart and believe in what you believe… let it’s music reach you, just listen. And live your life ’cause it’s all that you can know… take good with bad and let the balance flow.\nWould you call me mad if I told you that I knew…?…and didn’t know…because I know…?.."
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#section",
    "href": "music/albums/back-to-the-drawing-board/index.html#section",
    "title": "Back to the Drawing Board",
    "section": "24",
    "text": "24\nVocals: Ciara McHugh, Cillian McHugh Guitars: Cillian McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nVerse 1 Reverse the answer, It’s 24 But what is the question? No one cares anymore God is Forgotten, Who needs heaven’s door We’ve got money\nChorus And the more we learn, Learn about the earth The more we seem to forget Technologies and good careers, we try to beat our peer And 24 is all the time we get\nVerse 2 The edge of the rainbow goes up to 24 On the other side we’re getting fried, more than ever before Chainsaws and trucks expose the forest floor And we let it happen\n\n Intro: \n        F#m     Bm9    F#m      Bm9     F#m7  D  D/A    F#m  Bm9 Bm96\n  F#|---0---|---0---|--0----|---0---|---3---|-0--3--|---0---|-0--0--|\n  C#|---0---|---0---|--0----|---0---|---3---|-0--3--|---0---|-0--0--|\n  A-|---0---|---5---|--0----|---5---|---0---|-5--5--|---0---|-5--4--|\n  E-|---2---|---5---|--2----|---5---|---2---|-5--5--|---2---|-5--4--|\n  B-|---2---|---0---|--2-0h2|---0---|---2---|-3--3--|---2---|-0--0--|\n  F#|---0---|-------|--0----|-------|---0---|----3--|---0---|-------|\n\n  Verse: \n        F#m  Bm9 Bm96   F#m  Bm9 Bm96   F#m  Bm9 Bm96   F#m  Bm9Bm96Bm9\n  F#|---0---|-0--0--|---0---|-0--0--|---0---|-0--0--|---0---|-0--0--0-\n  C#|---0---|-0--0--|---0---|-0--0--|---0---|-0--0--|---0---|-0--0--0-\n  A-|---0---|-5--4--|---0---|-5--4--|---0---|-5--4--|---0---|-5--4--2-\n  E-|---2---|-5--4--|---2---|-5--4--|---2---|-5--4--|---2---|-5--4--2-\n  B-|---2---|-0--0--|---2---|-0--0--|---2---|-0--0--|---2---|-0--0--0-\n  F#|---0---|-------|---0---|-------|---0---|-------|---0---|---------\n\n  Chorus:                                  (not sure of the written timing here)\n       Dmaj7    A      Dmaj7  A  F#m   Dmaj7    A     Bm A      C#m \n  F#|---3---|---3---|---3---|-3--0--|---3---|---3---|-0--3--|---2---|\n  C#|---0---|---0---|---0---|-0--0--|---0---|---0---|-1--3--|---3---|\n  A-|---0---|---0---|---0---|-0--0--|---0---|---0---|-2--0--|---4---|\n  E-|---2---|---0---|---2---|-0--2--|---2---|---0---|-2--0--|---4---|\n  B-|---3---|---2---|---3---|-2--2--|---3---|---2---|-0--2--|---2---|\n  F#|-------|-------|-------|----0--|-------|-------|----3--|-------|"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#chasing-rainbows",
    "href": "music/albums/back-to-the-drawing-board/index.html#chasing-rainbows",
    "title": "Back to the Drawing Board",
    "section": "Chasing Rainbows",
    "text": "Chasing Rainbows\nVocals: Cillian McHugh Guitar: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nto love is to catch a rainbow the sun and the rain, the joy and the pain\nchase a rainbow ’till you catch the wind and it carries you off over the sky washed up left on the moon broken hearts chase rainbows\nswirl for a while with a surface smile but bubbles burst or float away on a breeze\nchase a rainbow ’till you catch the wind and it carries you off over the sky washed up left on the moon broken hearts chase rainbows"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#passing-days",
    "href": "music/albums/back-to-the-drawing-board/index.html#passing-days",
    "title": "Back to the Drawing Board",
    "section": "Passing Days",
    "text": "Passing Days\nVocals: Ciara McHugh Backing Vocals: Cillian McHugh Guitar: Cillian McHugh\n\n  \n    \n  \n\n\n\n\n\nThe days pass. But your heart still cries (But inside, inside it still rains)"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#back-to-the-drawing-board-1",
    "href": "music/albums/back-to-the-drawing-board/index.html#back-to-the-drawing-board-1",
    "title": "Back to the Drawing Board",
    "section": "Back to the Drawing Board",
    "text": "Back to the Drawing Board\nVocals: Ciara McHugh Backing Vocals: Cillian McHugh Guitars: Cillian McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\n\n  Verse:                                                Bridge:\n       G     C     G     C     G     C     G     C      Em D~ G  Am    F\n  E||--3--|--3--|--3--|--3--|--3--|--3--|--3--|--3--  |-0-----3--0-|--(0)--|\n  B||--3--|--3--|--3--|--3--|--3--|--3--|--3--|--3--  |-0--3--3--1-|---1---|\n  G||--0--|--0--|--0--|--0--|--0--|--0--|--0--|--0--  |-0--2--0--2-|---2---|\n  D||--0--|--2--|--0--|--2--|--0--|--2--|--0--|--2--  |-2--0--0--2-|---3---|\n  A||--2--|--3--|--2--|--3--|--2--|--3--|--2--|--3--  |-2-(0)-2--0-|---3---|\n  E||--3--|-----|--3--|-----|--3--|-----|--3--|-----  |-0--2--3----|-------|\n\n  Chorus:\n     Cmaj79  D~  Cmaj79  D~  Cmaj79  Bm~   Em D~  \n  E||--0--|-----|--0--|-----|--0--|--0--|--0-----|\n  B||--3--|--3--|--3--|--3--|--3--|--3--|--0--3--|\n  G||--4--|--2--|--4--|--2--|--4--|--4--|--0--2--|\n  D||--2--|--0--|--2--|--0--|--2--|--0--|--2--0--|\n  A||--3--|-(0)-|--3--|-(0)-|--3--|--2--|--2-(0)-|\n  E||-----|--2--|-----|--2--|-----|-----|--0--2--|\n\nVerse 1 paint a picture, write a song… to illustrate this life the page is yours, your work your life… you do your best to get it right\nbut the colours start to run … words become undone\nChorus 1 cross it out tear it up erase it till there leaves no trace and start again, start again\nVerse 2 ealainteoir ar leathnach bán … dathanna an saol amhran beo le mothúchán … focail don saol\nach theip an ghrá bhris an peann ní leor a bhí ann\nChorus 2 scrois é chuir sa tine é bris é go dtí píosaí beag is tosaigh arís, tosaigh arís\nVerse 3 start again on a brand new page … we all learn with age peann nua, dathanna nua … ach an amhrán chéanna\nnuair a theip an ghrá bhris an peann ní leor a bhí ann\nChorus 3 scrois é chuir sa tine é bris é go dtí píosaí beag is tosaigh arís, tosaigh arís"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#the-price-of-love",
    "href": "music/albums/back-to-the-drawing-board/index.html#the-price-of-love",
    "title": "Back to the Drawing Board",
    "section": "The Price Of Love",
    "text": "The Price Of Love\nVocals: Cillian McHugh Acoustic Guitar: Cillian McHugh Electric Guitar: Maurice McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh Harmonica: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nWaiting… Holding out Hoping time won’t run out Torn between a rock and a hard place Torn between love and happiness\nNeither is enough, neither will suffice Carry on the same - that’s the best advice Hide from the shame, hide from your heart Cause no pain, break no-one’s heart"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#broken-words",
    "href": "music/albums/back-to-the-drawing-board/index.html#broken-words",
    "title": "Back to the Drawing Board",
    "section": "Broken Words",
    "text": "Broken Words\nVocals: Ciara McHugh, Cillian McHugh Guitars: Cillian McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nYou mean it. You say it’s true. I believe it. I feel it too. You said to me, I said to you.\nWords pick me up and carry me. The thrill, the rush, the waves under me. (Surfing the waves)\nBut waves break words are lost. Crashing truths of changing tides, Washed ashore and left behind.\nWords are washed out ripples on the shore. (Washed out waves) Broken words can’t carry me anymore. (Broken, broken words)\nYou said it then. Means nothing now. Meaning lost. (From your) Broken words."
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#the-happy-song",
    "href": "music/albums/back-to-the-drawing-board/index.html#the-happy-song",
    "title": "Back to the Drawing Board",
    "section": "The Happy Song",
    "text": "The Happy Song\nVocals: Cillian McHugh Guitars: Cillian McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh Harmonica: Cillian McHugh\n\n  \n    \n  \n\n\n\n\n\n  Verse:\n       G   Bm7 E7    G   Bm7 E7    G   Bm7 E7  A7    C   D7  G   B7   \n  E||-(3)-----(0)-|-(3)-----(0)-|-(3)-----(0)-(0)-|--0---2---3---2--|\n  B||--3---3---3--|--3---3---3--|--3---3---3---2--|--1---1---3---0--|\n  G||--0---2---1--|--0---2---1--|--0---2---1---0--|--0---2---0---2--|\n  D||--0---0---0--|--0---0---0--|--0---0---0---2--|--2---0---0---1--|\n  A||-(2)--2---2--|-(2)--2---2--|-(2)--2---2---0--|--3-------2---2--|\n  E||--3-------0--|--3-------0--|--3-------0------|----------3------|\n\n  Bridge:\n       G   Bm7 E7    A7 Am7 D D9D D7   G   Bm  Em  C9    C9 E~ E B  D  D7 D6 D   \n  E||-(3)-----(0)-|- 0--0---2-0-----|-(3)------3---3--|-------(0)-|-------------|\n  B||--3---3---3--|--2--1---3-3-3-1-|--3---3---3---2--|--3--1--0--|-3--1--0-----|\n  G||--0---2---1--|--0--0---2-2-2-2-|--0---2---0---0--|--0--0--1--|--2--2----2--|\n  D||--0---0---0--|--2--2---0-0-0-0-|--0---0---2---2--|--2--0--2--|-0--0---0--0-|\n  A||-(2)--2---2--|--0--0-----------|-(2)--2---2---3--|--3--2--2--|-------------|\n  E||--3-------0--|-----------------|--3-------0------|--------0--|-------------|\n\nIf i’ve got somethin’ to say i say it If i’ve got something to do i do it And i feel so good\nDoesn’t even matter if the rains fall And i Don’t care if there’s no sun at all Coz i feel good\nSome things that happen you cannot change What reason why? But if i’m ever feelin down I know\nNothing can be thrown at me i cannot catch all obstacles can be overcome and even if i feel alone i know\ni’ve got my friends there’ll always be someone to call on someone to cheer me up\nwhen i’m happy i’m with friends and when i’m with friends i’m happyv and i believe"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#too-close",
    "href": "music/albums/back-to-the-drawing-board/index.html#too-close",
    "title": "Back to the Drawing Board",
    "section": "Too Close",
    "text": "Too Close\nVocals: Cillian McHugh Guitars: Cillian McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh Harmonica: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nListening to the silence of chords that were never played Humming through the lines of mistakes we made Listening while we wait is it too late\nThe final words he spoke still echoed in your mind Oh as a song comes to a close for a second time True for you now, wondering how"
  },
  {
    "objectID": "music/albums/back-to-the-drawing-board/index.html#sun",
    "href": "music/albums/back-to-the-drawing-board/index.html#sun",
    "title": "Back to the Drawing Board",
    "section": "Sun",
    "text": "Sun\nVocals: Ciara McHugh Backing Vocals: Cillian McHugh Guitar: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nKids are playin’ on the grass, The sun is out, the rain has passed. But the sun can’t warm the void inside When you’re all alone.\nOutside the window, bright and blue, The empty sky is reflected through. Through your eyes and into you, Just the same.\nThe sun shines. Burning inside.\nA gentle breeze shakes the leaves As it blows through the trees. From deep inside you breathe a sigh, Wondering why.\nThe sun is bright the sky is clear. You’re all alone, there’s no one here. Hurting inside you fight the tears, And the sun shines."
  },
  {
    "objectID": "music/albums/it-will-pass/index.html",
    "href": "music/albums/it-will-pass/index.html",
    "title": "It will pass…",
    "section": "",
    "text": "Track information for the album It Will Pass…. Preview the full album in the Spotify player below, or listen to the full album on youtube music, Spotify, or itunes. There is also a YouTube playlist at the bottom of this page (and available here)."
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#the-race",
    "href": "music/albums/it-will-pass/index.html#the-race",
    "title": "It will pass…",
    "section": "The Race",
    "text": "The Race\nVocals: Cillian McHugh Guitars: Cillian McHugh\n\n  \n    \n  \n\n\n\nWhat if you, if you were in my shoes? Would they fit? And would they keep you warm? What if you, if you were in my shoes? What have I missed?\nTime… Time… Time… Goes on, moves fast\nStealing words of green tinted blue Shadows of what we thought we knew No more…\n… time … time … time is caught up, is made up"
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#another-goodbye",
    "href": "music/albums/it-will-pass/index.html#another-goodbye",
    "title": "It will pass…",
    "section": "Another Goodbye",
    "text": "Another Goodbye\nVocals: Maggie McHugh Backing Vocals: Donal McHugh, Cillian McHugh Guitars: Cillian McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Simon O’Donnell\n\n  \n    \n  \n\n\n\n\n\n  Verse:\n          D/F#    G~  \n  E||------------------\n  B||-----(3)----(0)---\n  G||------2------2----\n  D||------0------0----\n  A||------------------\n  E||------2------3----\n\n  Chorus:\n           Bm     A9      G  \n  E||-----(0)----------------\n  B||------3------0------0---\n  G||------2------2------2---\n  D||------0------2------0---\n  A||------2------0----------\n  E||--------------------3---\n\nAll good things come to an end All good things come to an end\nYou don’t want them to You don’t want them to\nAnd all you’ve lived and all you own, and all you’ve learned an all you’ve known And all you’ve worked and all you’ve done And everything you’ve become\nAnd you don’t even know And you don’t even know And you don’t even know And you don’t even know\nAnd those you’ve loved and those you’ve known And those you’ve seen and those you’ve shown The ones who knew and understood The ones you thought would stay for good\nAnd you have to say goodbye And you have to say goodbye And you have to say goodbye And you have to say goodbye"
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#there-are-no-words",
    "href": "music/albums/it-will-pass/index.html#there-are-no-words",
    "title": "It will pass…",
    "section": "There Are No Words",
    "text": "There Are No Words\nVocals: Cillian McHugh Guitar: Cillian McHugh\n\n\n  \n    \n  \n\n\n\nThere are no words, there are no words, there’s just hanging on  And holding out, and holding on, until it’s gone Your broken heart, your broken heart, you thought had healed\nYou check the screen, you check the screen for any sign For her reply, for her reply, but you know inside The empty screen and what that means, it’s all you feel\nYou came too close, you got too close, like you did before And every time, and every time, no matter what you try Will come again, will come again…"
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#unknown-arms",
    "href": "music/albums/it-will-pass/index.html#unknown-arms",
    "title": "It will pass…",
    "section": "Unknown Arms",
    "text": "Unknown Arms\nVocals: Cillian McHugh Guitars: Cillian McHugh Organ: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh Harmonica: Cillian McHugh\n\n  \n    \n  \n\n\n\nChances that I blew And candles that I knew And all now, as I see Is all\nPeople said, and tears were shed We both knew it could only ever be as unknown arms wrapped around me unknown arms wrapped around me unknown arms wrapped around me\nChances that I blew and candles that I knew, And all Is all\nAll night tears were shed We both knew it could only ever be as unknown arms wrapped around me unknown arms wrapped around me unknown arms wrapped around me"
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#summer-rain",
    "href": "music/albums/it-will-pass/index.html#summer-rain",
    "title": "It will pass…",
    "section": "Summer Rain",
    "text": "Summer Rain\nVocals: Cillian McHugh Guitars: Cillian McHugh\n\n  \n    \n  \n\n\n\n\n  Verse:\n           Em     D/F#   C/G           X2\n  E||-------------------(0)----\n  B||-------------------(0)----\n  G||------0------2------0-----\n  D||------2------0------2-----\n  A||------2------0------3-----\n  E||------0------2------3-----\n\n           Em     Bm     D/F#         C9\n  E||--------------------------|------0----------\n  B||-------------3------3-----|------3------3---\n  G||------0------2------2-----|------0------2---\n  D||------2------0------0-----|------2------0---\n  A||------2------2------------|------3----------\n  E||------0-------------2-----|-------------2---\n\n  Chorus:\n  1st time\n           Em     D/F#   C/G       Bm      C/G     D/A\n  E||-------------------(0)----|---0---|---0---|---0---\n  B||-------------------(0)----|---3---|---0---|---0---\n  G||------0------2------0-----|---2---|---0---|---0---\n  D||------2------0------2-----|---0---|---2---|---4---\n  A||------2------0------3-----|---2---|---3---|---5---\n  E||------0------2------3-----|-------|---3---|---5---\n\n  2nd time “And I knowww. . . “\n        Em      Bm      C/G     D/A\n  E||---0---|---0---|---0---|---0---\n  B||---0---|---0---|---0---|---0---\n  G||---7---|---7---|---0---|---0---\n  D||---5---|---4---|---2---|---4---\n  A||---7---|---5---|---3---|---5---\n  E||---0---|---7---|---3---|---5---\n\nSmells like summer rain Smells like summer rain The path is wet But the air is warm\nFeels like summer rain Feels like summer rain My clothes are damp but my skin is warm\n\n\n\nMemories of the past Memories of the past What I’ve learned And what didn’t last\nChorus 1 And I know I’m too young to know But I’ve asked those questions long ago And April rain Still falls\nAnd I know She’s right but she’s wrong And passing showers Only last so long\n\n\n\nMemories of the past Of games on the path Memories of the past What’s changed and what can last\nSmells like summer rain Smells like summer rain What I’ve learned And what’s the same\nChorus 2 And I know what I want to know And I’ve seen it happen, I’ve seen it grow While April rain Still falls\nAnd I know It’s right but it’s wrong And passing showers Only last so long"
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#counting-the-days",
    "href": "music/albums/it-will-pass/index.html#counting-the-days",
    "title": "It will pass…",
    "section": "Counting the Days",
    "text": "Counting the Days\nVocals: Cillian McHugh Guitar: Cillian McHugh\n\n\n  \n    \n  \n\n\n\nWalking home across the city from the train to the plane You’re on your own, there’s no pity there’s no pain in vain When you said good-bye on the platform this time Like you said good-bye the first time\nYou know this place by day but it’s night this time Cold breeze on your face, night air and the city lights shine Across the water reflect the sights Across the water you’ll trek tonight\nGrowing apart, time and distance will take their toll But in your heart you know it’s wort it to make you whole The signs are there but hope remains ’Cause love is to share, it’s worth the strain\nBack on land back to normal doubt left behind You’re almost home, just the drive and in your mind You count the days till the next time Like you count the days each time"
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#if-i-care",
    "href": "music/albums/it-will-pass/index.html#if-i-care",
    "title": "It will pass…",
    "section": "If I Care",
    "text": "If I Care\nVocals: Cillian McHugh Guitars: Cillian McHugh\n\n  \n    \n  \n\n\n\n\n\n  Bridge:\n           G~     D     Cmaj7   C9\n  E||------3------2------3------0---\n  B||------3------3------5------3---\n  G||------0------2------4------0---\n  D||------2------0------5------2---\n  A||--------------------3------3---\n  E||-------------------------------\n\n  Verse (picked):\n           Bm~    D~    Cmaj79  \n  E||-----(0)------------0---\n  B||------3------3------5---\n  G||------2------2------4---\n  D||------0------0------2---\n  A||------2-------------3---\n  E||-------------2----------\n\n  (Final chord is that D~)\n\nWhat if the best advice was wrong? Advice you followed for too long. And you turn to the bottle or the rolling good times, Did you find it there? Do you even care?\nI remember the secret joy, I remember your voice…… ……..your eyes…..\nI don’t know if i care for you, And I don’t know if you want me to. I don’t know…………. ….You…..\nI don’t care if you miss me too Coz I don’t care (how I felt for you) Coz if I care……. I’d care"
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#it-will-pass-1",
    "href": "music/albums/it-will-pass/index.html#it-will-pass-1",
    "title": "It will pass…",
    "section": "It Will Pass",
    "text": "It Will Pass\nVocals: Cillian McHugh Guitar: Cillian McHugh\n\n  \n    \n  \n\n\n\nToo many times, But it will pass and come again, As history plays out, the cycle comes round again. And it’s back to the drawing board but you know its the same song over again same song over again…..same song over again\nYou can only be a happy fool for so long, Till you have to face the truth and accept that you’ve got it wrong. And poets of the past speak truths that will always last And the band sings your heart, drawing distractions\nToo many times, But it will pass and come again."
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#the-last-flight-home",
    "href": "music/albums/it-will-pass/index.html#the-last-flight-home",
    "title": "It will pass…",
    "section": "The Last Flight Home",
    "text": "The Last Flight Home\nVocals: Cillian McHugh Backing Vocals: Maggie McHugh Acoustic Guitar: Cillian McHugh Electric Guitar (solo and slide): David Park Electric Guitar (rhythm and harmonies): Cillian McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Simon O’Donnell\n\n  \n    \n  \n\n\n\n  \n  (Capo 4th fret)\n  Intro/first 2 lines: \n       C#m     B       A       E  \n  G#|--0----|--0----|-(0)(0)|--0--0-|\n  D#|--1----|--1----|--1--1-|--1--1-|\n  B-|--2----|--0----|--2-p0-|--0--0-|\n  F#|--2-2-0|--0-2-0|--3--3-|--2--0-|\n  C#|--0----|--0----|--3--3-|--3--3-|\n  G#|--0----|--3----|-------|-------|\n  \n  Second 2 lines: \n       A       B       A       E  B \n  G#|-(0)---|--0--0-|-(0)---|--0--3-|\n  D#|--1----|--1--1-|--1----|--1--0-|\n  B-|--2----|--0--0-|--2----|--0--0-|\n  F#|--3----|--2--0-|--3----|--2--0-|\n  C#|--3----|--3--3-|--3----|--3--2-|\n  G#|-------|-------|-------|-----3-|\n\nAnd you know it’s done, and you know it’s too late to talk ’cause nothing you can say, and nothing you can do will change what’s come And you always knew this day could come, But your heart still believed\nBelieved you could overcome, believed she was the one, You tried to make it work, you thought you could make it work, And even when the cracks began to show, You never gave up hope\nThe four leaf charm, to hang on her arm, The phone call in the sun, remembering as one, one last time Back to how it used to be and what we had.\nTwo months on, the time you bought is gone, Today’s flight home, the last flight home"
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#three-cars-back",
    "href": "music/albums/it-will-pass/index.html#three-cars-back",
    "title": "It will pass…",
    "section": "Three Cars Back",
    "text": "Three Cars Back\nVocals: Cillian McHugh Guitar: Cillian McHugh Harmonica: Cillian McHugh\n\n  \n    \n  \n\n\n\nSitting three cars back from the red light The fan’s on full, and the windows are down The light’s change as you wipe the sweat from your face\nBut the oncoming cars Mean nothing is going right And you’re stuck three cars back And you’re going nowhere\nTake out your phone and glance at the screen Still no reply, or answer to be see And you know that it’s worth waiting for But you know that you’ve been here before\nAnd is that all this is? Just another drunken kiss? And you’re three cars back And you’re going nowhere"
  },
  {
    "objectID": "music/albums/it-will-pass/index.html#writers-block",
    "href": "music/albums/it-will-pass/index.html#writers-block",
    "title": "It will pass…",
    "section": "Writer’s Block",
    "text": "Writer’s Block\nVocals: Cillian McHugh Everything Else: Lobster Johnson Electric Guitars (rhythm and clean lead): Andrew (Duke) Park Electric Guitars (lead): David Park Bass: Simon O’Donnell Drums: Donal McHugh Organ: Donal McHugh\n\n  \n    \n  \n\n\n\nHe tries to speak his mind and he finds the line but the words that he chose don’t fit, He tries to hide behind the disguise but he’s blind in the maze that he built himself, And he knows he shouldn’t worry And he knows that its only a song and that There really is no hurry But at the same time\nIt’s been a long long time and he’s falling behind and he feels he’s losing touch It’s been a long long time and its on his mind and he knows that that’s enough It’s been a long time he knows It’s been a long time he knows\nHe tries to sing his song but its been so long, the chords sound wrong and the melody’s gone He’s forgotten the lines that were in his mind, he can’t shake the feeling that he’s stealing from someone And he knows there’s something better And he knows that a reason will come And the next time will sound better But at the same time\nIt’s been a long long time and he’s falling behind and he feels he’s losing touch It’s been a long long time and its on his mind and he knows that that’s enough It’s been a long time he knows It’s been a long time he knows"
  },
  {
    "objectID": "music/albums/spectrum/index.html",
    "href": "music/albums/spectrum/index.html",
    "title": "Spectrum",
    "section": "",
    "text": "Spectrum is a collection of the earliest songs I wrote. The shorter songs were later included on Back to the Drawing Board. The three longer songs (Thoughtcrime, Only in Dreams, and Riding Out) have not been properly released and are only available on Soundcloud (see links below)."
  },
  {
    "objectID": "music/albums/spectrum/index.html#i-dust",
    "href": "music/albums/spectrum/index.html#i-dust",
    "title": "Spectrum",
    "section": "(i) Dust",
    "text": "(i) Dust\n\n“war is peace, freedom is slavery, ignorance is strength”\n\nBlue overalls and dusty streets Doublethink and newspeak His poster’s on the wall The party will never fall.\nAlways watching you Watching every move Thought police in the ministry of love Like the powers of above\nTelescreen’s always on Even when you’re gone Never out of sight Watches through the night\nControl you’re entire life Even choose your wife Kids grow up to think like them And they’ll get you in the end\nAlways watching you…\nTwo minutes hate he saw her there Perfect figure and dark hair Chastity ribbon around her waist And the party in her face\nHe hated her from the start Saw a coldness in her heart She was one of them They’ll get you in the end\nAlways watching you…\nEvening in the city the light is dim He saw her had she followed him Hurried home with a quickened pace He wanted to smash her face"
  },
  {
    "objectID": "music/albums/spectrum/index.html#ii-truth",
    "href": "music/albums/spectrum/index.html#ii-truth",
    "title": "Spectrum",
    "section": "(ii) Truth?",
    "text": "(ii) Truth?\nThey came face to face She fell in the corridor he helped her up She put something into his hand\nCould it be true? On the flat piece of paper when he flattened it out Were the words “I love you”\nThey couldn’t meet People all around and the telescreen notice Every move and every sound\nVictory square They made their plans on the crowded street For a moment she held his hand\nMany weeks had passed They’d fallen in love but they knew it could not last\nLife was never the same Hiding off the streets…a dangerous game\n“We are the dead This game we’re playing…we can’t win”\n“Six months a year Five years…Death and life are the same thing”\n“Stop talking about death Don’t you enjoy life? Don’t you like feeling?”\n“We’re not dead yet I’m real, I’m solid I’m alive”\n“We are the dead”…”We are the dead”\n“YOU ARE THE DEAD”"
  },
  {
    "objectID": "music/albums/spectrum/index.html#iii-you-are-the-dead",
    "href": "music/albums/spectrum/index.html#iii-you-are-the-dead",
    "title": "Spectrum",
    "section": "(iii) You are the Dead",
    "text": "(iii) You are the Dead\nDeep inside the ministry of love The walls are white the light is bright\nOthers come and go as the day turns to night Or is it night to day there’s no way to say\nIt’s the place of no darkness and the truncheon blows You’re at their mercy and no one knows\nConfess it all and more on top Anything to make the torture stop"
  },
  {
    "objectID": "music/albums/spectrum/index.html#iv-2-2-5",
    "href": "music/albums/spectrum/index.html#iv-2-2-5",
    "title": "Spectrum",
    "section": "(iv) 2 + 2 = 5",
    "text": "(iv) 2 + 2 = 5\n\n“Freedom is the freedom to say that two plus two make four. If that is granted all else follows”\n\n(instrumental)"
  },
  {
    "objectID": "music/albums/spectrum/index.html#v-the-inevitable",
    "href": "music/albums/spectrum/index.html#v-the-inevitable",
    "title": "Spectrum",
    "section": "(v) The Inevitable",
    "text": "(v) The Inevitable\n\n“Thoughtcrime doesn’t entail death, Thoughtcrime is death”\n\nStrapped into the chair The rules of this are fair The rats would eat his face Unless she could take his place\nIn time they let him go How long he did not know Accepted he was dead With a bullet in his head\nDidn’t know when or where But the hope was always there For a victory of his own To be pure to the bone\nAt last came the day As always is the way Finally it was true He loved him through and through"
  },
  {
    "objectID": "music/albums/spectrum/index.html#part-i",
    "href": "music/albums/spectrum/index.html#part-i",
    "title": "Spectrum",
    "section": "Part I",
    "text": "Part I\nSummer time and the dancing Music playing and the band sings Across the lake you see the lights Colourful glow on a beautiful night\nFootsteps along the path glimmer of white Through the bushes it was love at first sight Her eyes, her arms, her hair…he was lost He helped her hide he showed her where\nWhen the boys came down he stood his ground in the middle of the path He was ready to fight, fight for her that night but it didn’t come to that They cleared off he went across to find her in the dark A few brief words he went back to work she was gone only black\nHolidays came it was still the same She filled his thoughts he dreamt her name Across the street and down the road Was it really her he had to know\nFollowed her home and called her name, she turned around No doubt, it was her, she was found In they went, met the lads…coffee and chat He asked her out for Friday night…simple as that\nFriday at eight he could not wait in a daze all the week University Park instead of the bar just talking by the creek Her past days in a secretive way he loved to hear her speak Drunk with love swimming with love…he was in a dream"
  },
  {
    "objectID": "music/albums/spectrum/index.html#part-ii",
    "href": "music/albums/spectrum/index.html#part-ii",
    "title": "Spectrum",
    "section": "Part II",
    "text": "Part II\nPlywood nailed across the door, left her sleeping on a friend’s floor Nowhere else to go…money running low She wished she’d let him in, explained her past to him Bell rang in the empty hall. No hope at all.\nHe found the deserted house. His heart had been ripped out. He walked the streets alone. He couldn’t stay at home. He longed to see her there. Fought wild without a care. Broken promises and lies, betrayal made him wise."
  },
  {
    "objectID": "music/albums/spectrum/index.html#part-iii",
    "href": "music/albums/spectrum/index.html#part-iii",
    "title": "Spectrum",
    "section": "Part III",
    "text": "Part III\nAnd the memory fades…faded like a dream The morning of the raid…like a sword in between Night and day…searching through the streets Sorrow and dismay…they would never meet\nHow was he to know…? His name it was a lie The white Mercedes rolled…he ran out of time He told him where to go…guilty of the crime Inside she died alone…she couldn’t find the light"
  },
  {
    "objectID": "music/albums/spectrum/index.html#i-natural-selection",
    "href": "music/albums/spectrum/index.html#i-natural-selection",
    "title": "Spectrum",
    "section": "(i) Natural Selection",
    "text": "(i) Natural Selection\nWe grow…Prosperity is unlimited We grow…Exponentially we’re unlimited\nIron bronze and steel, farming and the wheel Farms and walls and towns, we settled down Tools by human hand, we exploit the land Fishing in empty seas, we’re running out of trees\nChorus: Avoiding natural selection Survival of the fittest but it seems we’ve got exemption We may postpone but we can’t escape the question The earth has limits and we’re no exception\nRoads spread near and far for lorry truck and car Factories industry and trade, an easy life was made We can do anything if we try, we even learned to fly New medicines heal the sick, lives saved by a pin prick\nChorus"
  },
  {
    "objectID": "music/albums/spectrum/index.html#ii-final-generation",
    "href": "music/albums/spectrum/index.html#ii-final-generation",
    "title": "Spectrum",
    "section": "(ii) Final Generation",
    "text": "(ii) Final Generation\nGot a job, a car, a home and family A happy life it’s trouble free Famine and wars and deaths on TV Don’t worry about it; it’s not you or me\nIt’s far away and out of sight Not our problem not our fight Global warming and UV light Sure we don’t care we’re doing alright\nWasted time waiting in line, Every day in the way Mine or yours, yours or mine, It’s ourselves that we mind\nTraffic jams in the street;  We’ve all got cars we’ve all got feet Block the roads with empty seats, We all contribute to the heat\nPoverty, inflation, famine devastation Are we the final generation?"
  },
  {
    "objectID": "music/albums/spectrum/index.html#iii-riding-out",
    "href": "music/albums/spectrum/index.html#iii-riding-out",
    "title": "Spectrum",
    "section": "(iii) Riding Out",
    "text": "(iii) Riding Out\n(instrumental)"
  },
  {
    "objectID": "music/albums/spectrum/index.html#iv-we-never-learn",
    "href": "music/albums/spectrum/index.html#iv-we-never-learn",
    "title": "Spectrum",
    "section": "(iv) We Never Learn",
    "text": "(iv) We Never Learn\nIs it too late? Have we missed our chance? Will the world of today burn out or fade away? The answer and the rainbow and the song\nIt can’t go on. Time’s running out. Will we ever listen and will we ever learn? Warnings in time\nWho starts the wars? Who builds the cars? Who cuts down the trees as they poison the air? Who owns the land? Whose God is real? And who gets the food?\nWe need change to save tomorrow To slow the growth and stop killing our world The answer and the rainbow and the song"
  },
  {
    "objectID": "music/other_music/launch-gig-photos/index.html",
    "href": "music/other_music/launch-gig-photos/index.html",
    "title": "Launch Gig (photos and videos)",
    "section": "",
    "text": "A collection of photos from the official launch of It will pass… in The Commercial on the 12th of April. Many thanks to Mags (and others) for her photography skills.\n\n\n\nalbum cover\n\n\nThe Artwork by Celina Buckley \n\n  \n    \n  \n\n\nFull set: Writer’s Block (With CLEF Choir); Counting the Days; There are no Words; Summer Rain; Unknown Arms; The Race; 24; The Happy Song; Back to the Drawing Board; It will pass…; The Last Flight Home; Bigness; Another Goodbye (With CLEF Choir) \n\n  \n    \n  \n\nAnother Goodbye with Clef Choir \n\n\n\nchoir\n\n\nClef Choir \n\n\n\nmaggie and cillian\n\n\nMaggie McHugh & Cillian McHugh \n\n\n\nmaggie and cillian\n\n\nCillian McHugh & Maggie McHugh \n\n\n\nmaggie and cillian\n\n\nMaggie McHugh & Cillian McHugh \n\n\n\ncillian\n\n\nCillian McHugh \n\n\n\nmaggie and cillian\n\n\nCillian McHugh & Maggie McHugh \n\n\n\nmaggie and cillian\n\n\nCillian McHugh & Maggie McHugh \n\n\n\nmaggie and cillian\n\n\nCillian McHugh & Maggie McHugh \n\n\n\ncillian\n\n\nCillian McHugh \n\n\n\nchoir\n\n\nClef Choir \n\n\n\ncillian\n\n\nCillian McHugh \n\n\n\npaul’s photo\n\n\nNiall Carmody, Cillian McHugh & Maggie McHugh"
  },
  {
    "objectID": "music/other_music/videos/index.html",
    "href": "music/other_music/videos/index.html",
    "title": "Videos",
    "section": "",
    "text": "In addition to the studio albums I also have some home video recordings available here.\nThese include recordings of my own songs as well as some covers."
  },
  {
    "objectID": "music/other_music/videos/index.html#there-are-no-words",
    "href": "music/other_music/videos/index.html#there-are-no-words",
    "title": "Videos",
    "section": "There are no Words",
    "text": "There are no Words"
  },
  {
    "objectID": "music/other_music/videos/index.html#it-will-pass",
    "href": "music/other_music/videos/index.html#it-will-pass",
    "title": "Videos",
    "section": "It will pass…",
    "text": "It will pass…"
  },
  {
    "objectID": "music/other_music/videos/index.html#the-race",
    "href": "music/other_music/videos/index.html#the-race",
    "title": "Videos",
    "section": "The Race",
    "text": "The Race"
  },
  {
    "objectID": "music/other_music/videos/index.html#bon-iver-cover-29-strafford-apts",
    "href": "music/other_music/videos/index.html#bon-iver-cover-29-strafford-apts",
    "title": "Videos",
    "section": "Bon Iver Cover: 29 #Strafford APTS",
    "text": "Bon Iver Cover: 29 #Strafford APTS"
  },
  {
    "objectID": "other.html",
    "href": "other.html",
    "title": "Other",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMoral Networks\n\n\n\n\n\nA Shiny App for Making Moral Networks\n\n\n\n\n\n\nDec 10, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nJS Mediation\n\n\n\n\n\nA Shiny App for running JS Mediation\n\n\n\n\n\n\nMay 15, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nChess\n\n\n\n\n\nIn my spare time I also enjoy playing chess…\n\n\n\n\n\n\nMar 18, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "other/chess/index.html",
    "href": "other/chess/index.html",
    "title": "Chess",
    "section": "",
    "text": "In my spare time I also enjoy playing chess. Check out the puzzle of the day from lichess below. Find me on lichess and challenge me to a game at CillianMacAodh.\n\n\n\nchess\n\n\n\n\n\nchess\n\n\n\n\n\nchess\n\n\n\n\n\nchess"
  },
  {
    "objectID": "other/2019-05-15-js-mediation-shinyapp/index.html",
    "href": "other/2019-05-15-js-mediation-shinyapp/index.html",
    "title": "JS Mediation",
    "section": "",
    "text": "In response to this paper by Yzerbyt, Batailler and Judd (2018) which outined a new method of conducting mediation analyses (with less susceptability to false positives than Hayes’ PROCESS) I created a ShinyApp so that their R-package could be used by SPSS users. Upload your SPSS file below and select the variables you wish to compare.\n\n\n \n\n(see https://cillianmacaodh.shinyapps.io/JS_mediation/)\nYzerbyt, V., Muller, D., Batailler, C., & Judd, C. M. (2018). New Recommendations for Testing Indirect Effects in Mediational Models: The Need to Report and Test Component Paths. Journal of Personality and Social Psychology: Attitudes and Social Cognition, 115(6), 929–943. http://dx.doi.org/10.1037/pspa0000132"
  },
  {
    "objectID": "other/2019-12-10-moral-networks-shinyapp/index.html",
    "href": "other/2019-12-10-moral-networks-shinyapp/index.html",
    "title": "Moral Networks",
    "section": "",
    "text": "Following discussions with members of the DAFINET team, I have become increasingly interested in the idea that moral values might be strengthened by the way in which people might be connected by their values. To investigate this, I started playing around with building networks in R. I downloaded some existing data on Moral Foundations Theory from the OSF (see here). In order to play with it more interactively, I built this ShinyApp."
  },
  {
    "objectID": "other/2019-12-10-moral-networks-shinyapp/index.html#instructions",
    "href": "other/2019-12-10-moral-networks-shinyapp/index.html#instructions",
    "title": "Moral Networks",
    "section": "Instructions",
    "text": "Instructions\n\nSelect the range of participants you want included in your network; there are 522 participants, numbered 1-522, you can select the range by inputting the first and last participants in the range.\nSelect the Moral Foundations, or the specific questions you want to look at.\n\n\nNotes\n\nWhen it first loads it will show an error because nothing is selected;\nIf there is no network to be built (e.g., 2 participants who disagree on the only item selected) it will also show up an error\nIf too many participants and too many items are selected it will probably crash (I tried to create a full network of all participants and all items, and after 3 hours I gave up)\nDue to the amount of content in the App it doesn’t fit too well embedded below, so it might going directly to https://cillianmacaodh.shinyapps.io/moral_networks/ to play with it.\n\n\n \n\n(see https://cillianmacaodh.shinyapps.io/moral_networks/)"
  },
  {
    "objectID": "rblog.html",
    "href": "rblog.html",
    "title": "Rblog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nA Shiny App for JS Mediation\n\n\n\n\n\nA decription of how to make a Shiny App for mediation analyses\n\n\n\n\n\n\nAug 23, 2019\n\n\nCillianMacAodh (Cillian McHugh)\n\n\n\n\n\n\n  \n\n\n\n\nR Markdown Workshop\n\n\n\n\n\nMaterials from a workshop on using RMarkdown\n\n\n\n\n\n\nAug 3, 2019\n\n\nCillianMacAodh (Cillian McHugh)\n\n\n\n\n\n\n  \n\n\n\n\nA Lazy Function\n\n\n\n\n\nA description of a function I wrote that does a specific job\n\n\n\n\n\n\nOct 20, 2018\n\n\nCillianMacAodh (Cillian McHugh)\n\n\n\n\n\n\n  \n\n\n\n\nWriting functions - Part two\n\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2017\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nWriting functions - Part one\n\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2017\n\n\nCillian McHugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rblog/2017-07-27-writing_functions_1/index.html",
    "href": "rblog/2017-07-27-writing_functions_1/index.html",
    "title": "Writing functions - Part one",
    "section": "",
    "text": "(This post originally appeared on my R blog)"
  },
  {
    "objectID": "rblog/2017-07-27-writing_functions_1/index.html#writing-functions",
    "href": "rblog/2017-07-27-writing_functions_1/index.html#writing-functions",
    "title": "Writing functions - Part one",
    "section": "Writing functions",
    "text": "Writing functions\nThis post outlines the writing of a basic function. Writing functions in R (R Core Team 2021) is fairly simple, and the usefulness of function writing cannot be conveyed in a single post. I have included “Part one” in the title, and I will add follow-up posts in time.\nThe basic code to write a function looks like this:\nfunction_name &lt;- function(){}\nThe code for the task you want your function to perform goes inside the curly brackets {}, and the object you wish the function to work on goes inside the parenthesis().\n\nThe problem\nI have often found myself using a number of different functions together for multiple variables. For each variable, I need re-type each function. For example, when looking at a variable, I would often run the functions mean(), sd(), min(), max(), and length() together. Each time I wanted to inspect a new variable, I had to type all five functions for the variable in question. For example, looking at the Temp variable, from the airquality dataset in the datasets package, would require typing the following: mean(airquality$Temp), sd(airquality$Temp), min(airquality$Temp), max(airquality$Temp), length(airquality$Temp). This can get very tedious and repetitive.\n\n\nThe solution\nIn response to repeatedly typing these functions together, I created the descriptives() function which combines these frequently used functions into a single function.\n\nThe descriptives() function\nThe descriptives() function combines the functions mean(), sd(), min(), max(), and length() to return a table displaying the mean, standard deviation, minimum, maximum, and length of a vector.1 The code for creating this function is below, each line of code within the function is explained in the comment above (denoted with the # symbol). The code below can be copied and pasted into your R session to create the descriptives() function.\ndescriptives &lt;- function(x){\n\n      # create an object \"mean\" which contains the mean of x\n  mean &lt;- mean(x, na.rm = TRUE)\n\n      # create an object \"sd\" which contains the sd of x\n  sd &lt;- sd(x, na.rm = TRUE)\n\n      # create an object \"min\" which contains the min of x\n  min &lt;- min(x, na.rm = TRUE)\n\n      # create an object \"max\" which contains the max of x\n  max &lt;- max(x, na.rm = TRUE)\n\n      # create an object \"len\" which contains the length of x\n  len &lt;- length(x)\n\n      # combine the objects created into a table\n  data.frame(mean, sd, min, max, len)\n}\nWhen you pass a vector x through the function descriptives(), it creates 5 objects which are then combined into a table. Running the function returns the table:\ndescriptives(airquality$Temp)\n##       mean      sd min max len\n## 1 77.88235 9.46527  56  97 153\n\n\n\nThings to bear in mind when writing functions\n\nTry to give your function a name that is short and easy to remember.\nIf you are writing a longer more complex function, it may be useful to test it line-by-line, before seeing if it “works”; this will help to identify any errors before they cause your function to fail.\nIf the function returns an error, testing the code line by line will help you find the source of the error.\nThe final line of code in a function will be the “output” of the function.\nObjects created within the function are not saved in the global environment: in the descriptives() function, all that is returned is a table containing the variables specified. The individual objects that were created disappear when the function has finished running.\nThe disappearing of objects created within a function described above can be very useful for keeping a tidy working environment.\n\n\n\nConclusion\nI find myself writing functions regularly, for various tasks. Often a function may be specific to a particular task, or even to a particular dataset. One example of such a function builds on the previous post, in which I described how to create a dataframe from multiple files. In practice, I rarely create data frames exactly as described. I usually nest the “read.csv” function within a larger function that also sorts the data, creating a more manageable dataframe, better suited to my purposes; e.g., removing variables that are of no interest or computing/recoding variables. I can then run this function to build my dataframe at the start of a session.\n\n\nReferences"
  },
  {
    "objectID": "rblog/2017-07-27-writing_functions_1/index.html#footnotes",
    "href": "rblog/2017-07-27-writing_functions_1/index.html#footnotes",
    "title": "Writing functions - Part one",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMost of what descriptives() does can also be achieved by the summary() function, however sd() and length() are missing.↩︎"
  },
  {
    "objectID": "rblog/2017-08-27-writing_functions_2/index.html",
    "href": "rblog/2017-08-27-writing_functions_2/index.html",
    "title": "Writing functions - Part two",
    "section": "",
    "text": "(This post originally appeared on my R blog)\nThe current post will follow on from the previous post and describe another use for writing functions."
  },
  {
    "objectID": "rblog/2017-08-27-writing_functions_2/index.html#r-markdown-and-reporting-p-values-in-apa-format",
    "href": "rblog/2017-08-27-writing_functions_2/index.html#r-markdown-and-reporting-p-values-in-apa-format",
    "title": "Writing functions - Part two",
    "section": "R Markdown and reporting p values in APA format",
    "text": "R Markdown and reporting p values in APA format\nThe function described here is designed for use with R Markdown. I would write a post about how great R Markdown is, and how to use it, but there is already a wealth of information out there; see here, here, and here for a sample. This post relates to producing an APA formatted pdf using the papaja package (Aust 2017). Specifically, I describe a function that can be used to report p values correctly according to APA guidelines.\n\nThe problem\nOne of the great things about R Markdown is the “in-line code” option, whereby, instead of typing numbers, you can insert the code for the value you wish to report, and when the document is compiled, the correct number is reported.\nHowever, the reporting of a p value in APA format varies depending on what the p value actually is. It is consistently reported to three decimal places, with no “zero” preceding the decimal point. Values less than “.001” are reported as: “p &lt; .001.” For example, a p value of “.8368621” would be reported as “p = .837”; while a p value of “.0000725” would be reported as “p &lt; .001”.\nThe specific formatting requirements, and the variation in the reporting of the p value depending on the value being reported means that simply including in-line code to generate the p value is not always sufficient.\n\n\nThe solution\nIn order to remove the need tweak the formatting each time I report a new p value, I have created a function to do it for me.1\n\nThe p_report() function\nThe p_report() function takes any number less than 1, and reports it as an APA formatted p value. Let’s say you run a test, and save the p value from that test in the object p1, all you need to type in your R Markdown document then is\n*p* `r paste(p_report(p1))`\nThe p_report() function will remove the preceding zero, correctly identify whether “=” or “&lt;” is needed, and report p1 to three decimal places. Nesting it within paste() ensures that its output is included in the compiled pdf.\nAs in the previous post, the code for creating the function is below, and each line of code within the function is explained in the comment above (denoted with the # symbol). Again, this code can be copied and pasted into your R session to create the p_report() function.\n\np_report &lt;- function(x){\n\n      # create an object \"e\" which contains x, the p value you are reporting,\n      # rounded to 3 decimal places\n\n  e &lt;- round(x, digits = 3)\n\n      # the next two lines of code prints \"&lt; .001\" if x is indeed less than .001\n\n  if (x &lt; 0.001)\n    print(paste0(\"&lt;\", \" \", \".001\"))\n\n      # if x is greater than .001, the code below prints the object \"e\"\n      # with an \"=\" sign, and with the preceeding zero removed\n\n  else\n    print(\n      paste0(\"=\",\n                 \" \",\n                 sub(\"^(-?)0.\", \"\\\\1.\", sprintf(\"%.3f\",e))))\n\n}"
  },
  {
    "objectID": "rblog/2017-08-27-writing_functions_2/index.html#usage",
    "href": "rblog/2017-08-27-writing_functions_2/index.html#usage",
    "title": "Writing functions - Part two",
    "section": "Usage",
    "text": "Usage\nThe best way to illustrate the usage of p_report() is through examples. We will use the airquality dataset and compare the variation in temperature (Temp) and wind speed (Wind) depending on the month.\n\nPreparing the dataset\nFirst we need to load the dataset and make it (more) usable.\n\n      # create a dataframe df, containing the airquality dataset\n\ndf &lt;- airquality\n\n      # change the class of df$Month from \"integer\" to \"factor\"\n\ndf$Month &lt;- as.factor(df$Month)\n\n\n\nWind\nWe can test for differences in wind speed depending on Month. Run an anova and save the p value in an object b.\n\n    # create an object \"aov\" containing the summary of the anova\n\naov &lt;- summary(aov(Wind~Month, data = df))\n\n    # create an object \"b\" containing the p value of aov\n\nb &lt;- aov[[1]][[\"Pr(&gt;F)\"]][1]\n\nThe output of aovis:\n\n\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nMonth         4  164.3   41.07   3.529 0.00879 **\nResiduals   148 1722.3   11.64                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs you can see, the p value is 0.00879.\nIncluding b in-line returns 0.0087901, however if we pass b through p_report() by enclosing paste(p_report(b)) in r denoted back ticks. Typing the following in an R Markdown document:\n*p* `r paste(p_report(b))`\nreturns: p = .009.\n\n\nTemp\nSimilarly, we can test for differences in temperature depending on Month. By using the same names for the objects, we can use the same in-line code to report the p values.\n\n    # create an object \"aov\" containing the summary of the anova\n\naov &lt;- summary(aov(Temp~Month, data = df))\n\n    # create an object \"b\" containing the p value of aov\n\nb &lt;- aov[[1]][[\"Pr(&gt;F)\"]][1]\n\nThe output of aovis:\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nMonth         4   7061  1765.3   39.85 &lt;2e-16 ***\nResiduals   148   6557    44.3                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs you can see, the p value is &lt;2e-16.\nWhen this is run through p_report() using:\n*p* `r paste(p_report(b))`\nwhich will return: “p &lt; .001”.\n\n\nConclusion\nThe p_report() function is an example of using R to make your workflow easier. R Markdown replaces the need to type the numbers you report with the option of including in-line code to generate these numbers. p_report() means that you do not have to worry about formatting issues when these numbers are reported. Depending on how you structure your code chunks around your writing, and how name your objects, it may be possible to recycle sections of in-line code, speeding up the writing process. Furthermore, the principle behind p_report() can be applied to the writing of other functions (e.g., reporting F values or \\(\\chi\\)2).\n\n\nReferences"
  },
  {
    "objectID": "rblog/2017-08-27-writing_functions_2/index.html#footnotes",
    "href": "rblog/2017-08-27-writing_functions_2/index.html#footnotes",
    "title": "Writing functions - Part two",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe function described here, along with the descriptives() function described in the previous post, are part of a package I created called desnum (McHugh 2017). Writing functions as part of a package means that instead of writing the function anew for each session, you can just load the package. Follow up posts will probably describe more functions in the desnum package. If you wish to install the desnum package run the following code:\ndevtools::install_github(\"cillianmiltown/R_desnum\")\n↩︎"
  },
  {
    "objectID": "rblog/2018-10-20-a_lazy_function/index.html",
    "href": "rblog/2018-10-20-a_lazy_function/index.html",
    "title": "A Lazy Function",
    "section": "",
    "text": "It has been quite a while since I posted, but I haven’t been idle, I completed my PhD since the last post, and I’m due to graduate next Thursday. I am also delighted to have recently been added to R-bloggers.com so I’m keen to get back into it."
  },
  {
    "objectID": "rblog/2018-10-20-a_lazy_function/index.html#a-lazy-function",
    "href": "rblog/2018-10-20-a_lazy_function/index.html#a-lazy-function",
    "title": "A Lazy Function",
    "section": "A Lazy Function",
    "text": "A Lazy Function\nI have already written 2 posts about writing functions, and I will try to diversify my content. That said, I won’t refrain from sharing something that has been helpful to me. The function(s) I describe in this post is an artefact left over from before I started using R Markdown. It is a product of its time but may still be of use to people who haven’t switched to R Markdown yet. It is lazy (and quite imperfect) solution to a tedious task.\n\nThe Problem\nAt the time I wrote this function I was using R for my statistics and Libreoffice for writing. I would run a test in R and then write it up in Libreoffice. Each value that needed reporting had to be transferred from my R output to Libreoffice - and for each test there are a number of values that need reporting. Writing up these tests is pretty formulaic. There’s a set structure to the sentence, for example writing up a t-test with a significant result nearly always looks something like this:\n\nAn independent samples t-test revealed a significant difference in X between the Y sample, (M = [ ], SD = [ ]), and the Z sample, (M = [ ], SD = [ ]), t([df]) = [ ], p = [ ].\n\nAnd the write up of a non-significant result looks something like this:\n\nAn independent samples t-test revealed no significant difference in X between the Y sample, (M = [ ], SD = [ ]), and the Z sample, (M = [ ], SD = [ ]), t([df]) = [ ], p = [ ].\n\nSeven values (the square [ ] brackets) need to be reported for this single test. Whether you copy and paste or type each value, the reporting of such tests can be very tedious, and leave you prone to errors in reporting.\n\n\nThe Solution\nIn order to make reporting values easier (and more accurate) I wrote the t_paragraph() function (and the related t_paired_paragraph() function). This provided an output that I could copy and paste into a Word (Libreoffice) document. This function is part of the desnum1 package (McHugh 2017).\n\nThe t_parapgraph() Function\nThe t_parapgraph() function runs a t-test and generates an output that can be copied and pasted into a word document. The code for the function is as follows:\n\n# Create the function t_paragraph with arguments x, y, and measure\n# x is the dependent variable\n# y is the independent (grouping) variable\n# measure is the name of dependent variable inputted as string\n\nt_paragraph &lt;- function (x, y, measure){\n  \n  # Run a t-test and store it as an object t\n  \n  t &lt;- t.test(x ~ y)\n  \n  \n  # If your grouping variable has labelled levels, the next line will store them for reporting at a later stage\n  \n  labels &lt;- levels(y)\n  \n  # Create an object for each value to be reported\n  \n  tsl &lt;- as.vector(t$statistic)\n  ts &lt;- round(tsl, digits = 3)\n  tpl &lt;- as.vector(t$p.value)\n  tp &lt;- round(tpl, digits = 3)\n  d_fl &lt;- as.vector(t$parameter)\n  d_f &lt;- round(d_fl, digits = 2)\n  ml &lt;- as.vector(tapply(x, y, mean))\n  m &lt;- round(ml, digits = 2)\n  sdl &lt;- as.vector(tapply(x, y, sd))\n  sd &lt;- round(sdl, digits = 2)\n  \n  # Use print(paste0()) to combine the objects above and create two potential outputs\n  # The output that is generated will depend on the result of the test\n  \n  \n  # wording if significant difference is observed\n  \n  if (tp &lt; 0.05) \n    print(paste0(\"An independent samples t-test revealed a significant difference in \", \n                 measure, \" between the \", labels[1], \" sample, (M = \", \n                 m[1], \", SD = \", sd[1], \"), and the \", labels[2], \n                 \" sample, (M =\", m[2], \", SD =\", sd[2], \"), t(\", \n                 d_f, \") = \", ts, \", p = \", tp, \".\"), quote = FALSE, \n          digits = 2)\n  \n  # wording if no significant difference is observed      \n  \n  if (tp &gt; 0.05) \n    print(paste0(\"An independent samples t-test revealed no difference in \", \n                 measure, \" between the \", labels[1], \" sample, (M = \", \n                 m[1], \", SD = \", sd[1], \"), and the \", labels[2], \n                 \" sample, (M = \", m[2], \", SD =\", sd[2], \"), t(\", \n                 d_f, \") = \", ts, \", p = \", tp, \".\"), quote = FALSE, \n          digits = 2)\n}\n\nWhen using t_paragraph(), x is your DV, y is your grouping variable while measure is a string value that the name of the dependent variable. To illustrate the function I’ll use the mtcars dataset.\n\n\nApplications of the t_parapgraph() Function\nThe mtcars dataset is comes with R. For information on it simply type help(mtcars). The variables of interest here are am (transmission; 0 = automatic, 1 = manual), mpg (miles per gallon), qsec (1/4 mile time). The two questions I’m going to look at are:\n\nIs there a difference in miles per gallon depending on transmission?\nIs there a difference in 1/4 mile time depending on transmission?\n\nBefore running the test it is a good idea to look at the data2. Because we’re going to look at differences between groups we want to run descriptives for each group separately. To do this I’m going to combine the the descriptives() function which I previously covered here (also part of the desnum package) and the tapply() function.\nThe tapply() function allows you to run a function on subsets of a dataset using a grouping variable (or index). The arguments are as follows tapply(vector, index, function). vector is the variable you want to pass through function; and index is the grouping variable. The examples below will make this clearer.\nWe want to run descriptives on mtcars$mpg and on mtcars$qsec and for each we want to group by transmission (mtcars$am). This can be done using tapply() and descriptives() together as follows:\n\ntapply(mtcars$mpg, mtcars$am, descriptives)\n\n$`0`\n      mean       sd  min  max len\n1 17.14737 3.833966 10.4 24.4  19\n\n$`1`\n      mean       sd min  max len\n1 24.39231 6.166504  15 33.9  13\n\n\nRecall that 0 = automatic, and 1 = manual. Replace mpg with qsec and run again:\n\ntapply(mtcars$qsec, mtcars$am, descriptives)\n\n$`0`\n      mean       sd   min  max len\n1 18.18316 1.751308 15.41 22.9  19\n\n$`1`\n   mean       sd  min  max len\n1 17.36 1.792359 14.5 19.9  13\n\n\n\n\n\nRunning t_paragraph()\nNow that we know the values for automatic vs manual cars we can run our t-tests using t_paragraph(). Our first question:\n\nIs there a difference in miles per gallon depeding on transmission?\n\n\nt_paragraph(mtcars$mpg, mtcars$am, \"miles per gallon\")\n\n[1] An independent samples t-test revealed a significant difference in miles per gallon between the  sample, (M = 17.15, SD = 3.83), and the  sample, (M =24.39, SD =6.17), t(18.33) = -3.767, p = 0.001.\n\n\nThere is a difference, and the output above can be copied and pasted into a word document with minimal changes required.\nOur second question was:\n\nIs there a difference in 1/4 mile time depending on transmission?\n\n\nt_paragraph(mtcars$qsec, mtcars$am, \"quarter-mile time\")\n\n[1] An independent samples t-test revealed no difference in quarter-mile time between the  sample, (M = 18.18, SD = 1.75), and the  sample, (M = 17.36, SD =1.79), t(25.53) = 1.288, p = 0.209.\n\n\nThis time there was no significant difference, and again the output can be copied and pasted into word with minimal changes.\n\n\nLimitations\nThe function described was written a long time ago, and could be updated. However I no longer copy and paste into word (having switched to R markdown instead). The reporting of the p value is not always to APA standards. If p is &lt; .001 this is what should be reported. The code for t_paragraph() could be updated to include the p_report function (described here) which would address this. Another limitation is that the formatting of the text isn’t perfect, the letters (N,M,SD,t,p) should all be italicised, but having to manually fix this formatting is still easier than manually transferring individual values.\n\n\nConclusion\nDespite the limitations the functions t_paragraph() and t_paired_paragraph()3 have made my life easier. I still use them occasionally. I hope they can be of use to anyone who is using R but has not switched to R Markdown yet."
  },
  {
    "objectID": "rblog/2018-10-20-a_lazy_function/index.html#footnotes",
    "href": "rblog/2018-10-20-a_lazy_function/index.html#footnotes",
    "title": "A Lazy Function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo install desnum just run devtools::install_github(\"cillianmiltown/R_desnum\")↩︎\nIn this case this is particularly useful because there are no value labels for mtcars$am, so it won’t be clear from the output which values refer to the automatic group and which refer to the manual group. Running descriptives will help with this.↩︎\nIf you want to see the code for t_paired_paragraph() just load desnum and run t_paired_paragraph (without parenthesis)↩︎"
  },
  {
    "objectID": "rblog/2019-08-03-rmarkdown_workshop/index.html",
    "href": "rblog/2019-08-03-rmarkdown_workshop/index.html",
    "title": "R Markdown Workshop",
    "section": "",
    "text": "This is an unusual post for me, I have avoided writing about R Markdown because there are so many resources already available on the topic (e.g., here, here, and here). However, recently I ran a session on using RMarkdown for my colleagues in the Centre for Social Issues Research. The aim of this was to demonstrate the usefulness of R Markdown (and hopefully convert a few people). For this session I created a set of resources1 aimed at making the transition from SPSS to R Markdown a bit easier. The statistics content of these resources is mainly just some of the simpler standard tests taught to psychology undergraduate students.\nThe complete resources are available on this project page on the OSF. The main purpose of the exercise was to provide people with the tools to create this pdf using this R Markdown template. My hope is that by using this template, SPSS users might make the tranistion to R, and R Markdown (with the help of the wonderful papaja package Aust (2017))."
  },
  {
    "objectID": "rblog/2019-08-03-rmarkdown_workshop/index.html#working-with-dataframes",
    "href": "rblog/2019-08-03-rmarkdown_workshop/index.html#working-with-dataframes",
    "title": "R Markdown Workshop",
    "section": "Working with dataframes",
    "text": "Working with dataframes\nA dataframe is structured much like an SPSS file. There are rows and columns, the columns are named and generally represent variables. The rows (can also be named) generally represent cases. You can have multiple data frames loaded with different names, although they are commonly saved as df (and these can be numbered df1 df2 df3. If your document/code is well organised, it can be useful have a generic name for dataframes that you are working with. This means that much of your code can be recycled (particularly if the variable names are the same - if you run repeated studies, or studies with only minor changes, you will find that there is massive scope for recycling code - both chunks and in-line)\n\nSome basics:\n\nThe entire dataframe can be printed to the console by running the name of the data frame\nThe dollar sign can be used to call specific variables from the data frame i.e., df$variable_name\nA function has the form “function name” followed by parenthesis: function_name().\nThe object that you want to run the function on goes in the parenthesis. e.g., if our dataframe was called df, and age was called age and we wanted to get the mean age we would run mean(df$age).\nSometimes missing data denoted by NA can mess with some functions, to account for this it is helpful to include the argument na.rm = TRUE in the function, e.g., mean(df$age, na.rm = TRUE)\n\nThe mtcars dataset comes with R. For information on it simply type help(mtcars). The variables of interest here are am (transmission; 0 = automatic, 1 = manual), mpg (miles per gallon), qsec (1/4 mile time). Below we practice a few simple functions to find out information about the dataset.\n\nExample code and output:\n\nLoad the mtcars dataset into an object called df using the command df &lt;- mtcars\nView the variable names associated with df by running variable.names(df)\n\n\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\nThe mean miles per gallon can be calculated using mean(df$mpg)\n\n\n\n[1] 20.09062\n\n\n\nThe standard deviation of the same variable is calculated using sd(df$mpg)\n\n\n\n[1] 6.026948\n\n\n\nOr if you want to see basic descriptives use descriptives(df$mpg)3\n\n\n\n      mean       sd  min  max len\n1 20.09062 6.026948 10.4 33.9  32\n\n\n\nTo index by a variable we use square brackets [] and the which() function.\n\nThe following command gets the mean miles per gallon for all cars with manual transmission:\n\nmean(df$mpg[which(df$am==1)])\n\n\n\n\n\n[1] 24.39231"
  },
  {
    "objectID": "rblog/2019-08-03-rmarkdown_workshop/index.html#t-test-transmission-and-mpg",
    "href": "rblog/2019-08-03-rmarkdown_workshop/index.html#t-test-transmission-and-mpg",
    "title": "R Markdown Workshop",
    "section": "T-test: Transmission and MPG",
    "text": "T-test: Transmission and MPG\n\nLoad mtcars and save it in your environment using df &lt;- mtcars\nCreate a new dataframe with a generic name e.g., x using the command: x &lt;- df\nThis command runs the t-test and you can see the output in the console t.test(x$mpg~x$am)\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  x$mpg by x$am\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231 \n\n\n\nThe following code runs the t-test but saves the output as a list t that can be called later: t &lt;- t.test(x$mpg~x$am)\n\n\nAs with dataframes, specific variables within a list can be called using the dollar sign\nTo call the p value simply type t$p.value\n\n\n\n[1] 0.001373638\n\n\n\nTo call the t statistic, type t$statistic\n\n\n\n        t \n-3.767123 \n\n\n\nAnd to call the degrees of freedom, type t$parameter\n\n\n\n      df \n18.33225 \n\n\n\nFinally, to calculate the effect size and save it to an object type td &lt;- cohensD(mpg~am, data=x)\n\nFrom the above we can call each value we need using in-line code to write up our results section as follows\n\nThis is what the paragraph will look like in your Rmd document:\nAn independent samples t-test revealed a significant difference in miles per gallon between cars with automatic transmission (*M* = `r mean(x$mpg[which(x$am==0)])`, *SD* = `r sd(x$mpg[which(x$am==0)])`), and cars with manual transmission, (*M* = `r mean(x$mpg[which(x$am==1)])`, *SD* = `r sd(x$mpg[which(x$am==1)])`), *t*(`r t$parameter`) = `r t$statistic`, *p* `r paste(p_report(t$p.value))`, *d* = `r td.\n\n\nThe above syntax will return the following:\nAn independent samples t-test revealed a significant difference in miles per gallon between cars with automatic transmission (M = 17.15, SD = 3.83), and cars with manual transmission, (M = 24.39, SD = 3.83), t(18.33) = -3.767, p = .001, d = 1.48.\nIf you want to run another t-test later on in your document you simply run it in a code chunk and create new objects (t and td) with the same names as before and you can use the same write up as above to report it."
  },
  {
    "objectID": "rblog/2019-08-03-rmarkdown_workshop/index.html#footnotes",
    "href": "rblog/2019-08-03-rmarkdown_workshop/index.html#footnotes",
    "title": "R Markdown Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe aim of this post is to help make these resources more accessible. As such, there will likely be a lot of duplication between this post and the resources on the OSF.↩︎\nItalics are achieved by placing a star either side of the text you want italicised *italics* = italics; Bold is achieved by placing a double star either side of the text you want italicised **bold** = bold↩︎\nfrom the desnum package↩︎\nThis test, and all tests that follow are for illustration purposes only, I have not checked any assumptions to see if I can run the tests, I just want to provide sample code that you can use for your own analyses.↩︎"
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html",
    "href": "rblog/2019-08-11-js_mediation/index.html",
    "title": "A Shiny App for JS Mediation",
    "section": "",
    "text": "This is a brief post about making my first Shiny App (see also). I made this app following a meeting of the Advancing Social Cognition lab (ASC-Lab) where we discussed this paper by Yzerbyt et al. (2018) proposing a new method for mediation analysis. Any attempt to detail the differences in methods is well beyond the scope of a blog post. The take home message is that the method proposed by Yzerbyt et al. (2018) is less prone to Type I errors (or false positives) than the most commonly used methods (e.g., Hayes 2017). In addition to identifying a problem and proposing a solution, the authors also provide the tools to implement their solution with an R package (Batailler et al. 2019). Unfortunately, not everyone uses R, and this is why I set about developing a simple way for SPSS users to access this new method."
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#set-up-the-dataframe",
    "href": "rblog/2019-08-11-js_mediation/index.html#set-up-the-dataframe",
    "title": "A Shiny App for JS Mediation",
    "section": "Set up the dataframe",
    "text": "Set up the dataframe\nFor ease of reusing code (particularly later on) I’ll save mtcars as a dataframe df and rename the variables of interest as iv (predictor variable), dv (outcome variable), and mediator.\n\ndf &lt;- mtcars          # create df from mtcars\n\n# create new variables with generic names\ndf$dv &lt;- df$qsec      # save 1/4 mile time as dv\ndf$iv &lt;- df$hp        # save horsepower as iv\ndf$mediator &lt;- df$wt  # save weight as mediator"
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#simple-regression",
    "href": "rblog/2019-08-11-js_mediation/index.html#simple-regression",
    "title": "A Shiny App for JS Mediation",
    "section": "Simple Regression",
    "text": "Simple Regression\nBefore running the mediation I’ll run a quick regression to assess the nature of the relationship between the variables.\n\nfit &lt;- lm(dv ~ iv + mediator, data=df)  # save the regression in an object 'fit'\nsummary(fit)                            # show the results\n\n\nCall:\nlm(formula = dv ~ iv + mediator, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8283 -0.4055 -0.1464  0.3519  3.7030 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.825585   0.671867  28.020  &lt; 2e-16 ***\niv          -0.027310   0.003795  -7.197 6.36e-08 ***\nmediator     0.941532   0.265897   3.541  0.00137 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.09 on 29 degrees of freedom\nMultiple R-squared:  0.652, Adjusted R-squared:  0.628 \nF-statistic: 27.17 on 2 and 29 DF,  p-value: 2.251e-07\n\n\nAs you can see from the output, 1/4 mile time is predicted by both horsepower and by weight."
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#simple-mediation",
    "href": "rblog/2019-08-11-js_mediation/index.html#simple-mediation",
    "title": "A Shiny App for JS Mediation",
    "section": "Simple Mediation",
    "text": "Simple Mediation\nNow that we have a picture of the relationships between the variables we can run the mediation analysis. The code for this is detailed below.\n\nJS_model &lt;- mdt_simple(data = df, # create an object 'JS_model'\n                       DV = dv,\n                       IV = iv,\n                       M  = mediator)\nadd_index(JS_model)               # display the results of the mediation\n\nTest of mediation (simple mediation)\n==============================================\n\nVariables:\n\n- IV: iv \n- DV: dv \n- M: mediator \n\nPaths:\n\n====  ==============  =====  ======================\nPath  Point estimate     SE  APA                   \n====  ==============  =====  ======================\na              0.009  0.002  t(30) = 4.80, p &lt; .001\nb              0.942  0.266  t(29) = 3.54, p = .001\nc             -0.018  0.003  t(30) = 5.49, p &lt; .001\nc'            -0.027  0.004  t(29) = 7.20, p &lt; .001\n====  ==============  =====  ======================\n\nIndirect effect index:\n\n- type: Indirect effect \n- point estimate: 0.00885 \n- confidence interval:\n  - method: Monte Carlo (5000 iterations)\n  - level: 0.05 \n  - CI: [0.00346; 0.0155]\n\nFitted models:\n\n- X -&gt; Y \n- X -&gt; M \n- X + M -&gt; Y \n\n\n\nHere we can see that horsepower predicts both 1/4 mile time and weight.\nThere is also an indirect effect of horsepower on 1/4 mile time through weight."
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#the-geography-of-the-shiny-app",
    "href": "rblog/2019-08-11-js_mediation/index.html#the-geography-of-the-shiny-app",
    "title": "A Shiny App for JS Mediation",
    "section": "The Geography of the Shiny App",
    "text": "The Geography of the Shiny App\nThe Shiny App has two panels.\n\nOn the left we have:\n\nThe data upload option\nA dropdown menu for selecting the data you wish to use (the uploaded file, the mtcars data set, or the iris data set)\nDropdown menus for defining each of your variables,\nText describing the App\n\nOn the right we have:\n\nThe output of the regression\nThe output from the mediation analysis\n\n\nThe code for generating these panels is below (comments above relevant lines describe the purpose of the various sections):\n# UI for app\nui&lt;-(pageWithSidebar(\n\n# We use headerPanel() to give a title to our app \n  headerPanel(\"JS Mediation\"),\n  \n# use sidebarPanel() to create the content of the side panel (panel on the left)\n  sidebarPanel\n  (\n# use fileInput() to create a dialogue for inputting a file\n    fileInput(\"file1\", \"Upload SPSS File\",\n              multiple = TRUE,\n              accept = c(\".sav\")),\n# create a horizontal line break\n    tags$hr(),\n    \n# create a dropdown menu for selecting the dataset to be used\n    selectInput(\"dataset\",\"Data:\",\n                choices =list(iris = \"iris\",\n                              mtcars = \"mtcars\",\n                              uploaded_file = \"inFile\"), selected=NULL),\n# create a dropdown menu for selecting the dependent variable to be used\n    selectInput(\"dv\",\"Dependent Variable:\", choices = NULL),\n# create a dropdown menu for selecting the Independent variable to be used\n    selectInput(\"iv\",\"Independent Variable:\", choices = NULL),\n# create a dropdown menu for selecting the mediator to be used\n    selectInput(\"mediator\",\"Mediator:\", choices = NULL) #,\n    \n# use HTML() to input formatted text describing the App\n    ,HTML('In response to \n    &lt;a href=\"https://perso.uclouvain.be/vincent.yzerbyt/Yzerbyt%20et%20al.%20JPSP%202018.pdf\"&gt;this&lt;/a&gt;\n    paper by Yzerbyt, Batailler and Judd (2018) which outined a new method of conducting mediation analyses\n    (with less susceptability to false positives than Hayes’ PROCESS) I created a ShinyApp so that their\n    R-package could be used by SPSS users. Upload your SPSS file above and select the variables you wish\n    to compare.')\n    ,br(),br(),br()\n    ,HTML('&lt;p&gt;Yzerbyt, V., Muller, D., Batailler, C., &amp; Judd, C. M. (2018). New Recommendations for\n    Testing Indirect  Effects in Mediational Models: The Need to Report and Test Component Paths.\n    &lt;em&gt;Journal of Personality and Social Psychology: Attitudes and Social Cognition&lt;/em&gt;, 115(6), \n    929–943. &lt;a href=\"http://dx.doi.org/10.1037/pspa0000132\"\n    class=\"uri\"&gt;http://dx.doi.org/10.1037/pspa0000132&lt;/a&gt;&lt;/p&gt;')\n  ),\n  \n# use mainPanel() to create the panel on the right where the output of our tests will be\n  mainPanel(\n# give a title to the the first output\n    h3(\"Summary of Regression Model\"),\n# report the result of the regression, saved in the object 'fit'\n    verbatimTextOutput(\"fit\"),\n# give a title for the second output\n    h3(\"Mediation Results\"),\n# report the result of the mediation, saved in the object 'mediation'\n    verbatimTextOutput(\"mediation\")\n  )\n))"
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#the-backend-of-the-shiny-app",
    "href": "rblog/2019-08-11-js_mediation/index.html#the-backend-of-the-shiny-app",
    "title": "A Shiny App for JS Mediation",
    "section": "The Backend of the Shiny App",
    "text": "The Backend of the Shiny App\nAbove we have the code for setting up and modifying the look and feel of our app. Below we go through the code for making the app do what it is supposed to do. The code in full is at the bottom of this post, however I have isolated specific sections of code to describe their function.\n\nInputting data from file\nThe code below runs read.spss() on whatever file you have uploaded using the dialogue box in the side panel and creates a dataframe called inFile.\n upload_data&lt;-reactive({\n    inFile &lt;- input$file1\n    if (is.null(inFile))\n      return(NULL)\n    read.spss(input$file1$datapath, to.data.frame = TRUE)\n  })\n  \n  observeEvent(input$file1,{\n    inFile&lt;&lt;-upload_data()\n  })\n\n\n\nSelecting data and variables\nThe code below retrieves information about the dataset that is selected, and displays the variables associated with the selected dataset in the dropdown menus for each of your variables (IV, DV, & mediator).\n# update variables based on the data\n  observe({\n# make sure upload exists\n    if(!exists(input$dataset)) return() \n# retrieve names of columns (variable names) and save as 'var.opts'\n    var.opts&lt;-colnames(get(input$dataset))\n# set var.opts as the options for the drop down menus\n    updateSelectInput(session, \"dv\", choices = var.opts)\n    updateSelectInput(session, \"iv\", choices = var.opts)\n    updateSelectInput(session, \"mediator\", choices = var.opts)\n  })\n\n\nSetting up data for analysis\nBelow we extract the data and variables selected in the dropdown menus and save them as objects that we can use in functions. Specifically we create a list obj which contains the vectors dv, iv, and mediator.\n \n# get data object\n  get_data&lt;-reactive({\n    if(!exists(input$dataset)) return() # if no upload\n    check&lt;-function(x){is.null(x) || x==\"\"}\n    if(check(input$dataset)) return()\n# retrieve the selected data and create objects and     \n    obj&lt;-list(data=get(input$dataset),\n              dv=input$dv,\n              iv=input$iv,\n              mediator=input$mediator\n    )\n    \n# require all to be set to proceed\n    if(any(sapply(obj,check))) return()\n# make sure choices had a chance to update\n    check&lt;-function(obj){\n      !all(c(obj$dv,obj$iv,obj$mediator) %in% colnames(obj$data))\n    }\n    if(check(obj)) return()\n# return 'obj' on completion     \n    obj\n  })\n  \n\n\nRunning the analyses\nNow that we can retrieve the selected data and variables, we can turn them into a dataframe and run our analyses on them.\n\nRegression\nThe code below creates an object output$fit which contains the output of the regression.\n  output$fit &lt;- renderPrint({\n# create an object 'data_list', which is a list that contains the selected data and variables\n    dataset_list &lt;- get_data()\n    \n# isloate the elements in the list as separate objects    \n    a &lt;- dataset_list$dv\n    b &lt;- dataset_list$iv\n    m &lt;- dataset_list$mediator\n    c &lt;- dataset_list$data\n   \n# create a dataframe 'df' from the object 'c' the selected dataset    \n    df &lt;- `colnames&lt;-`(\n      cbind.data.frame(\n# we extract and use the variables from 'c' that have the same names as those selected\n        c[which(colnames(c)==a)],\n        c[which(colnames(c)==b)],\n        c[which(colnames(c)==m)]\n      ), c(\"dv\",\"iv\",\"mediator\"))\n# now we have a dataframe df with 3 variables named 'dv', 'iv', and 'mediator'\n\n# we need to ensure data is numeric\n    df$dv &lt;- suppressWarnings(as.numeric(df$dv))\n    df$iv &lt;- suppressWarnings(as.numeric(df$iv))\n    df$mediator &lt;- suppressWarnings(as.numeric(df$mediator))\n    \n# using the same code previously discussed, we run the regression    \n    fit &lt;- lm(dv ~ iv + mediator, data=df)\n    summary(fit) # show results\n    \n  })\n\n\nMediation\nBelow we follow mostly the same steps to create our dataframe, and this time we run the mediation instead of the regression.\n  output$mediation &lt;- renderPrint({\n# create an object 'data_list', which is a list that contains the selected data and variables\n    dataset_list &lt;- get_data()\n    \n# isloate the elements in the list as separate objects    \n    a &lt;- dataset_list$dv\n    b &lt;- dataset_list$iv\n    m &lt;- dataset_list$mediator\n    c &lt;- dataset_list$data\n    \n# create a dataframe 'df' from the object 'c' the selected dataset    \n    df &lt;- `colnames&lt;-`(\n      cbind.data.frame(\n# we extract and use the variables from 'c' that have the same names as those selected\n        c[which(colnames(c)==a)],\n        c[which(colnames(c)==b)],\n        c[which(colnames(c)==m)]\n      ), c(\"dv\",\"iv\",\"mediator\"))\n# now we have a dataframe df with 3 variables named 'dv', 'iv', and 'mediator'\n    \n# we need to ensure data is numeric\n    df$dv &lt;- suppressWarnings(as.numeric(df$dv))\n    df$iv &lt;- suppressWarnings(as.numeric(df$iv))\n    df$mediator &lt;- suppressWarnings(as.numeric(df$mediator))\n\n# and we run the mediation using the same code as at the beginning of this post    \n    JS_model &lt;- mdt_simple(data = df,\n                           DV = dv,\n                           IV = iv,\n                           M  = mediator)\n    add_index(JS_model)\n  })"
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#footnotes",
    "href": "rblog/2019-08-11-js_mediation/index.html#footnotes",
    "title": "A Shiny App for JS Mediation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe purpose of this post is to demonstrate the code for these analyses, as such there may be issues with the analyses reported - I haven’t checked any assumptions or anything.↩︎\nIn order to enable people to use the app for their own analysis I needed a way for them to upload their data into the app. After a bit of googling I found this example, for uploading .csv files. I copied the code and modified it to include read.spss() from the package foreign instead of read.csv()↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Cillian McHugh - Brief professional bio",
    "section": "",
    "text": "Current position"
  },
  {
    "objectID": "shiny.html",
    "href": "shiny.html",
    "title": "Shiny",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "publications/cognitive-load/index.html",
    "href": "publications/cognitive-load/index.html",
    "title": "Cognitive Load Can Reduce Reason-Giving in a Moral Dumbfounding Task",
    "section": "",
    "text": "Moral dumbfounding occurs when people defend a moral judgment, without reasons in support of this judgment. The phenomenon has been influential in moral psychology, however, despite its influence, it remains poorly understood. Based on the notion that cognitive load enhances biases and shortcomings in human judgment when elaboration is beneficial, we hypothesized that under cognitive load, people would be less likely to provide reasons for a judgment and more likely to be dumbfounded (or to change their judgment). In a pre-registered study (N = 1686) we tested this prediction. Our findings suggest that cognitive load reduces reason-giving, and increases dumbfounding (but does not lead to changes in judgments). Our results provide new insights into the phenomenon of moral dumbfounding while also advancing theory in moral psychology.\nMoral dumbfounding occurs when people defend a moral judgment even though they cannot provide a reason in support of this judgment (Haidt et al., 2000; Haidt 2001; see also McHugh, et al., 2017, 2020). It has traditionally been seen as evidence for intuitionist and dual-process theories of moral judgment (e.g., Crockett 2013; Cushman, Young, and Greene 2010; Cushman 2013; Greene 2008; Haidt 2001; Prinz 2005; though this narrative has been contested, e.g., Guglielmo 2018; Royzman et al., 2015). Despite the influence of moral dumbfounding on the morality literature, the phenomenon is not well understood. Here we present a pre-registered test of one prediction of a dual-process explanation of moral dumbfounding."
  },
  {
    "objectID": "publications/cognitive-load/index.html#method",
    "href": "publications/cognitive-load/index.html#method",
    "title": "Cognitive Load Can Reduce Reason-Giving in a Moral Dumbfounding Task",
    "section": "Method",
    "text": "Method\n\nParticipants and design\nThis study was a between subjects design. The dependent variable was rates of reason-giving/dumbfounding (measured using the critical slide with 3 response options: 1: reason-giving; 2: nothing-wrong; 3: dumbfounded response - admission). The primary independent variable was cognitive load with two levels: present and absent. To manipulate cognitive load, a stream of numbers scrolled across the screen above the question text, and participants were required to pay attention to how many times they saw a given number. The scenario served as a secondary independent variable, we used four scenarios: Julie and Mark (Incest), Jennifer (Cannibal), Trolley, Heinz (see Supplementary Materials for full text of each).\nA total sample of 1899 participants (984 female, 876 male, 17 non-binary, 1 other, 5 prefer not to say; Mage = 43.22, min = 18, max = 84, SD = 15.85) started the survey. Participants in this sample were recruited from Prolific (nUK = 963, nUS = 936).1\nParticipants who failed both manipulation checks (n = 7) or who had missing data for the measures of interest were removed, leaving a total sample of 1686 participants (867 female, 799 male, 14 non-binary, 1 other, 5 prefer not to say; Mage = 43.81, min = 18, max = 83, SD = 15.76), nUK = 842, nUS = 844.\n\n\nProcedure and materials\nData were collected using an online questionnaire developed with jsPsych and distributed with cognition.run. Participants were presented with one of four moral scenarios (Julie and Mark, Jennifer, Trolley, Heinz, see supplementary materials for full wording), previously used in studies of moral dumbfounding (McHugh et al., 2017). Participants rated on a 7-point Likert scale how right or wrong the behavior of the character in the scenario was (where, 1 = Morally wrong; 4 = neutral; 7 = Morally right), and were given an opportunity to provide reasons for their judgment. Following this, participants were presented with a series of counter-arguments, which refute commonly used justifications for rating the behavior as “wrong” (see supplementary materials for full text of scenarios and all counter-arguments).\nDumbfounding was measured using the critical slide (developed by McHugh et al., 2017). This contained a statement defending the behavior and a question as to how the behavior could be wrong (e.g., “Julie and Mark’s behavior did not harm anyone, how can there be anything wrong with what they did?”). There were three possible answer options: (a) “It’s wrong, and I can provide a valid reason” (reasons); (b) “It’s wrong, but I can’t think of a reason” (an admission of not having reasons); (c) “There is nothing wrong”. The order of these response options was randomized. Participants who selected (a) were prompted to type a reason. The selecting of an option (b), the admission of not having reasons, was taken to be a dumbfounded response.2 We note that this measure provides a conservative measure of dumbfounded responding [see McHugh et al. (2017) for discussion). A key advantage of this measure of dumbfounding is its suitability for use with cognitive load manipulations. The task requirements for each of the three response options are qualitatively the same (selecting a response), eliminating the potential confounding influence of different types of task requirements. Importantly, participants who selected (a) were only prompted to provide a reason after their response to the critical slide had been submitted and recorded, and the survey had proceeded to the next page. Participants did not know they would be required to provide a reason prior to the presentation of this prompt.\nWe included a video stream of numbers scrolling above the question text for our cognitive load manipulation, drawing on Greene et al. (2008). The video was wide enough to display 3 numbers at a time, and the numbers scrolled past at a speed of 2 numbers per second. Participants were asked to attend to and report (on a subsequent page) how many times a particular number appeared in the stream, while answering the target question. Following an initial training task, the video was presented while participants made their initial judgments, while they responded to the critical slide, and while they were providing their revised judgments.\nTwo attention check tasks were included for all participants, these included a brief paragraph of text where instructions for the correct response were embedded within the text. The wording of the text was misleading such that if participants skimmed or only read some of the text they would likely provide an incorrect response.\nParticipants clicked on the survey link and were randomly assigned to either the experimental condition or the control condition, within which they were randomly presented with one of the four scenarios. The study was complete within 5 minutes."
  },
  {
    "objectID": "publications/cognitive-load/index.html#results",
    "href": "publications/cognitive-load/index.html#results",
    "title": "Cognitive Load Can Reduce Reason-Giving in a Moral Dumbfounding Task",
    "section": "Results",
    "text": "Results\nOne thousand three hundred sixty-five participants (80.96%) rated the behavior described as wrong initially, and one thousand three hundred forty three participants (79.66%) rated the behavior as wrong at the end of the task. Initial ratings (M = 2.26, SD = 1.63) were significantly more severe than revised ratings (M = 2.34, SD = 1.66), t(1685) = -2.6888442, p = .007; d = 0.07. Inspection of the binned judgments revealed that two hundred (11.86%) participants changed the valence of their judgments, breakdown of the changes in judgments is in Table 16 (full sample) and Table 17 (by scenario) in the supplementary materials.\nA 2 \\(\\times\\) 2 factorial ANOVA revealed significant differences in initial judgments depending on both condition F(1, ,, , 1678) = 26.65, p &lt; .001, partial \\(\\eta\\)2 = .016, and scenario F(3, ,, , 1678) = 69.3, p &lt; .001, partial \\(\\eta\\)2 = .110. Participants under cognitive load were significantly (p &lt; .001) less harsh in their judgments (M = 2.46, SD = 1.75) than those in the control condition (M = 2.07, SD = 1.49). Participants rated Jennifer as the most wrong (M = 1.53, SD = 1.13), followed by Julie and Mark (M = 2.05, SD = 1.65, p &lt; .001), then Heinz (M = 2.49, SD = 1.65, p &lt; .001), with Trolley receiving the least severe judgment (M = 2.98, SD = 1.69, p &lt; .001). There was no significant condition \\(\\times\\) scenario interaction F(3, ,, , 1678) = 0.46, p = .708, partial \\(\\eta\\)2 &lt; .001.\nA 2 \\(\\times\\) 2 factorial ANOVA revealed significant differences in revised judgments depending on both condition F(1, ,, , 1678) = 12.82, p &lt; .001, partial \\(\\eta\\)2 = .008, and scenario F(3, ,, , 1678) = 80.69, p &lt; .001, partial \\(\\eta\\)2 = .126. Participants under cognitive load were significantly (p &lt; .001) less harsh in their judgments (M = 2.47, SD = 1.71) than those in the control condition (M = 2.2, SD = 1.59). Participants rated Jennifer as the most wrong (M = 1.54, SD = 1.12), followed by Julie and Mark (M = 2.15, SD = 1.73, p &lt; .001), then Heinz (M = 2.52, SD = 1.58, p = .003), with Trolley receiving the least severe judgment (M = 3.14, SD = 1.72, p &lt; .001). There was no significant condition \\(\\times\\) scenario interaction F(3, ,, , 1678) = 1.34, p = .260, partial \\(\\eta\\)2 = .002.\nDumbfounding was recorded using the critical slides, participants who selected the admission of not having reasons on the critical slide were identified as dumbfounded. Four hundred and seventeen participants (24.73%) selected “It’s wrong but I can’t think of a reason”. One thousand and thirty-two participants (61.21%) selected “It’s wrong and I can provide a valid reason”; and two hundred and thirty-seven participants (14.06%) selected “There is nothing wrong”.\nA chi-squared test for independence revealed a significant association between experimental condition and response to the critical slide, \\(\\chi\\)2(2, N = 1686) = 25.485, p &lt; .001, V = 0.1229449, the observed power was 0.997. As predicted, under cognitive load fewer participants (458; 55.4479419%) provided reasons than in the control condition (574; 66.744186%), and more participants (245; 29.6610169%) selected “It’s wrong but I can’t think of a reason.” than in the control group (172; 20%). The responses to the critical slide for the experimental group (N = 826) and the control group (N = 860) are displayed in Figure @ref(fig:S6ch5S6fig1criticalconditionb). The observed counts, expected counts and standardised residuals are displayed in Table @ref(tab:S6tab1dumb).\nThe setup of the jsPsych script ensured we collected response time data for the critical slide as well as the corresponding responses for the cognitive load task (e.g., “how many times did you see the number 3?”). Combining these items allowed us to develop a measure of participants’ performance on the secondary task for the critical slide (we also have this information for the revised judgment, however, a typo in the jsPsych script meant we cannot develop this measure for the initial judgment; we did not record reaction time for the practice task).\nFor the critical slide, 383 participants (46.37%) responded correctly to the secondary task (while for the revised judgment only 12 participants [1.45%] responded correctly to the secondary task). There was no significant difference in responses to the critical slide between participants who provided an accurate response and those who provided an inaccurate response to the secondary task, \\(\\chi\\)2(2, N = 1686) = 4.248, p = .120, V = 0.0717154, the observed power was 0.438, see Table 24 in the supplementary materials.\nWe additionally conducted an equivalence test to investigate if rates of reason giving varied depending on performance in the cognitive load task. We recoded responses to the critical slide as 1 = reason given, 0 = reason not given. Our sub sample in the experimental group contained a total of n = 826 participants and with this sample we can detect equivalence at the level of d = .1017 with 80% power. The equivalence test was non-significant, t(804) = 0.14, p = .443 given equivalence bounds of -0.051 and 0.051 (on a raw scale) and an alpha of 0.05. The null hypothesis test was non-significant, t(804) = -1.31, p = .189, given an alpha of 0.05. Thus while we did not find a significant effect for task performance, we cannot conclude that task performance had no effect on reason-giving/response to the critical slide.\nWe conducted a follow-up regression analysis to attempt to disentangle the effect of the cognitive load condition vs performance on the cognitive load task, on reason-giving. As expected the overall model significantly predicted reason-giving \\(R^2 = .01\\), \\(F(2, 1,683) = 12.38\\), \\(p &lt; .001\\). Participants in the control condition were significantly more likely to give reasons than participants who provided the correct response \\(b = -0.14\\), 95% CI \\([-0.20, -0.08]\\), \\(t(1683) = -4.62\\), \\(p &lt; .001\\), and participants who provided an incorrect response \\(b = -0.09\\), 95% CI \\([-0.15, -0.04]\\), \\(t(1683) = -3.24\\), \\(p = .001\\). There was no significant relationship between rates of reason-giving and whether participants provided a correct or an incorrect response \\(b = 0.05\\), 95% CI \\([-0.02, 0.11]\\), \\(t(1683) = 1.35\\), \\(p = .177\\)\n\n\n\n\nResponses to critical slide depending on cognitive load\n\n\n\n\n(#tab:S6tab1dumb)\n\n\nObserved counts, expected counts, and standardised residuals for each response to the critical slide depending on cognitive load (full sample)\n\n\n\n\n\n\nCognitive Load\nControl\n\n\n\n\nObserved count\nReasons\n458\n574\n\n\n\nDumbfounded\n245\n172\n\n\n\nNothing Wrong\n123\n114\n\n\nExpected count\nReasons\n505.59\n526.41\n\n\n\nDumbfounded\n204.3\n212.7\n\n\n\nNothing Wrong\n116.11\n120.89\n\n\nStandardised residuals\nReasons\n-4.76**\n4.76**\n\n\n\nDumbfounded\n4.6**\n-4.6**\n\n\n\nNothing Wrong\n0.97\n-0.97\n\n\n\n\nNote. * = sig. at &lt; .05; ** = sig. at &lt; .001\n\n \n\n\nThis pattern was observed for all scenarios individually with the exception of Julie and Mark, which showed no association between experimental condition and cognitive load, \\(\\chi\\)2(2, N = 412) = 0.489, p = .783, V = 0.0344471, power = 0.089. The association was significant for Jennifer \\(\\chi\\)2(2, N = 432) = 17.329, p &lt; .001, V = 0.2002813, power = 0.969, Trolley \\(\\chi\\)2(2, N = 424) = 10.953, p = .004, V = 0.1607228, power = 0.851, and Heinz, \\(\\chi\\)2(2, N = 418) = 7.158, p = .028, V = 0.1308616, power = 0.666, see Figure @ref(fig:S6ch5S6fig2criticalconditionb). Supplementary Tables 20-23 show the direction of the effect for each scenario. Under cognitive load, fewer participants provided reasons and more participants provided a dumbfounded response for Jennifer, Trolley, and Heinz\nGiven the null result for Julie and Mark, we conducted an exploratory follow-up (not-preregistered) equivalence test, to investigate if our results provided evidence for the absence of an effect for cognitive load. Our key dependent variable was reason-giving, operationalized by participants response to the critical slide. As such, our equivalence test focused specifically on reason-giving, we recoded responses to the critical slide as 1 = reason given, 0 = reason not given. Our sub sample who responded to the Julie and Mark scenario contained a total of n = 412 participants. With this sample we can detect equivalence at the level of d = .204 with 80% power.\nThe equivalence test was non-significant, t(406) = 1.38, p = .085 given equivalence bounds of -0.102 and 0.102 (on a raw scale) and an alpha of 0.05. The null hypothesis test was non-significant, t(406) = -0.69, p = .489, given an alpha of 0.05. We did not find equivalence at the level of d = .204, neither did we find a significant effect.\n\n\n\nResponses to critical slide and for the experimental group and the control group for each scenario\n\n\nA chi-squared test for independence revealed a significant association between scenario and response to the critical slide, \\(\\chi\\)2(6, N = 1686) = 61.341, p &lt; .001, V = 0.1907417, the observed power was 1. Participants were significantly more likely to select “There is nothing wrong” for Julie and Mark (p = .002), more likely to provide reasons (p = .002) and less likely to select “There is nothing wrong” (p &lt; .001) for Jennifer, and more likely to be dumbfounded by Trolley (p = .031).\nA multinomial logistic regression was conducted to test the effects of cognitive load and scenario on dumbfounded responding. Overall the model was significant, \\(\\chi\\)2(8, N = 1686) = 95.9, p &lt; .001, and explained between 6.07% (Cox and Snell R square) and 7.22% (Nadelkerke R squared) of the variance in responses to the critical slide, the observed power was 1. Participants in the control condition were significantly less likely to provide a dumbfounded response than to provide reasons, Wald = 25.04, p &lt; .001, OR = 0.5534159, 95% CI [0.4389422, 0.6977437], in addition, participants in the control condition were also signifcantly less likely to select “There is nothing wrong”, than to provide reasons, Wald = 5.23, p = .022, OR = 0.7149973, 95% CI [0.5362742, 0.953283]. For Jennifer, participants were significantly less likely to select “There is nothing wrong” than to provide a reason, Wald = 30.87, p &lt; .001, OR = 0.226737, 95% CI [0.1343332, 0.3827027]; while for Trolley participants were significantly more likely to present as dumbfounded than to provide a reason, Wald = 6.89, p = .009, OR = 1.5452346, 95% CI [1.1164474, 2.1387035]."
  },
  {
    "objectID": "publications/cognitive-load/index.html#implications-limitations-and-future-directions",
    "href": "publications/cognitive-load/index.html#implications-limitations-and-future-directions",
    "title": "Cognitive Load Can Reduce Reason-Giving in a Moral Dumbfounding Task",
    "section": "Implications, Limitations, and Future Directions",
    "text": "Implications, Limitations, and Future Directions\nThese studies offer new understandings of the phenomenon of moral dumbfounding. While our studies illustrate the complexity of attempting to understand moral judgments, we do demonstrate some reliable patterns that offer support to our core theoretically-informed hypotheses. Furthermore, these studies showcase a methodology for measuring dumbfounding under different empirical conditions, offering a path for future researchers to explore the role of other contextual and individual difference variables in influencing moral dumbfounding.\nFrom a theoretical perspective, our findings are consistent with a dual-process explanation of moral dumbfounding, whereby dumbfounding occurs when an intuitive/habitual response is in conflict with a deliberative response (Evans 2007; again we note that the assumption of dual-processes is not necessarily required, and that this prediction is also consistent with the unimodel, Kruglanski and Gigerenzer 2011; or categorization approaches McHugh et al. 2022). In line with existing theorizing on moral judgments (Cushman 2013; McHugh et al. 2022; Railton 2017), we propose that an individual may make an intuitive judgment and that subsequent attempts to identify reasons for this judgment occur through deliberation. If deliberation is unsuccessful and the individual does not revise their judgment, dumbfounding is observed. In support of this explanation, our studies demonstrated that by reducing the cognitive resources available for deliberation, reason-giving was reduced and dumbfounding increased.\nOur results do display some unexplained variability, with the classic dumbfounding scenario (Julie and Mark) showing no significant change in responding with increased cognitive load (we note that the pilot studies used only this scenario and also showed mixed results). An exploratory follow-up test for equivalence did not provide evidence for the absence of an effect; thus, our results regarding Julie and Mark are inconclusive.\nOne potential explanation for this may point to an overall tendency for participants to be less likely to provide reasons for their judgment of Julie and Mark than for the other scenarios. This would be in line with its apparent standing as the classic dumbfounding scenario (Haidt 2001; Royzman, Kim, and Leeman 2015). Examination of Figure 3 seems to provide some support for this interpretation. In the control condition, the rate of providing reasons was lowest for Julie and Mark, and this remained descriptively lower than rates of reason-giving for both Heinz and Jennifer in the cognitive load conditions (i.e., rates of reason-giving for both Heinz and Jennifer were higher under cognitive load than rates of reason-giving in the control condition for Julie and Mark). It is possible that any effect of cognitive load on reason-giving was confounded by this lower base rate of reason-giving. We note however that this interpretation is not necessarily supported when the results for Julie and Mark are compared with the results for Trolley. That is, rates of reason-giving for Trolley under cognitive load were lower than rates of reason-giving for Julie and Mark in either condition. Future research should attempt to better understand these findings.\nIt is also possible that these inconclusive results reflect a lack of statistical power. Recall that a sample of N = 1966 was required to detect a small effect size (V = .07) with 80% power. It is possible that cognitive load does reduce reason-giving, even in the Julie and Mark scenario, but that our sub-sample of a sample of N = 412 did not have sufficient power to detect this effect. We cautiously present two reasons why this interpretation may be plausible. First, our equivalence test was sufficiently powered to detect equivalence at the level of d = .204, however, this failed to show equivalence. Second, all five pilot studies used the Julie and Mark dilemma, and while the results of these appear to be similarly inconclusive, the direction in the pattern of results remains consistent across all studies – that is the opposing pattern of results has not been shown in any study. None of the studies conducted showed an increase in reason-giving under cognitive load (even a non-significant increase).\nWe note that while we did not find any significant differences in responding to the critical slide depending on performance for the cognitive load task, neither did we find evidence of equivalence. It is plausible that the quality of engagement with the secondary task will impact the influence of this secondary task on a moral judgment task, and this provides a further avenue for future research.\nIt may be possible that some of the observed variability in our results can be attributed to methodological limitations. Classic studies of dual-process conflict are characterized by binary response options [an intuitive response contrasted with a deliberative response; Evans (2007)], whereas our studies included three response options that varied in their relative amount of deliberation. As such the dumbfounding paradigm is more complex than classic binary response paradigms, and cognitive load manipulations may not be well suited for this additional complexity. It is also possible that this variability reflects a lack of statistical power to detect small effects for the specific scenario.\nIt is also possible that these results provide evidence that the conflict in dual-process explanation tested here is only part of the story, illustrating that moral dumbfounding displays high variability and context dependency (as with moral judgment more generally, see McHugh et al. 2022). For example, a further complication is that responses in the dumbfounding paradigm may involve competing intuitions. For example, participants may have intuitions relating to the nature of moral knowledge, such as moral judgments should be justifiable by reasons, that may become salient during the course of the study. This means that, in addition to experiencing a conflict between habitual and deliberative responding, participants may also experience competing intuitions. The relative salience of these competing intuitions may depend on a range of factors, including features of the scenario (e.g., severity/seriousness of the transgression, the kind of transgression), features of the characters in the scenario, personal familiarity with discourse around scenario content, and other factors such as current goals. Future research should unpack the influences of these competing intuitions.\nOne interesting finding that emerged that was not hypothesized was the apparent influence of cognitive load on participants’ judgements. Under cognitive load, participants were significantly less harsh in their judgments (note that this appears to reflect judgments of lower extremity rather than a change in the valence of participants’ judgments). This was true for both initial judgments and revised judgments, suggesting that this is more than just an increased receptiveness to the counter-arguments under cognitive load (initial judgments were made before any counterarguments were presented). Future research should follow up on this finding, and investigate the influence of cognitive load on moral judgments more generally, e.g., does it only influence specific kinds of judgments (such as utilitarian judgments specifically, see Greene et al., 2008).\nOur findings have relevance for society more broadly. Moral considerations inform a range of behaviors and decisions, both at the individual level and at the societal level (Sinnott-Armstrong et al., 2010). It is likely that moral dumbfounding may be a contributing factor to moral disagreement on contentious issues. Our studies provide evidence that dumbfounding may be more prevalent in situations that constrain people’s ability to engage in deliberation. In practice, this suggests that when discussing contentious moral issues, disagreement through dumbfounding may be reduced by ensuring conditions suitable for deliberation (e.g., avoiding emotionally charged, high-pressure environments). Future research should build on this to test experimental manipulations that may help to reduce dumbfounding. Such a program of research may lead to the development of practical interventions that can reduce instances of dumbfounding in real-world situations. Examples of relevant experimental manipulations may draw on approaches such as construal level theory to encourage more abstract reasoning, e.g., through increased psychological/temporal distancing (e.g., Liberman et al., 2002). We note that attempts to test this will be similarly constrained by the methodological considerations identified above (three response options, competing intuitions).\nWhile the studies presented here were in line with our theoretical predictions, we note that our samples were either student or online participants from Westernized countries, limiting the generalizability of our findings. While emerging research suggests that dumbfounding is observed in certain non-WEIRD populations (McHugh et al. 2023), future research should test if the explanation tested here generalizes beyond Western contexts."
  },
  {
    "objectID": "publications/cognitive-load/index.html#footnotes",
    "href": "publications/cognitive-load/index.html#footnotes",
    "title": "Cognitive Load Can Reduce Reason-Giving in a Moral Dumbfounding Task",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA priori power analysis indicated that, for the primary research question (the influence of cognitive load on dumbfounded responding), in order to detect a large effect size (V = .35) with 80% power, a sample of N = 79 participants was required; in order to detect a medium effect size (V = .21) with 80% power a sample of N = 218 participants was required; in order to detect a small effect size (V = .07) with 80% power a sample of N = 1966 was required.↩︎\nThis measure avoids the potential confounding influence of qualitative differences between different response types; that is, participants indicate whether they can provide reasons for their judgments or not, and this is our measure (not whether or not they actually provide reasons, as this different type of response would not be comparable to a dumbfounded response).↩︎"
  }
]