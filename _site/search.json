[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Cillian McHugh - Brief professional bio",
    "section": "",
    "text": "Current position"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Cillian McHugh",
    "section": "",
    "text": "I am an Experimental Psychologist specialising in cognitive and social psychology. I have worked with a diverse range of research techniques, and I have applied experience in the development of new methods and materials, and in theory development. The main focus of my research to date has been on morality and moral judgement. I have applied understandings from the cognitive psychology of concepts and categories to the moral domain to propose a novel theory of Moral Judgment as Categorization (MJAC).\nMy empirical work has largely focused on the phenomenon of moral dumbfounding; when a person defends a moral judgement in the absence of reasons. The dumbfounding paradigm has been influential in moral psychology; however, there has been limited empirical work investigating it. This has meant that the main focus of my work to date has been testing, and attempting to understand the phenomenon.\nMy interests extend beyond the moral domain to include learning and knowledge acquisition, categorisation, skill/expertise, meaning, motivation, and memory. My theoretical interests include Ecological Psychology, Embodiment, and Dynamical Systems."
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Posts",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMoral Judgment as Categorization (MJAC) - Explanation and Development\n\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2020\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nReasons or Rationalisations\n\n\n\n\n\n\n\n\n\n\n\n\nJul 28, 2020\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nSearching for Moral Dumbfounding\n\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-10-02-tidy-tuesday-exploration/index.html",
    "href": "posts/2019-10-02-tidy-tuesday-exploration/index.html",
    "title": "Analysis of pizza restaurants",
    "section": "",
    "text": "When I taught the course in fall 2019, one of the weekly assignments for the students was to participate in TidyTuesday. I did the exercise as well, this is my product. You can get the R Markdown/Quarto file to re-run the analysis here.\n\nIntroduction\nIf you are not familiar with TidyTuesday, you can take a quick look at the TidyTuesday section on this page.\nThis week’s data was all about Pizza. More on the data is here.\n\n\nLoading packages\n\nlibrary('readr')\nlibrary('ggplot2')\nlibrary(\"dplyr\")\nlibrary(\"cowplot\")\nlibrary(\"plotly\")\nlibrary(\"forcats\")\nlibrary(\"geosphere\")\nlibrary(\"emoji\")\n\n\n\nData loading\nLoad date following TidyTueday instructions.\n\npizza_jared <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_jared.csv\")\npizza_barstool <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_barstool.csv\")\npizza_datafiniti <- readr::read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2019/2019-10-01/pizza_datafiniti.csv\")\n\n\n\nAnalysis Ideas\nSee the TidyTuesday website for a codebook. These are 3 datasets. Looks like the 1st dataset is ratings of pizza places through some (online?) survey/poll, the 2nd dataset again has ratings of pizza places from various sources, and the 3rd dataset seems to have fairly overlapping information to the 2nd dataset.\nNote: When I looked at the website, the codebook for the 3rd dataset seemed mislabeled. Might be fixed by now.\nPossibly interesting questions I can think of:\n\nFor a given pizza restaurant, how do the different ratings/scores agree or differ?\nAre more expensive restaurants overall rated higher?\nIs there some systematic dependence of rating on location? Do restaurants located in a certain area in general get rated higher/lower compared to others?\n\nI think those are good enough questions to figure out, let’s see how far we get.\n\n\nInitial data exploration\nStart with a quick renaming and general check.\n\n#saves typing\nd1 <- pizza_jared \nd2 <- pizza_barstool \nd3 <- pizza_datafiniti \nglimpse(d1)\n\nRows: 375\nColumns: 9\n$ polla_qid   <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5…\n$ answer      <chr> \"Excellent\", \"Good\", \"Average\", \"Poor\", \"Never Again\", \"Ex…\n$ votes       <dbl> 0, 6, 4, 1, 2, 1, 1, 3, 1, 1, 4, 2, 1, 1, 0, 1, 1, 0, 3, 0…\n$ pollq_id    <dbl> 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5…\n$ question    <chr> \"How was Pizza Mercato?\", \"How was Pizza Mercato?\", \"How w…\n$ place       <chr> \"Pizza Mercato\", \"Pizza Mercato\", \"Pizza Mercato\", \"Pizza …\n$ time        <dbl> 1344361527, 1344361527, 1344361527, 1344361527, 1344361527…\n$ total_votes <dbl> 13, 13, 13, 13, 13, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 5, 5, 5,…\n$ percent     <dbl> 0.0000, 0.4615, 0.3077, 0.0769, 0.1538, 0.1429, 0.1429, 0.…\n\nglimpse(d2)\n\nRows: 463\nColumns: 22\n$ name                                 <chr> \"Pugsley's Pizza\", \"Williamsburg …\n$ address1                             <chr> \"590 E 191st St\", \"265 Union Ave\"…\n$ city                                 <chr> \"Bronx\", \"Brooklyn\", \"New York\", …\n$ zip                                  <dbl> 10458, 11211, 10017, 10036, 10003…\n$ country                              <chr> \"US\", \"US\", \"US\", \"US\", \"US\", \"US…\n$ latitude                             <dbl> 40.85877, 40.70808, 40.75370, 40.…\n$ longitude                            <dbl> -73.88484, -73.95090, -73.97411, …\n$ price_level                          <dbl> 1, 1, 1, 2, 2, 1, 1, 1, 2, 2, 1, …\n$ provider_rating                      <dbl> 4.5, 3.0, 4.0, 4.0, 3.0, 3.5, 3.0…\n$ provider_review_count                <dbl> 121, 281, 118, 1055, 143, 28, 95,…\n$ review_stats_all_average_score       <dbl> 8.011111, 7.774074, 5.666667, 5.6…\n$ review_stats_all_count               <dbl> 27, 27, 9, 2, 1, 4, 5, 17, 14, 6,…\n$ review_stats_all_total_score         <dbl> 216.3, 209.9, 51.0, 11.2, 7.1, 16…\n$ review_stats_community_average_score <dbl> 7.992000, 7.742308, 5.762500, 0.0…\n$ review_stats_community_count         <dbl> 25, 26, 8, 0, 0, 3, 4, 16, 13, 4,…\n$ review_stats_community_total_score   <dbl> 199.8, 201.3, 46.1, 0.0, 0.0, 13.…\n$ review_stats_critic_average_score    <dbl> 8.8, 0.0, 0.0, 4.3, 0.0, 0.0, 0.0…\n$ review_stats_critic_count            <dbl> 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, …\n$ review_stats_critic_total_score      <dbl> 8.8, 0.0, 0.0, 4.3, 0.0, 0.0, 0.0…\n$ review_stats_dave_average_score      <dbl> 7.7, 8.6, 4.9, 6.9, 7.1, 3.2, 6.1…\n$ review_stats_dave_count              <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ review_stats_dave_total_score        <dbl> 7.7, 8.6, 4.9, 6.9, 7.1, 3.2, 6.1…\n\nglimpse(d3)\n\nRows: 10,000\nColumns: 10\n$ name            <chr> \"Shotgun Dans Pizza\", \"Sauce Pizza Wine\", \"Mios Pizzer…\n$ address         <chr> \"4203 E Kiehl Ave\", \"25 E Camelback Rd\", \"3703 Paxton …\n$ city            <chr> \"Sherwood\", \"Phoenix\", \"Cincinnati\", \"Madison Heights\"…\n$ country         <chr> \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", \"US\", …\n$ province        <chr> \"AR\", \"AZ\", \"OH\", \"MI\", \"MD\", \"MD\", \"CA\", \"CA\", \"FL\", …\n$ latitude        <dbl> 34.83230, 33.50927, 39.14488, 42.51667, 39.28663, 39.2…\n$ longitude       <dbl> -92.18380, -112.07304, -84.43269, -83.10663, -76.56698…\n$ categories      <chr> \"Pizza,Restaurant,American restaurants,Pizza Place,Res…\n$ price_range_min <dbl> 0, 0, 0, 25, 0, 0, 0, 0, 0, 0, 25, 25, 25, 25, 0, 0, 0…\n$ price_range_max <dbl> 25, 25, 25, 40, 25, 25, 25, 25, 25, 25, 40, 40, 40, 40…\n\n\nThe first question I have is if the pizza places in the 3 datasets are the same or at least if there is decent overlap. If not, then one can’t combine the data.\n\nd1names = unique(d1$place)\nd2names = unique(d2$name)\nd3names = unique(d3$name)\nsum(d1names %in% d2names) #check how many restaurants in d1 are also in d2. Note that this assumes exact spelling.\n\n[1] 22\n\nsum(d1names %in% d3names) #check how many restaurants in d1 are also in d2. Note that this assumes exact spelling.\n\n[1] 9\n\nsum(d2names %in% d3names)\n\n[1] 66\n\n\n22 restaurants out of 56 in dataset 1 are also in dataset 2. Only 9 overlap between dataset 1 and 3. 66 are shared between datasets 2 and 3.\nThe last dataset has no ratings, and if I look at the overlap of dataset 1 and 2, I only get a few observations. So I think for now I’ll focus on dataset 2 and see if I can address the 3 questions I posed above with just that dataset. Maybe I’ll have ideas for the other 2 datasets as I go along (would be a shame to not use them.)\n\n\nRatings agreement analysis\nOk, I’ll focus on dataset 2 now and look closer at the scores/rating. From the codebook, it’s not quite clear to me what the different scores and counts in dataset 2 actually mean, so let’s look closer to try and figure that out.\nFrom the glimpse function above, I can’t see much of a difference between average and total score. Let’s look at that. Here are a few plots comparing the different score-related variables.\n\nplot(d2$review_stats_community_total_score,d2$review_stats_community_average_score)\n\n\n\nplot(d2$review_stats_community_total_score - d2$review_stats_community_average_score* d2$review_stats_community_count)\n\n\n\nplot(d2$review_stats_critic_total_score-d2$review_stats_critic_average_score)\n\n\n\nplot(d2$review_stats_dave_total_score-d2$review_stats_dave_average_score)\n\n\n\nplot(d2$review_stats_all_total_score- (d2$review_stats_community_total_score+d2$review_stats_critic_total_score+d2$review_stats_dave_total_score))  \n\n\n\n\nOk, so based on the plots above, and a few other things I tried, it seems that average score is total score divided by number of counts, and the all score is just the sum of dave, critic and community.\nSo to address my first question, I’ll look at correlations between average scores for the 3 types of reviewers, namely dave, critic and community.\nHowever, while playing around with the data in the last section, I noticed a problem. Look at the counts for say critics and the average score.\n\ntable(d2$review_stats_critic_count)\n\n\n  0   1   5 \n401  61   1 \n\ntable(d2$review_stats_critic_average_score)\n\n\n   0    4  4.3  4.5  4.8    5  5.1  5.4  5.5  5.7  5.8  5.9 5.96    6  6.2 6.31 \n 401    1    1    1    1    3    1    1    1    1    1    1    1    1    1    1 \n 6.5  6.6  6.7 6.76  6.8  6.9    7  7.2  7.3  7.4  7.6 7.76  7.8  7.9    8  8.1 \n   3    1    1    1    2    1    5    2    2    1    1    1    2    1    4    2 \n 8.5  8.7  8.8    9  9.3  9.4  9.8   10   11 \n   3    1    1    1    1    2    1    4    1 \n\n\nA lot of restaurants did not get reviewed by critics, and the score is coded as 0. That’s a problem since if we take averages and such, it will mess up things. This should really be counted as NA. So let’s create new average scores such that any restaurant with no visits/reviews gets an NA as score.\n\nd2 <- d2 %>% mutate( comm_score = ifelse(review_stats_community_count == 0 ,NA,review_stats_community_average_score)) %>%\n             mutate( crit_score = ifelse(review_stats_critic_count == 0 ,NA,review_stats_critic_average_score)) %>%\n             mutate( dave_score = ifelse(review_stats_dave_count == 0 ,NA,review_stats_dave_average_score)) \n\nNow let’s plot the 3.\n\np1 <- d2 %>% ggplot(aes(x=comm_score, y = crit_score)) + geom_point() + geom_smooth(method = \"lm\")\np2 <- d2 %>% ggplot(aes(x=comm_score, y = dave_score)) + geom_point() + geom_smooth(method = \"lm\")\np3 <- d2 %>% ggplot(aes(x=crit_score, y = dave_score)) + geom_point() + geom_smooth(method = \"lm\")\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nLooks like there is some agreement between Dave, the critics and the community on the ratings of various pizza places, though there is a good bit of variation.\nI think it would be fun to be able to click on specific points to see for a given score which restaurant that is. For instance I’m curious which restaurant has a close to zero score from both the community and Dave (bottom left of plot B).\nI think that can be done with plotly, let’s google it.\nOk, figured it out. This re-creates the 3 scatterplots from above and when one moves over the dots, it shows restaurant name.\n\nplot_ly(d2, x = ~comm_score, y = ~crit_score, text = ~paste('Restaurant: ', name))\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\n\n\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\nplot_ly(d2, x = ~comm_score, y = ~dave_score, text = ~paste('Restaurant: ', name))\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\nplot_ly(d2, x = ~crit_score, y = ~dave_score, text = ~paste('Restaurant: ', name))\n\nNo trace type specified:\n  Based on info supplied, a 'scatter' trace seems appropriate.\n  Read more about this trace type -> https://plotly.com/r/reference/#scatter\nNo scatter mode specifed:\n  Setting the mode to markers\n  Read more about this attribute -> https://plotly.com/r/reference/#scatter-mode\n\n\n\n\n\n\nSo apparently the lousy restaurant that got a 1 from the community and almost 0 from Dave is called Amtrak. I’m wondering if that refers to pizza on Amtrak trains? Just for the heck of it and because I’m curious, let’s look at that entry.\n\nd2 %>% filter(name == \"Amtrak\") %>% knitr::kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\naddress1\ncity\nzip\ncountry\nlatitude\nlongitude\nprice_level\nprovider_rating\nprovider_review_count\nreview_stats_all_average_score\nreview_stats_all_count\nreview_stats_all_total_score\nreview_stats_community_average_score\nreview_stats_community_count\nreview_stats_community_total_score\nreview_stats_critic_average_score\nreview_stats_critic_count\nreview_stats_critic_total_score\nreview_stats_dave_average_score\nreview_stats_dave_count\nreview_stats_dave_total_score\ncomm_score\ncrit_score\ndave_score\n\n\n\n\nAmtrak\n234 W 31st St\nNew York\n10001\nUS\n40.74965\n-73.9934\n0\n3\n345\n0.54\n2\n1.08\n1\n1\n1\n0\n0\n0\n0.08\n1\n0.08\n1\nNA\n0.08\n\n\n\n\n\nI googled the address, and it seems to be indeed Amtrak. Note to self: Never order pizza on an Amtrak train.\n\n\nPrice vs ratings analysis\nNext, let’s look at possible impact of restaurant price level on rating.\n\ntable(d2$price_level)\n\n\n  0   1   2   3 \n 21 216 218   8 \n\n\nThere isn’t much spread, most pizza places are in the middle. Maybe not too surprising. Let’s look at a few plots to see if there is a pattern. First, we should recode price level as a factor.\n\nd2 <- d2 %>% mutate(price = as.factor(price_level))\n\n\np1 <- d2 %>% ggplot(aes(x=price, y=comm_score)) + geom_violin() + geom_point()\np2 <- d2 %>% ggplot(aes(x=price, y=crit_score)) + geom_violin() + geom_point()\np3 <- d2 %>% ggplot(aes(x=price, y=dave_score)) + geom_violin() + geom_point()\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n\n\n\nHard to tell if there’s a trend. Could do some stats to look in more detail, but since this exercise focuses on exploring, I won’t do that. Instead I’ll leave it at that.\n\n\nRating versus location\nOk, on to the last of the questions I started out with. Maybe there are some areas where restaurants are in general better? Or maybe an area where diners are more critical? Let’s see if there is some correlation between ratings and location.\n\ntable(d2$country)\n\n\n US \n463 \n\nsort(table(d2$city))\n\n\n        Alpharetta            Augusta             Austin         Austintown \n                 1                  1                  1                  1 \n        Blacksburg          Braintree           Brockton            Buffalo \n                 1                  1                  1                  1 \n        Charleston          Charlotte      Chestnut Hill           Chilmark \n                 1                  1                  1                  1 \n        Clearwater            Clifton         Coralville      Daytona Beach \n                 1                  1                  1                  1 \n          Dearborn        Dennis Port              DUMBO        East Meadow \n                 1                  1                  1                  1 \n             Edina          Elizabeth             Elmont         Gansevoort \n                 1                  1                  1                  1 \n              Gary       Hampton Bays          Hopkinton       Howard Beach \n                 1                  1                  1                  1 \n        Huntington          Iowa City            Jackson Jacksonville Beach \n                 1                  1                  1                  1 \n       Jersey City        Kew Gardens          Lakeville      Lawrenceville \n                 1                  1                  1                  1 \n              Lynn    Manhattan Beach       Mashantucket              Miami \n                 1                  1                  1                  1 \n    Middle Village       Mount Vernon      New Hyde Park      New York City \n                 1                  1                  1                  1 \n   North Arlington             Nutley         Oak Bluffs           Oak Lawn \n                 1                  1                  1                  1 \n     Oklahoma City             Orange         Palm Beach           Pembroke \n                 1                  1                  1                  1 \n         Princeton             Ramsey           Randolph             Revere \n                 1                  1                  1                  1 \n        Rutherford      San Francisco           Sandwich        Southampton \n                 1                  1                  1                  1 \n         Stoughton              Tampa     Vineyard Haven     West Melbourne \n                 1                  1                  1                  1 \n   West Palm Beach       West Roxbury             Woburn            Yonkers \n                 1                  1                  1                  1 \n   East Rutherford          Edgartown       Elmwood Park            Hyannis \n                 2                  2                  2                  2 \n       Miami Beach       Philadelphia         Saint Paul       Santa Monica \n                 2                  2                  2                  2 \n          Stamford              Bronx       Indianapolis          Lexington \n                 2                  3                  3                  3 \n        Morgantown        San Antonio          San Diego          Ann Arbor \n                 3                  3                  3                  4 \n        Louisville          New Haven      Staten Island         Youngstown \n                 4                  4                  4                  4 \n           Atlanta            Chicago           Columbus            Hoboken \n                 6                  6                  6                  6 \n         Nantucket   Saratoga Springs        Minneapolis          Las Vegas \n                 6                  6                  8                 11 \n            Boston           Brooklyn           New York \n                13                 20                251 \n\n\nOk so all restaurants are in the US, and most are in New York. We could look at NY versus “rest of the cities”. Though isn’t Brooklyn (the 2nd largest entry) basically a part of New York? I’m not enough of an expert on all things NY to be sure (for any real analysis, you need to know a good bit about the subject matter, or work closely with a subject matter expert. If not, more likely than not something dumb will happen).\nFor now, I assume that it’s different enough, and make 2 categories, NY and “other” and see if there are differences. Let’s try.\n\np1 <- d2 %>% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %>%\n              ggplot(aes(x=newcity, y = comm_score)) + geom_violin() + geom_point()\np2 <- d2 %>% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %>%\n              ggplot(aes(x=newcity, y = crit_score)) + geom_violin() + geom_point()\np3 <- d2 %>% dplyr::mutate(newcity = forcats::fct_lump(city, n = 1)) %>%\n              ggplot(aes(x=newcity, y = dave_score)) + geom_violin() + geom_point()\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n\n\n\nLooks like the community in NY gives lower scores compared to other locations, less noticeable difference for critics and Dave.\nOk, the next analysis might not make much sense, but why not check if there is a North-South or East-West trend related to ratings. Maybe restaurants are better in one of those directions? Or people in the South are more polite and give better scores? 😁. I’m mostly doing this because longitude and latitude are continuous variables, so I can make a few more scatterplots. I don’t have any real goal for this otherwise.\n\np1 <- d2 %>%  ggplot(aes(x=longitude, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 <- d2 %>%  ggplot(aes(x=longitude, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 <- d2 %>%  ggplot(aes(x=longitude, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nSo as we go from the west (-120) to the east (-70), there is a trend in restaurants getting higher scores, by all 3 groups. I guess as we are moving closer to Italy, the pizza quality goes up? 😃.\nNext, let’s look at latitude.\n\np1 <- d2 %>%  ggplot(aes(x=latitude, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 <- d2 %>%  ggplot(aes(x=latitude, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 <- d2 %>%  ggplot(aes(x=latitude, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12)\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nSo doesn’t seem as much of a trend going from South (25) to North (45). That finding of course fully confirms our “closer to Italy” theory!\nOk, I was going to leave it at that with location, but since I’m already going down a crazy rabbit hole regarding Italy, let’s do it for real: We’ll take both longitude and latitude of each restaurant and use it compute the distance of each location to Naples, the home of Pizza. And then we’ll plot that and see.\nSince I have no idea how to do that, I need Google. Fortunately, the first hit worked, found this one: https://stackoverflow.com/questions/32363998/function-to-calculate-geospatial-distance-between-two-points-lat-long-using-r\nLet’s try.\n\ncoord_naples=cbind(rep(14.2,nrow(d2)),rep(40.8,nrow(d2)))  #location of naples\ncoord_restaurants = cbind(d2$longitude,d2$latitude)\ndistvec = rep(0,nrow(d2))\nfor (n in 1:nrow(d2))\n{\n  distvec[n] = distm( coord_restaurants[n,], coord_naples[n,], fun = distGeo)\n}\nd2$distvec = distvec / 1609 #convert to miles since we are in the US :)\n\nIt’s not tidyverse style, which I tried first but couldn’t get it to work. The trusty old for-loop seems to always work for me. I checked the numbers in distvec, they look reasonable.\nOk, let’s redo the plots above, now with distance to Naples.\n\np1 <- d2 %>%  ggplot(aes(x=distvec, y = comm_score)) + geom_point() + geom_smooth(method = 'lm')\np2 <- d2 %>%  ggplot(aes(x=distvec, y = crit_score)) + geom_point() + geom_smooth(method = 'lm')\np3 <- d2 %>%  ggplot(aes(x=distvec, y = dave_score)) + geom_point() + geom_smooth(method = 'lm')\ncowplot::plot_grid(p1, p2, p3, labels = c('A', 'B','C'), label_size = 12, nrow = 3)\n\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nHm ok, no smoking gun. Looks like there is a bit of a trend that the further away you are from Naples, the lower the score. But really not much.\n\n\nHyping our result\nBut since this distance-from-Naples makes such a good story, let’s see if I can hype it.\nFirst, to increase potential statistical strength, I’ll combine all 3 scores into an overall mean, i.e. similar ot the all variable in the original. I don’t trust that one since I don’t know if they averaged over 0 instead of properly treating it as NA. Of course I could check, but I’m just re-creating it here.\n\nd2$all_score = rowMeans(cbind(d2$dave_score,d2$crit_score,d2$comm_score),na.rm=TRUE)\n\nOk, let’s check if correlation between this new score and distance is significant!\n\n#compute a linear fit and p-value (it's significant!)\nfit=lm(d2$all_score ~ d2$distvec, data = d2)\nsummary(fit)\n\n\nCall:\nlm(formula = d2$all_score ~ d2$distvec, data = d2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-6.7854 -0.5866  0.3027  0.9612  2.3686 \n\nCoefficients:\n              Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  8.9895014  0.7008802  12.826  < 2e-16 ***\nd2$distvec  -0.0004772  0.0001525  -3.129  0.00187 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.478 on 459 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.02089,   Adjusted R-squared:  0.01875 \nF-statistic: 9.791 on 1 and 459 DF,  p-value: 0.001865\n\npval=anova(fit)$`Pr(>F)`[1]\nprint(pval)\n\n[1] 0.001865357\n\n\nIt is signficant, p<0.05! We hit pay dirt! Let’s make a great looking figure and go tell the press!\n\n#make final plot\np1 <- d2 %>%  ggplot(aes(x=distvec, y = all_score)) + geom_point(shape = 21, colour = \"black\", fill = \"red\",  size = 2 ) + geom_smooth(method = 'lm', se = TRUE, color = \"darkgreen\", size = 2) + xlab('Distance from Naples (miles)') + ylab('Pizza Quality (score)') + ylim(c(2.5,max(d2$all_score))) + theme_bw() +theme(axis.text=element_text(size=12), axis.title=element_text(size=14,face=\"bold\")) + annotate(\"text\", x=6000, y=9, label= paste(\"p =\",round(pval,4)),size = 12) \nggsave('pizzadistance.png')\nknitr::include_graphics(\"pizzadistance.png\")\n\n\n\n\n\n\nThe “press release”\nA novel study of pizza restaurants in the US found a clear, statistically significant correlation between the distance of the restaurant to Naples and the quality of the pizza as determined by the community and expert restaurant critics. The study authors attribute the finding to the ability of restaurants that are closer to Naples to more easily get genuine fresh and high quality ingredients, such as the famous San Marzano tomatoes.\n\n\n\n\n\n\n\n\n\n\n\nSummary\nThat was a fun exploration. It was the first time I played with the tidyverse data. I had no idea which direction it was going to go, and ideas just came as I was doing it. I’m sure there is interesting stuff in datasets 1 and 3 as well, but I already spent several hours on this and will therefore call it quits now.\nWhile the exercise was supposed to focus on cleaning/wrangling and visualizing, I couldn’t resist going all the way at the end and producing a statistically significant and somewhat plausible sounding finding. If this were a “real” study/analysis, such a nice result would be happily accepted by most analysts/authors, hyped by a university press release and - if the result is somewhat interesting/cute, picked up by various media outlets.\nI had no idea at the beginning what I was going to analyze, I did that longitude/latitude analysis on a whim, and if I hadn’t found this correlation and had that crazy distance to Italy idea, nothing would have happened. But now that I have a significant result and a good story to go with, I can publish! It’s not really much sillier than for instance the Chocolate and Nobel Laureates paper paper.\nWhat I illustrated here (without having had any plan to do so), is a big, general problem in secondary data analysis. It’s perfectly ok to do secondary analyses, and computing significance is also (kinda) ok, but selling exploratory (fishing) results as inferential/causal/confirmatory is wrong - and incredibly widespread. If you want to sharpen your critical thinking skills related to all those supposed significant and real findings in science we see a lot, a great (though at times sobering) read is Andrew Gelman’s blog where he regularly picks apart studies/results like the one I did here or the chocolate and Nobel laureates one. And now I’ll go eat some chocolate so I can increase my chances for a Nobel prize.\n\n\n\n\nCitationBibTeX citation:@online{handel2019,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Analysis of Pizza Restaurants},\n  date = {2019-10-12},\n  url = {https://www.andreashandel.com/posts/2019-10-02-tidy-tuesday-exploration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2019. “Analysis of Pizza Restaurants.”\nOctober 12, 2019. https://www.andreashandel.com/posts/2019-10-02-tidy-tuesday-exploration."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html",
    "href": "posts/2020-01-02-blogdown-website-1/index.html",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "",
    "text": "Update: Hugo and the Academic (now Wowchemy) kept changing rapidly and became, in my opinion, rather complex and confusing. Once Quarto arrived, I switched this website over (see my post on that here), and I think it’s much easier to use. I therefore don’t really recommend the setup described here anymore, though it might still be useful for some.\nThe following are step-by-step instructions for creating your own website using blogdown, Hugo and Netlify.\nIn part 2, you will learn how to add GitHub to your workflow to make things even more automated and efficient."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#get-a-netlify-account",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#get-a-netlify-account",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Get a Netlify account",
    "text": "Get a Netlify account\nGo to the Netlify website and sign up for an account. Follow the sign-up steps to set up your account."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#install-r-and-rstudio",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#install-r-and-rstudio",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nIf you don’t already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it’s a recent version. If you have an older version of RStudio, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach)."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#playing-with-widgets",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#playing-with-widgets",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Playing with widgets",
    "text": "Playing with widgets\nAll (or at least most) content goes into the content folder and its subfolders. Content is generally written in (R)Markdown. For this tutorial, you don’t need to know much Rmarkdown, but at some point you will have to learn it. Fortunately, (R)Markdown is very easy to learn. See e.g. the RMarkdown section on this page, check out this nice interactive tutorial or this cheatsheet.\nThe Wowchemy/Academic theme, and many other modern websites, use a layout that employs widgets, which are components of a website that are individually formatted and styled. On the demo site you just created, you see many different sections, each is a widget. Which widgets you want is controlled by files in the /content/home/ folder. Go into that folder (from within RStudio) and open the demo.md file. You will see a bunch of text. Some commands are between +++ signs, this is called the TOML (or YAML if it’s 3 ---) header. These are instructions for the layout. The text below is what is actually shown on the site.\nAs you stare at the content of the file, you might recognize that it corresponds to the 2nd block of the demo website with the dark bubble content. Let’s say you don’t want this particular widget on your home page. The easiest way is to set active = false. (You can also delete the whole file if you are sure you don’t want it). Do that. You should notice 2 things. In the bottom-left RStudio window (the R console) you should see a bit of code showing that the website was re-built and updated in real time. If you re-load the page in your browser, the widget and its content should be gone. You can try turning off other sections of the main page using this approach."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#making-things-personal",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#making-things-personal",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Making things personal",
    "text": "Making things personal\nNow let’s open the about.md file. You will notice that it doesn’t really contain any content. Instead, it pulls the content from another location, namely content in the authors folder. Go into /content/authors/admin/ and open the _index.md file. There you see the content that is displayed on the main page. Modify it with your personal information. Once you save your changes, you should see the website automatically being rebuilt. If you have, add a picture of yourself and replace the current avatar.jpg file. (Your picture needs to have that name). Also, while not required, you might want to rename the folder from admin to your name. Make sure this corresponds to the name you list in the _index.md file."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#cleaning-up-for-now",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#cleaning-up-for-now",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "Cleaning up for now",
    "text": "Cleaning up for now\nLet’s turn off all other parts of the main site apart from the about widget. The easiest way is to remove all files apart from the index.md and about.md files. You probably don’t want to completely delete them (since you might want to use them later), thus I recommend you move them to some other folder on your computer. For instance you can make a folder called myfiles as a subfolder of your website folder and move the files into that folder.\nIf all of this worked, there should be a main page containing only a brief description of yourself."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#a-word-on-error-messages.",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#a-word-on-error-messages.",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "A word on error messages.",
    "text": "A word on error messages.\nIf you make some changes that break the site, you will see an error message in the R console and the site won’t re-compile until you fix the problem. You often have to be careful to write things exactly as specified, and often with the right indentation, etc. Some fiddling is at times required. If you are stuck and think you broke it too badly, you can either look in the Wowchemy theme documentation or go into the themes/starter-academic/exampleSite folder and find the corresponding file you are editing there and see how it needs to look."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#config.yaml",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#config.yaml",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "config.yaml",
    "text": "config.yaml\nI mentioned above that TOML/YAML is a language/structure used by Hugo to control all kinds of things. Most files have some TOML/YAML part, a few files are nothing but TOML and control a lot of settings. Let’s look at the most important files. The first one is config.yaml (sometimes also called config.toml) located in the main website folder. Find and open it. You will see that it lists as title Academic. Change that to e.g. Website of YOURNAME. You will see this change show up on the main site. You can try what happens if you write something in the copyright section. The rest of this file doesn’t need further editing for now."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#menus.toml",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#menus.toml",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "menus.toml",
    "text": "menus.toml\nLet’s go into the config/_default/ folder and open the menus.toml file. You’ll see that those correspond to the menu buttons on the main page. Most of them don’t work since we removed the widgets. For now, let’s go ahead and disable (by placing # symbols in front) all entries apart from the Posts block of text."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#params.toml",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#params.toml",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "params.toml",
    "text": "params.toml\nOpen params.toml. This file lets you specify and control a lot of things. Try setting a different theme. Then read through the other parts. We won’t change them for now, but you might want to come back to them."
  },
  {
    "objectID": "posts/2020-01-02-blogdown-website-1/index.html#more-edits",
    "href": "posts/2020-01-02-blogdown-website-1/index.html#more-edits",
    "title": "Creating a website in less than 1 hour - part 1",
    "section": "More edits",
    "text": "More edits\nCongratulations, you have built a website and wrote a blog post! Of course there is a lot you can do next. Write more posts, look at all the different elements/widgets you can turn on and modify, etc. As mentioned, the Academic theme has a lot of features. If you like what you see, continue exploring. If you think you want something simpler, check out other Hugo themes until you find one you like, then customize it. A lot of things are very similar across all Hugo themes (e.g. the TOML/YAML bits and the folder structure), but some details differ, so it’s good to pick a theme before you really start customizing it.\nBut for now, we’ll leave it at this. There is one more crucial step missing though."
  },
  {
    "objectID": "posts/2020-01-04-blogdown-website-2/index.html",
    "href": "posts/2020-01-04-blogdown-website-2/index.html",
    "title": "Creating a website in less than 1 hour - part 2",
    "section": "",
    "text": "If you haven’t seen part 1 yet, I suggest you read through that first. In that part, I covered how to create a website using blogdown, Hugo, and Netlify.\n\nRequired skills\nI assume that you have general computer literacy, but no experience with any of the tools that will be used. Also, no coding, web-development or related experience is expected.\nI assume your website is at a stage as described at the end of part 1.\n\n\nQuick tool overview\nThe only new tool for this part is GitHub. GitHub is a very powerful and common way of developing projects like code, research projects or a website. Github is a great tool to be familiar with in general, and most importantly, it very nicely integrates with the other tools to make things seamless and automated. As you learned in part 1, it is not strictly needed, but it makes updating automatic and is such an overall useful tool to be exposed to that I’m including it in the setup.\n\n\nGet a GitHub account\nIf you do not already have a GitHub account, create one. Note that GitHub is widely used professionally, and you might want to allow other people to see your GitHub presence. I, therefore, recommend using a future-proof, professional user name.\nNote that Git and GitHub (which are technically different, here I’m using GitHub to refer to both) can be initially confusing, mainly because they use a lot of specialized terminology. I will try and walk you through all steps for getting a website up and running in detail, but you might have to look up a few things on the way. If you are completely new to GitHub, I recommend you take a quick look at this page (and links provided there) so you can get some idea of what it’s all about.\n\n\nGet Gitkraken\nDownload and install Gitkraken, link it with your GitHub account. Gitkraken is a graphical Git/GitHub client that makes a lot of tasks related to GitHub easier. It’s not strictly needed, and if you already have your own way of using Git/GitHub (e.g. with another client or the command line) you can stick with that. There is also an option to use Git/GitHub through RStudio, which is fine for most things, but overall Gitkraken is more powerful. So if you plan to use GitHub more in the future, I recommend using it. In the following, I assume you are using Gitkraken. If you interface with GitHub some other way, you will have to adjust those specific instructions accordingly.\n\n\nRecommended, but optional: Upgrade GitHub (& Gitkraken)\nOn GitHub, by default, all repositories are public (a repository is the collection of files and folders that make up a specific GitHub project, such as your website.) If you have public repositories, you need to be careful about files that shouldn’t be shared publicly (e.g. because of copyright restrictions or because it might violate data privacy). Normally, if you want private repositories, you have to pay. As student or educator, you can get private repositories for free.\nIf you are a student, I strongly recommend you get the GitHub student developer pack. This gives you access to private repositories. You also get 1 year of free Gitkraken Pro access. While the free version of Gitkraken works well, you can’t access private repositories with it. Often, being able to use private repositories is useful.\nEducators can also get a free GitHub Pro account here. As far as I’m aware, there is no free Gitkraken Pro for educators, but it’s fairly cheap. So if you want to use a private repository for your website (I don’t know why you would), you need to pay for Gitkraken Pro or use a different way to manage your GitHub repositories.\n\n\nCreate a GitHub repository\n\nLog into GitHub, click on ‘Repositories’ and create a new repository (green button). Choose a repository (repo) name that tells you what’s in the repo (e.g. YOURNAME-website). You can give it the same name as you named your main website directory/project in part 1, but that’s not required. Check the box Initialize this repository with a README. Set the .gitignore option to R, you can leave the license at none. Click create repository.\nClone the repository from GitHub to your local computer (using Gitkraken or whatever software/method you decided to use). Place the local folder in a location on your computer where it is not synced with some other software (e.g., Dropbox, OneDrive).\n\n\n\nMove your website folder to the GitHub repo\n\nFind the main folder of your website and move all of it into your newly created GitHub repository. To make sure everything is up-to-date, close RStudio before doing so.\nOpen the repository you just created in Gitkraken. In the top right of Gitkraken, there should be a notification about changed files. Click View changes, then Stage. Write a commit message, commit the changes. You should see the master with the computer symbol above the one with some random logo. That means your local repository is ahead of the one on github.com. To get them in sync, you click the push button. If things work, the two symbols should now be in the same line.\nIf your code cannot sync you will likely receive an option from GitKraken to perform a force push. A force push will overwrite the remote repo with the local repo forcibly. This means that the remote will be updated with any changes that exist on your local computer however, if any additional changes have been made to the code since you began editing (i.e. someone else has made a commit to the repo while you were working on it) they will be destroyed by the force push since they are not on your local repo. For this project, you are the only person working on your introduction so it is unlikely you will encounter any issues with force push, but it is good to be aware of this action.\nGo back to GitHub.com and to your repository. You should see all your files in there.\n\n\n\nConnecting GitHub to Netlify\n\nThe last step is to set up Netlify so it can automatically monitor your GitHub repository and process any changes into an updated website. To do so, log into your Netlify account. Select your website. Under Site Settings find the Build and deploy menu. Under Continuous deployment click on Link site to Git. Choose GitHub and the follow the steps to link your webpage repository. Once finished, Netlify will monitor that repository and automatically pull any updates and rebuild your website.\n\n\n\nThe new workflow\nThe workflow for your website using GitHub now works as follows:\n\nOpen your website in Rstudio by clicking on the .Rproj file. Load blogdown with library(blogdown). Make any edits you want. While you make your edits, you can run serve_site() to see your updates.\n\nOnce you are done with your updates, restart R (to shut down serve_site()) and run build_site(). While Netlify will automatically run Hugo on your files, it won’t do any processing of Rmarkdown files, therefore you need to run the blogdown build command.\nUse Gitkraken to push/pull and therefore sync changes between your local computer (or multiple computers if you work on more than one machine) and Github.com.\n\nNetlify will monitor your Github repository and when it sees changes, automatically rebuild your website. Note that this means that if you start working on a document and don’t finish it, and then push to GitHub, your half-finished document shows up. To avoid that, you can set draft: true in your TOML/YAML heading for the document in progress.\n\n\nMore Information\nFor the non-GitHub aspects of this, see the resources mentioned at the end of part 1. For some more GitHub related information, see e.g. here.\n\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Creating a Website in Less Than 1 Hour - Part 2},\n  date = {2020-01-20},\n  url = {https://www.andreashandel.com/posts/2020-01-04-blogdown-website-2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Creating a Website in Less Than 1 Hour -\nPart 2.” January 20, 2020. https://www.andreashandel.com/posts/2020-01-04-blogdown-website-2."
  },
  {
    "objectID": "posts/2020-01-08-resources-website/index.html",
    "href": "posts/2020-01-08-resources-website/index.html",
    "title": "Research and teaching resources website",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Research and Teaching Resources Website},\n  date = {2020-01-08},\n  url = {https://www.andreashandel.com/posts/2020-01-08-resources-website},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Research and Teaching Resources\nWebsite.” January 8, 2020. https://www.andreashandel.com/posts/2020-01-08-resources-website."
  },
  {
    "objectID": "posts/2020-02-01-publications-analysis-1/index.html",
    "href": "posts/2020-02-01-publications-analysis-1/index.html",
    "title": "Using R to analyze publications - part 1",
    "section": "",
    "text": "Overview\nI needed some information on all my publications for “bean counting” purposes related to preparing my promotion materials. In the past, I also needed similar information for NSF grant applications.\nInstead of doing things by hand, there are nicer/faster ways using R. The following shows a few things one can do with the scholar package. I describe an alternative approach using the bibliometrix package in part 2 of this post.\n\n\nNotes\n\nAs of this writing, the scholar R package seems semi-dormant and not under active development. If Google changes their API for Scholar and the package isn’t updated, the below code might stop working.\nA problem I keep encountering with Google Scholar is that it starts blocking requests, even after what I consider are not that many attempts to retrieve data. I notice that when I try to pull references from Google Scholar using JabRef and also with the code below. If that happens to you, try a different computer, or clear cookies. This is a well known problem and if you search online, you find others complaining about it. I haven’t found a great solution yet, other than not using the Google Scholar data. I describe such an approach in part 2 of this post. However, some analyses are only able with Google Scholar information.\nTo minimize chances of getting locked out by Google, I wrote the code below such that it only sends a request if there isn’t a local file already containing that data. To refresh data, delete the local files.\n\n\n\nRequired packages\n\nlibrary(scholar)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(knitr)\nlibrary(ggplot2)\n\n\n\nGet all citations for an individual\nFirst, I’m using Google Scholar to get all citations for a specific author (in this case, myself).\n\n#Define the person to analyze\nscholar_id=\"bruHK0YAAAAJ\" \n#either load existing file of publications\n#or get a new one from Google Scholar\n#delete the file to force an update\nif (file.exists('citations.Rds'))\n{\n  cites &lt;- readRDS('citations.Rds')\n} else {\n  #get citations\n  cites &lt;- scholar::get_citation_history(scholar_id) \n  saveRDS(cites,'citations.Rds')\n}\n\n\n\nCompare citations for different time periods\nFor my purpose, I want to compare citations between 2 time periods (my Assistant Professor time and my Associate Professor time). I’m splitting them into 2. I’m doing this analysis at the beginning of 2020 and want only full years. The code snippets below give me what I need, two time periods 2009-2014 and 2014-2019.\n\nperiod_1_start = 2009\nperiod_2_start = 2015\ncites_1 &lt;- cites %&gt;% dplyr::filter((year&gt;=period_1_start & year&lt;period_2_start ))\n#remove last year since it's not a full year\ncites_2 &lt;- cites %&gt;% dplyr::filter((year&gt;=period_2_start & year&lt;2020 )) \n\nFitting a linear model to both time segments to look at increase in citations over both periods.\n\nfit1=lm(cites ~ year, data = cites_1)\nfit2=lm(cites ~ year, data = cites_2)\ninc1 = fit1$coefficients[\"year\"]\ninc2 = fit2$coefficients[\"year\"] \nprint(sprintf('Annual increase for periods 1 and 2 are %f, %f',inc1,inc2))\n\n[1] \"Annual increase for periods 1 and 2 are 22.257143, 43.100000\"\n\n\nMaking a figure to show citation count increases\n\n# combine data above into single data frame\n#add a variable to indicate period 1 and period 2\ncites_1$group = \"1\"\ncites_2$group = \"2\"\ncites_df = rbind(cites_1,cites_2)\nxlabel = cites_df$year[seq(1,nrow(cites_df),by=2)]\n#make the plot and show linear fit lines\np1 &lt;- ggplot(data = cites_df, aes(year, cites, colour=group, shape=group)) + \n      geom_point(size = I(4)) + \n      geom_smooth(method=\"lm\",aes(group = group), se = F, size=1.5) + \n      scale_x_continuous(name = \"Year\", breaks = xlabel, labels = xlabel) +     scale_y_continuous(\"Citations according to Google Scholar\") +\n      theme_bw(base_size=14) + theme(legend.position=\"none\") + \n      geom_text(aes(NULL,NULL),x=2010.8,y=150,label=\"Average annual \\n increase 22%\",color=\"black\",size=5.5) +\n      geom_text(aes(NULL,NULL),x=2017,y=150,label=\"Average annual \\n increase 43%\",color=\"black\",size=5.5) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n#open a new graphics window\n#note that this is Windows specific. Use quartz() for MacOS\n# ww=5; wh=5; \n# windows(width=ww, height=wh)                  \n# print(p1)\n# dev.print(device=png,width=ww,height=wh,units=\"in\",res=600,file=\"citations.png\")\n\n\n\nGetting list of publications\nAbove I got citations, but not publications. This function retrieves all publications for a specific author and returns it as a data frame.\n\n#get all pubs for an author (or multiple)\nif (file.exists('publications.Rds'))\n{\n  publications &lt;- readRDS('publications.Rds')\n} else {\n  #get citations\n  publications &lt;- scholar::get_publications(scholar_id) \n  saveRDS(publications,'publications.Rds')\n}\n\n\n\nQuick peek at publications\n\nglimpse(publications)\n\nRows: 90\nColumns: 8\n$ title   &lt;fct&gt; \"Severe outcomes are associated with genogroup 2 genotype 4 no…\n$ author  &lt;fct&gt; \"R Desai, CD Hembree, A Handel, JE Matthews, BW Dickey, S McDo…\n$ journal &lt;fct&gt; \"Clinical infectious diseases\", \"BMC public health\", \"Journal …\n$ number  &lt;fct&gt; \"55 (2), 189-193\", \"11 (S1), S7\", \"7 (42), 35-47\", \"3 (12)\", \"…\n$ cites   &lt;dbl&gt; 163, 158, 129, 124, 123, 115, 105, 89, 71, 71, 55, 53, 52, 49,…\n$ year    &lt;dbl&gt; 2012, 2011, 2010, 2007, 2006, 2012, 2006, 2017, 2016, 2008, 20…\n$ cid     &lt;fct&gt; 1979732925283755485, 10982184786304722425, 1038596204985444772…\n$ pubid   &lt;fct&gt; 5nxA0vEk-isC, _FxGoFyzp5QC, 9yKSN-GCB0IC, d1gkVwhDpl0C, u5HHmV…\n\n\nThis shows the variables obtained in the data frame. One thing I notice is that this contains more entries than I have peer-reviewed publications. Since most people’s Google Scholar profile (including my own) list items beyond peer-reviewed journal articles, one likely needs to do some manual cleaning before analysis. That is not ideal. I’ll do/show a few more possible analyses, but decided to do the analyses below using the approach in part 2.\n\n\nMaking a table of journals and impact factors\nThis used to work, but as of 2022-09-10 when I tried to re-run, it failed. Seems like get_impactfactor() doesn’t exist anymore. Maybe they got in trouble with the owners of ImpactFactor? Leaving it here, but code chunk below doesn’t run.\nThe scholar package has a function that allows one to get impact factors for journals. This data doesn’t actually come from Google Scholar, instead the package comes with an internal spreadsheet/table with impact factors. Looking a bit into the scholar package indicates that the data was taken from some spreadsheet posted on ResearchGate (probably not fully legal). Either way, let’s give it a try.\n\n#here I only want publications since 2015\npub_reduced &lt;- publications %&gt;% dplyr::filter(year&gt;2014)\n# my guess is they got in trouble with the owners of ImpactFactor?\nifdata &lt;- scholar::get_impactfactor(pub_reduced$journal) \n#Google SCholar collects all kinds of 'publications'\n#including items other than standard peer-reviewed papers\n#this sorts and removes some non-journal entries  \niftable &lt;- ifdata %&gt;% dplyr::arrange(desc(ImpactFactor) ) %&gt;% tidyr::drop_na()\nknitr::kable(iftable)\n\nOK so this doesn’t quite work. I know for instance that I didn’t publish anything in Cancer Journal for Clinicians and the 2 Rheumatology entries are workshop presentations. Oddly, when I look at publications$journal there is no Cancer Journal listed. Somehow this is a bug created by the get_impactfactor() function. I could fix that by hand. The bigger problem is what to do with all those publications that are not peer-reviewed papers. I could remove them from my Google scholar profile. But I kind of want to keep them there since some of them link to useful stuff. I could alternatively manually clean things at this step. This somewhat defeats the purpose of automation.\n\n\nGetting list of co-authors\nAnother useful piece of information to have, e.g. for NSF grants, is a table with all co-authors. Unfortunately, get_publications() only pulls from the main Google Scholar page, which cuts off the author list. To get all authors, one needs to run through each paper using get_complete_authors(). The problem is that Google cuts off access if one sends too many queries. If you get error messages, it might be that Google blocked you. See the Notes section.\n\nallauthors = list()\nif (file.exists('allauthors.Rds'))\n{\n  allauthors &lt;- readRDS('allauthors.Rds')\n} else {\n  for (n in 1:nrow(publications)) \n  {\n    allauthors[[n]] = get_complete_authors(id = scholar_id, pubid = publications[n,]$pubid)\n  }\n  saveRDS(allauthors,'allauthors.Rds')\n}\n\nTheoretically, if the above code runs without Google blocking things, I should end up with a list of all co-authors which I could then turn into a table. The problem is still that it pulls all entries on my Google Scholar profile, and not just peer-reviewed papers. With a bit of cleaning I could get what I need. But overall I don’t like this approach too much.\n\n\nDiscussion\nWhile the scholar package has some nice features, it has 2 major problems:\n\nGoogle blocking the script if it decides too many requests are made (that can happen quickly).\nSince most people’s Google Scholar profile (including my own) list items beyond peer-reviewed journal articles, one likely needs to do some manual cleaning before analysis.\n\nI do keep all my published, peer-reviewed papers in a BibTeX bibliography file in my reference manager (I’m using Zotero and/or Jabref). I know that file is clean and only contains peer-reviewed papers. Unfortunately, the scholar package can’t read in such data. In part 2 of this post series, I’ll use a different R package to produce the journal and author tables I tried making above.\nThe one feature only available through Google Scholar is the citation record and the analysis I did at the beginning if this post.\n\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Handel, Andreas},\n  title = {Using {R} to Analyze Publications - Part 1},\n  date = {2020-02-01},\n  url = {https://www.andreashandel.com/posts/2020-02-01-publications-analysis-1},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHandel, Andreas. 2020. “Using R to Analyze Publications - Part\n1.” February 1, 2020. https://www.andreashandel.com/posts/2020-02-01-publications-analysis-1."
  },
  {
    "objectID": "posts/2020-02-02-publications-analysis-2/index.html",
    "href": "posts/2020-02-02-publications-analysis-2/index.html",
    "title": "Using R to analyze publications - part 2",
    "section": "",
    "text": "Required packages\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(bibliometrix)\n\n\n\nLoading data\nOld: I keep all references to my published papers in a BibTeX file, managed through Zotero/Jabref. I know this file is clean and correct. I’m loading it here for processing. If you don’t have such a file, make one using your favorite reference manager. Or create it through a saved search on a bibliographic database, as explained on the bibliometrix website.\nNew: In the current version of bibliometrix, reading in my bibtex file failed. A fairly good alternative is to go to your NIH “My Bibliography” (which anyone with NIH funding needs to have anyway) and export it in MEDLINE format. Then read in the file with the code below. As of the time of writing this, it requires the Github version of bibliometrix.\n\n#read bib file, turn file of references into data frame\npubs <- bibliometrix::convert2df(\"medline.txt\", dbsource=\"pubmed\",format=\"pubmed\") \n\n\nConverting your pubmed collection into a bibliographic dataframe\n\nDone!\n\n\nGenerating affiliation field tag AU_UN from C1:  Done!\n\n\nEach row of the data frame created by the convert2df function is a publication, the columns contain information for each publication. For a list of what each column variable codes for, see the bibliometrix documentation.\n\n\nAnalyzing 2 time periods\nFor my purpose, I want to analyze 2 different time periods and compare them. Therefore, I split the data frame containing publications, then run the analysis on each.\n\n#get all pubs for an author (or multiple)\nperiod_1_start = 2009\nperiod_2_start = 2015\n#here I want to separately look at publications in the 2 time periods I defined above\npubs_old <- data.frame(pubs) %>% dplyr::filter((PY>=period_1_start & PY<period_2_start ))\npubs_new <- data.frame(pubs) %>% dplyr::filter(PY>=period_2_start)\nres_old <- bibliometrix::biblioAnalysis(pubs_old, sep = \";\") #perform analysis\nres_new <- bibliometrix::biblioAnalysis(pubs_new, sep = \";\") #perform analysis\n\n\n\nGeneral information\nThe summary functions provide a lot of information in a fairly readable format. I apply them here to both time periods so I can compare.\nTime period 1\n\nsummary(res_old, k = 10)\n\n\n\nMAIN INFORMATION ABOUT DATA\n\n Timespan                              2009 : 2014 \n Sources (Journals, Books, etc)        12 \n Documents                             19 \n Annual Growth Rate %                  3.71 \n Document Average Age                  10.3 \n Average citations per doc             0 \n Average citations per year per doc    0 \n References                            1 \n \nDOCUMENT TYPES                     \n clinical trial;journal article;research support, non-u.s. gov't                                               1 \n comparative study;journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                                               1 \n journal article                                               2 \n journal article;research support, n.i.h., extramural                                               5 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                                               3 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.                               1 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;review;systematic review      1 \n journal article;research support, non-u.s. gov't                                               3 \n journal article;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;research support, u.s. gov't, p.h.s.                               1 \n journal article;review                                               1 \n \nDOCUMENT CONTENTS\n Keywords Plus (ID)                    148 \n Author's Keywords (DE)                148 \n \nAUTHORS\n Authors                               45 \n Author Appearances                    80 \n Authors of single-authored docs       0 \n \nAUTHORS COLLABORATION\n Single-authored docs                  0 \n Documents per Author                  0.422 \n Co-Authors per Doc                    4.21 \n International co-authorships %        0 \n \n\nAnnual Scientific Production\n\n Year    Articles\n    2009        5\n    2010        2\n    2011        1\n    2012        3\n    2013        2\n    2014        6\n\nAnnual Percentage Growth Rate 3.713729 \n\n\nMost Productive Authors\n\n   Authors        Articles Authors        Articles Fractionalized\n1   HANDEL A            19  HANDEL A                         5.55\n2   ANTIA R              6  ANTIA R                          1.78\n3   DOHERTY PC           3  LONGINI IM JR                    1.00\n4   LA GRUTA NL          3  DOHERTY PC                       0.56\n5   LONGINI IM JR        3  LA GRUTA NL                      0.56\n6   THOMAS PG            3  THOMAS PG                        0.56\n7   PILYUGIN SS          2  BEAUCHEMIN CA                    0.50\n8   ROHANI P             2  LI Y                             0.50\n9   STALLKNECHT D        2  ROHANI P                         0.50\n10  TURNER SJ            2  ROZEN DE                         0.50\n\n\nTop manuscripts per citations\n\n                              Paper                                   DOI TC TCperYear NTC\n1  ZHENG N, 2014, PLOS ONE                   10.1371/JOURNAL.PONE.0105721  0         0 NaN\n2  HANDEL A, 2014, PROC BIOL SCI             10.1098/RSPB.2013.3051        0         0 NaN\n3  NGUYEN TH, 2014, J IMMUNOL                10.4049/JIMMUNOL.1303147      0         0 NaN\n4  LI Y, 2014, J THEOR BIOL                  10.1016/J.JTBI.2014.01.008    0         0 NaN\n5  HANDEL A, 2014, J R SOC INTERFACE         10.1098/RSIF.2013.1083        0         0 NaN\n6  CUKALAC T, 2014, PROC NATL ACAD SCI U S A 10.1073/PNAS.1323736111       0         0 NaN\n7  HANDEL A, 2013, PLOS COMPUT BIOL          10.1371/JOURNAL.PCBI.1002989  0         0 NaN\n8  THOMAS PG, 2013, PROC NATL ACAD SCI U S A 10.1073/PNAS.1222149110       0         0 NaN\n9  JACKWOOD MW, 2012, INFECT GENET EVOL      10.1016/J.MEEGID.2012.05.003  0         0 NaN\n10 DESAI R, 2012, CLIN INFECT DIS            10.1093/CID/CIS372            0         0 NaN\n\n\nCorresponding Author's Countries\n\n    Country Articles   Freq SCP MCP MCP_Ratio\n1 USA             14 0.7778  11   3     0.214\n2 AUSTRALIA        3 0.1667   2   1     0.333\n3 CANADA           1 0.0556   1   0     0.000\n\n\nSCP: Single Country Publications\n\nMCP: Multiple Country Publications\n\n\nTotal Citations per Country\n\n  Country      Total Citations Average Article Citations\n1    AUSTRALIA               0                         0\n2    CANADA                  0                         0\n3    USA                     0                         0\n\n\nMost Relevant Sources\n\n                                                                                                          Sources       \n1  JOURNAL OF THE ROYAL SOCIETY INTERFACE                                                                               \n2  JOURNAL OF THEORETICAL BIOLOGY                                                                                       \n3  JOURNAL OF IMMUNOLOGY (BALTIMORE MD. : 1950)                                                                         \n4  PLOS ONE                                                                                                             \n5  PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA                                      \n6  BMC EVOLUTIONARY BIOLOGY                                                                                             \n7  BMC PUBLIC HEALTH                                                                                                    \n8  CLINICAL INFECTIOUS DISEASES : AN OFFICIAL PUBLICATION OF THE INFECTIOUS DISEASES SOCIETY OF AMERICA                 \n9  EPIDEMICS                                                                                                            \n10 INFECTION GENETICS AND EVOLUTION : JOURNAL OF MOLECULAR EPIDEMIOLOGY AND EVOLUTIONARY GENETICS IN INFECTIOUS DISEASES\n   Articles\n1         3\n2         3\n3         2\n4         2\n5         2\n6         1\n7         1\n8         1\n9         1\n10        1\n\n\nMost Relevant Keywords\n\n   Author Keywords (DE)      Articles Keywords-Plus (ID)     Articles\n1      HUMANS                      13  HUMANS                      13\n2      MODELS  BIOLOGICAL           8  MODELS  BIOLOGICAL           8\n3      ANIMALS                      7  ANIMALS                      7\n4      COMPUTER SIMULATION          5  COMPUTER SIMULATION          5\n5      BIOLOGICAL EVOLUTION         4  BIOLOGICAL EVOLUTION         4\n6      MODELS  IMMUNOLOGICAL        4  MODELS  IMMUNOLOGICAL        4\n7      FEMALE                       3  FEMALE                       3\n8      MICE                         3  MICE                         3\n9      MUTATION                     3  MUTATION                     3\n10     AMINO ACID SEQUENCE          2  AMINO ACID SEQUENCE          2\n\n\nTime period 2\n\nsummary(res_new, k = 10)\n\n\n\nMAIN INFORMATION ABOUT DATA\n\n Timespan                              2015 : 2020 \n Sources (Journals, Books, etc)        22 \n Documents                             29 \n Annual Growth Rate %                  -9.71 \n Document Average Age                  4.72 \n Average citations per doc             0 \n Average citations per year per doc    0 \n References                            1 \n \nDOCUMENT TYPES                     \n comparative study;journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                             1 \n journal article                                               7 \n journal article;multicenter study;research support, n.i.h., extramural                                               1 \n journal article;research support, n.i.h., extramural                                               5 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't                                               5 \n journal article;research support, n.i.h., extramural;research support, non-u.s. gov't;review                                        1 \n journal article;research support, n.i.h., extramural;research support, u.s. gov't, non-p.h.s.                                       1 \n journal article;research support, n.i.h., extramural;research support, u.s. gov't, non-p.h.s.;review                                1 \n journal article;research support, non-u.s. gov't                                               4 \n journal article;research support, non-u.s. gov't;research support, n.i.h., extramural                                               1 \n journal article;research support, non-u.s. gov't;research support, u.s. gov't, non-p.h.s.;research support, n.i.h., extramural      1 \n letter                                               1 \n \nDOCUMENT CONTENTS\n Keywords Plus (ID)                    198 \n Author's Keywords (DE)                198 \n \nAUTHORS\n Authors                               209 \n Author Appearances                    332 \n Authors of single-authored docs       1 \n \nAUTHORS COLLABORATION\n Single-authored docs                  1 \n Documents per Author                  0.139 \n Co-Authors per Doc                    11.4 \n International co-authorships %        41.38 \n \n\nAnnual Scientific Production\n\n Year    Articles\n    2015        5\n    2016        7\n    2017        3\n    2018        6\n    2019        5\n    2020        3\n\nAnnual Percentage Growth Rate -9.711955 \n\n\nMost Productive Authors\n\n   Authors        Articles Authors        Articles Fractionalized\n1     HANDEL A          29    HANDEL A                      5.494\n2     WHALEN CC          7    ANTIA R                       0.810\n3     ANTIA R            5    SHEN Y                        0.723\n4     MARTINEZ L         5    WHALEN CC                     0.651\n5     SHEN Y             5    MCKAY B                       0.629\n6     LA GRUTA NL        4    EBELL MH                      0.571\n7     MCKAY B            4    THOMAS PG                     0.571\n8     THOMAS PG          4    LA GRUTA NL                   0.540\n9     ZALWANGO S         4    ROHANI P                      0.500\n10    DENHOLM JT         3    MARTINEZ L                    0.485\n\n\nTop manuscripts per citations\n\n                                                      Paper                                    DOI TC TCperYear NTC\n1  MCKAY B, 2020, PROC BIOL SCI                                      10.1098/RSPB.2020.0496         0         0 NaN\n2  MOORE JR, 2020, BULL MATH BIOL                                    10.1007/S11538-020-00711-4     0         0 NaN\n3  HANDEL A, 2020, NAT REV IMMUNOL                                   10.1038/S41577-019-0235-3      0         0 NaN\n4  MARTINEZ L, 2019, J INFECT DIS                                    10.1093/INFDIS/JIZ328          0         0 NaN\n5  WU T, 2019, NAT COMMUN                                            10.1038/S41467-019-10661-8     0         0 NaN\n6  MCKAY B, 2019, PLOS ONE                                           10.1371/JOURNAL.PONE.0217219   0         0 NaN\n7  DALE AP, 2019, J AM BOARD FAM MED SOCIOLOGICAL METHODS & RESEARCH 10.3122/JABFM.2019.02.180183   0         0 NaN\n8  WOLDU H, 2019, J APPL STAT                                        10.1080/02664763.2018.1470231  0         0 NaN\n9  HANDEL A, 2018, PLOS COMPUT BIOL                                  10.1371/JOURNAL.PCBI.1006505   0         0 NaN\n10 CASTELLANOS ME, 2018, INT J TUBERC LUNG DIS                       10.5588/IJTLD.18.0073          0         0 NaN\n\n\nCorresponding Author's Countries\n\n    Country Articles  Freq SCP MCP MCP_Ratio\n1 USA             16 0.696   7   9     0.562\n2 AUSTRALIA        5 0.217   1   4     0.800\n3 GEORGIA          2 0.087   0   2     1.000\n\n\nSCP: Single Country Publications\n\nMCP: Multiple Country Publications\n\n\nTotal Citations per Country\n\n  Country      Total Citations Average Article Citations\n1    AUSTRALIA               0                         0\n2    GEORGIA                 0                         0\n3    USA                     0                         0\n\n\nMost Relevant Sources\n\n                                                                                                                                       Sources       \n1  PLOS ONE                                                                                                                                          \n2  PLOS COMPUTATIONAL BIOLOGY                                                                                                                        \n3  THE INTERNATIONAL JOURNAL OF TUBERCULOSIS AND LUNG DISEASE : THE OFFICIAL JOURNAL OF THE INTERNATIONAL UNION AGAINST TUBERCULOSIS AND LUNG DISEASE\n4  THE LANCET. GLOBAL HEALTH                                                                                                                         \n5  THE LANCET. RESPIRATORY MEDICINE                                                                                                                  \n6  AMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE                                                                                        \n7  BMC INFECTIOUS DISEASES                                                                                                                           \n8  BULLETIN OF MATHEMATICAL BIOLOGY                                                                                                                  \n9  ELIFE                                                                                                                                             \n10 EPIDEMICS                                                                                                                                         \n   Articles\n1         4\n2         2\n3         2\n4         2\n5         2\n6         1\n7         1\n8         1\n9         1\n10        1\n\n\nMost Relevant Keywords\n\n          Author Keywords (DE)      Articles           Keywords-Plus (ID)     Articles\n1  HUMANS                                 23 HUMANS                                 23\n2  ANIMALS                                 8 ANIMALS                                 8\n3  FEMALE                                  7 FEMALE                                  7\n4  MALE                                    7 MALE                                    7\n5  MICE                                    6 MICE                                    6\n6  ADULT                                   5 ADULT                                   5\n7  CHILD                                   5 CHILD                                   5\n8  ADOLESCENT                              4 ADOLESCENT                              4\n9  ANTIVIRAL AGENTS/THERAPEUTIC USE        4 ANTIVIRAL AGENTS/THERAPEUTIC USE        4\n10 CHILD  PRESCHOOL                        4 CHILD  PRESCHOOL                        4\n\n\nNote that some values are reported as NA, e.g. the citations. Depending on which source you got the original data from, that information might be included or not. In my case, it is not.\n\n\nGetting a table of co-authors\nThis can be useful for NSF applications. For reasons nobody understands, that agency still asks for a list of all co-authors. An insane request in the age of modern science. If one wanted to do that, the following gives a table.\nUpdate: I have since created a short blog post describing how to do just that part in a bit more detail. It has a few additional components that might be useful, if interested check it out here.\nHere is the full table of my co-authors in the first period dataset.\n\n#removing the 1st one since that's me\nauthortable = data.frame(res_old$Authors[-1])\ncolnames(authortable) = c('Co-author name', 'Number of publications')\nknitr::kable(authortable)\n\n\n\n\nCo-author name\nNumber of publications\n\n\n\n\nANTIA R\n6\n\n\nDOHERTY PC\n3\n\n\nLA GRUTA NL\n3\n\n\nLONGINI IM JR\n3\n\n\nTHOMAS PG\n3\n\n\nPILYUGIN SS\n2\n\n\nROHANI P\n2\n\n\nSTALLKNECHT D\n2\n\n\nTURNER SJ\n2\n\n\nAKIN V\n1\n\n\nBEAUCHEMIN CA\n1\n\n\nBIRD NL\n1\n\n\nBROWN J\n1\n\n\nCHADDERTON J\n1\n\n\nCUKALAC T\n1\n\n\nDESAI R\n1\n\n\nDICKEY BW\n1\n\n\nFUNG IC\n1\n\n\nHALL AJ\n1\n\n\nHALL D\n1\n\n\nHEMBREE CD\n1\n\n\nJACKWOOD MW\n1\n\n\nKEDZIERSKA K\n1\n\n\nKJER-NIELSEN L\n1\n\n\nKOTSIMBOS TC\n1\n\n\nLEBARBENCHON C\n1\n\n\nLEON JS\n1\n\n\nLEVIN BR\n1\n\n\nLI Y\n1\n\n\nLOPMAN B\n1\n\n\nMARGOLIS E\n1\n\n\nMATTHEWS JE\n1\n\n\nMCDONALD S\n1\n\n\nMIFSUD NA\n1\n\n\nMOFFAT JM\n1\n\n\nNGUYEN TH\n1\n\n\nPARASHAR UD\n1\n\n\nPELLICCI DG\n1\n\n\nROWNTREE LC\n1\n\n\nROZEN DE\n1\n\n\nWHALEN CC\n1\n\n\nYATES A\n1\n\n\nZARNITSYNA V\n1\n\n\nZHENG N\n1\n\n\n\n\n\nSince I have many more co-authors in the second period, I’m not printing a table with all, instead I’m just doing those with whom I have more than 2 joint publications.\n\n#removing the 1st one since that's me\nauthortable = data.frame(res_new$Authors[-1])\nauthortable <- authortable %>% dplyr::filter(Freq>2)\ncolnames(authortable) = c('Co-author name', 'Number of publications')\nknitr::kable(authortable)\n\n\n\n\nCo-author name\nNumber of publications\n\n\n\n\nWHALEN CC\n7\n\n\nANTIA R\n5\n\n\nMARTINEZ L\n5\n\n\nSHEN Y\n5\n\n\nLA GRUTA NL\n4\n\n\nMCKAY B\n4\n\n\nTHOMAS PG\n4\n\n\nZALWANGO S\n4\n\n\nDENHOLM JT\n3\n\n\nEBELL M\n3\n\n\nMCBRYDE ES\n3\n\n\nSUMNER T\n3\n\n\nTRAUER JM\n3\n\n\n\n\n\n\n\nMaking a table of journals\nIt can be useful to get a list of all journals in which you published. I’m doing this here for the second time period. With just the bibliometrix package, I can get a list of publications and how often I have published in each.\n\njournaltable = data.frame(res_new$Sources)\n#knitr::kable(journaltable) #uncomment this to print the table\n\nAs mentioned in part 1 of this series of posts, the Impact Factor feature from the scholar package doesn’t work anymore. I’m leaving the old code in there in case it ever comes back. For now, there is no Impact Factor information. (I haven’t tried to figure out if there is another way to get it.)\nIt might also be nice to get some journal metrics, such as impact factors. While this is possible with the scholar package, the bibliometrix package doesn’t have it.\nHowever, the scholar package doesn’t really get that data from Google Scholar, instead it has an internal spreadsheet/table with impact factors (according to the documentation, taken - probably not fully legally - from some spreadsheet posted on ResearchGate). We can thus access those impact factors stored in the scholar package without having to connect to Google Scholar. As long as the journal names stored in the scholar package are close to the ones we have here, we might get matches.\n\n#library(scholar)\n#ifvalues = scholar::get_impactfactor(journaltable[,1], max.distance = 0.1)\n#journaltable = cbind(journaltable, ifvalues$ImpactFactor)\n#colnames(journaltable) = c('Journal','Number of Pubs','Impact Factor')\ncolnames(journaltable) = c('Journal','Number of Pubs')\nknitr::kable(journaltable)\n\n\n\n\n\n\n\n\nJournal\nNumber of Pubs\n\n\n\n\nPLOS ONE\n4\n\n\nPLOS COMPUTATIONAL BIOLOGY\n2\n\n\nTHE INTERNATIONAL JOURNAL OF TUBERCULOSIS AND LUNG DISEASE : THE OFFICIAL JOURNAL OF THE INTERNATIONAL UNION AGAINST TUBERCULOSIS AND LUNG DISEASE\n2\n\n\nTHE LANCET. GLOBAL HEALTH\n2\n\n\nTHE LANCET. RESPIRATORY MEDICINE\n2\n\n\nAMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE\n1\n\n\nBMC INFECTIOUS DISEASES\n1\n\n\nBULLETIN OF MATHEMATICAL BIOLOGY\n1\n\n\nELIFE\n1\n\n\nEPIDEMICS\n1\n\n\nEPIDEMIOLOGY AND INFECTION\n1\n\n\nFRONTIERS IN IMMUNOLOGY\n1\n\n\nJOURNAL OF APPLIED STATISTICS\n1\n\n\nJOURNAL OF THE AMERICAN BOARD OF FAMILY MEDICINE : JABFM\n1\n\n\nNATURE\n1\n\n\nNATURE COMMUNICATIONS\n1\n\n\nNATURE REVIEWS. IMMUNOLOGY\n1\n\n\nPHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY OF LONDON. SERIES B BIOLOGICAL SCIENCES\n1\n\n\nPLOS BIOLOGY\n1\n\n\nPROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA\n1\n\n\nPROCEEDINGS. BIOLOGICAL SCIENCES\n1\n\n\nTHE JOURNAL OF INFECTIOUS DISEASES\n1\n\n\n\n\n\nOk that worked somewhat. It couldn’t find several journals. The reported IF seem reasonable. But since I don’t know what year those IF are from, and if the rest is fully reliable, I would take this with a grain of salt.\n\n\nDiscussion\nThe bibliometrix package doesn’t suffer from the problems that I encountered in part 1 of this post when I tried the scholar package (and Google Scholar). The downside is that I can’t get some of the information, e.g. my annual citations. So it seems there is not (yet) a comprehensive solution, and using both packages seems best.\nA larger overall problem is that a lot of this information is controlled by corporations (Google, Elsevier, Clarivate Analytics, etc.), which might or might not allow R packages and individual users (who don’t subscribe to their offerings) to access certain information. As such, R packages accessing this information will need to adjust to whatever the companies allow.\n\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Using {R} to Analyze Publications - Part 2},\n  date = {2020-02-02},\n  url = {https://www.andreashandel.com/posts/2020-02-02-publications-analysis-2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Using R to Analyze Publications - Part\n2.” February 2, 2020. https://www.andreashandel.com/posts/2020-02-02-publications-analysis-2."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html",
    "title": "Text analysis of a mid-semester course survey",
    "section": "",
    "text": "Our center for teaching and learning administered a mid-semester survey to the students in my fall 2019 online Modern Applied Data Analysis course. I figured it would make for a nice and topical exercise if I performed some analysis of the survey results. Students agreed to have the - fully anonymous - results posted publicly. This is my quick and simple text analysis."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#some-cleaning-actions",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#some-cleaning-actions",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Some cleaning actions",
    "text": "Some cleaning actions\n\nd <- d %>% clean_names() #clean column names, which are the full questions\norig_quest <- data.frame(Number = paste0('Q',1:11), Question = names(d)) #save names and replace with simpler ones for now\nnames(d) = paste0('Q',1:11) #just call each column as Q1, Q2,... originallly asked question is stored in orig_quest\nkable(orig_quest) %>% kable_styling() #print them here for further reference  \n\n\n\n \n  \n    Number \n    Question \n  \n \n\n  \n    Q1 \n    whats_working_well_in_this_class_what_are_the_strengths_of_the_class_and_which_aspects_are_having_a_positive_impact_on_your_learning \n  \n  \n    Q2 \n    what_aos_not_working_so_well_in_this_class_what_aspects_are_having_a_less_positive_impact_on_your_learning \n  \n  \n    Q3 \n    what_specific_changes_do_you_think_should_be_made_to_improve_your_experience_in_this_class \n  \n  \n    Q4 \n    i_think_the_pace_of_this_class_is \n  \n  \n    Q5 \n    are_there_specific_modules_that_should_be_adjusted_and_how_5 \n  \n  \n    Q6 \n    the_quantity_of_material_covered_in_each_module_is \n  \n  \n    Q7 \n    are_there_specific_modules_that_should_be_adjusted_and_how_7 \n  \n  \n    Q8 \n    the_level_of_difficult_of_each_module_is \n  \n  \n    Q9 \n    are_there_specific_modules_that_should_be_adjusted_and_how_9 \n  \n  \n    Q10 \n    on_average_i_spend_this_many_hours_per_week_doing_work_for_this_course \n  \n  \n    Q11 \n    finally_what_is_your_gold_star_top_choice_number_one_recommendation_for_a_constructive_change_your_instructor_can_make_in_this_course \n  \n\n\n\n\n\nMore cleaning\n\nvisdat::vis_dat(d) #missing values\n\nWarning: `gather_()` was deprecated in tidyr 1.2.0.\nPlease use `gather()` instead.\nThis warning is displayed once every 8 hours.\nCall `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\n\n\n\n\n#looks like a few students left some entries blank. Should be ok. One student only answered 1 question. Quick look at entry.\nprint(d[12,2])\n\n# A tibble: 1 × 1\n  Q2                                       \n  <chr>                                    \n1 Elc system not work well for online class\n\n#ok, not too useful (though I agree with the statement). Let's remove that student/observation.\nd<- d[-12,]\n# most questions were free text, but some were specific choices, so should be grouped as factor.\nd <- d %>% dplyr::mutate_at(c(\"Q4\", \"Q6\",\"Q8\"), factor)\n#Q10 is number, should be numeric but was text field so different entries exist\n#small enough to print here\nprint(d$Q10)\n\n [1] \"20\"             \"15-20\"          \"15 or more\"     \"15\"            \n [5] \"14-16\"          \"12\"             \"15\"             \"10\"            \n [9] \"30\"             \"20\"             \"10 to 12 hours\" \">10 hours\"     \n[13] \"2\"              \"20\"            \n\n#ok, this is kinda bad style, but the dataset is so small that it's easiest to replace the non-numeric values by hand. I'll set them to their mean or the specified limit.\nd$Q10[c(2,3,5,11,12)] <- c(17.5,15,15,11,10)\nd$Q10 <- as.numeric(d$Q10)\nprint(d$Q10)\n\n [1] 20.0 17.5 15.0 15.0 15.0 12.0 15.0 10.0 30.0 20.0 11.0 10.0  2.0 20.0"
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#drawing-first-conclusions",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#drawing-first-conclusions",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Drawing first conclusions",
    "text": "Drawing first conclusions\n\nkable(orig_quest[c(4,6,8),]) %>% kable_styling()\n\n\n\n \n  \n      \n    Number \n    Question \n  \n \n\n  \n    4 \n    Q4 \n    i_think_the_pace_of_this_class_is \n  \n  \n    6 \n    Q6 \n    the_quantity_of_material_covered_in_each_module_is \n  \n  \n    8 \n    Q8 \n    the_level_of_difficult_of_each_module_is \n  \n\n\n\n\nd %>% dplyr::select(Q4, Q6, Q8) %>% summary()\n\n          Q4                Q6                Q8    \n just right:10   right amount:4   just right   :10  \n too fast  : 4   too much    :9   too difficult: 3  \n                 NA's        :1   NA's         : 1  \n\nplot(1:14,d$Q10, ylab = 'Time spent per week')\nlines(1:14,rep(12,14))\n\n\n\n\nBased on answers to questions 4,6 and 8, the majority of students think the pace and level of difficulty of the course is right but the amount of material covered is too much. Based on answer to Q10, students spend more time than my target (12 hours, solid line). Even accounting for some “inflation factor” (people generally over-estimate the time they spend on tasks like these, counting all the other things they do at the same time e.g., texting/email/FB/drinknig coffee/…), the overall amount seems too high, and it agrees with Q6 answers about too much material.\nFirst conclusion: Reduce weekly workload, probably best by reducing assigned reading (see text answers which I already glimpsed at 😃)."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#manual-text-analysis",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#manual-text-analysis",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Manual text analysis",
    "text": "Manual text analysis\n\n#dropping the question/variables analyzed above\nd <- d %>% dplyr::select( -c(\"Q4\", \"Q6\", \"Q8\", \"Q10\") )\n\nQuestions 5, 7 and 9 ask how modules should be adjusted regarding pace, quantity and difficulty, so it’s worth looking at those questions on their own.\n\nd2 <- d %>% dplyr::select( Q5, Q7, Q9)\nis.na(d2) #some students didn't write anything for any of those questions, remove before printing content.\n\n         Q5    Q7    Q9\n [1,] FALSE  TRUE FALSE\n [2,] FALSE FALSE FALSE\n [3,]  TRUE  TRUE  TRUE\n [4,] FALSE FALSE FALSE\n [5,] FALSE FALSE FALSE\n [6,]  TRUE  TRUE  TRUE\n [7,] FALSE FALSE FALSE\n [8,]  TRUE  TRUE  TRUE\n [9,]  TRUE  TRUE  TRUE\n[10,]  TRUE  TRUE  TRUE\n[11,] FALSE FALSE FALSE\n[12,]  TRUE  TRUE  TRUE\n[13,] FALSE  TRUE  TRUE\n[14,]  TRUE FALSE  TRUE\n\nd2 <- d2[which(rowSums(is.na(d2)) != 3),] #remove all rows/obersvations that have NA to all 3 questions\nnames(d2) <- c('too fast','too much','too hard')\nknitr::kable(d2) %>% kable_styling() ##show rest\n\n\n\n \n  \n    too fast \n    too much \n    too hard \n  \n \n\n  \n    All of them are to long except for module 2. \n    NA \n    The quizzes are becoming ridiculously difficult. \n  \n  \n    None, I have enjoyed the pace. \n    None, I think the content is appropriate. \n    The toughest module thus far in the course was the strings module. Though it should be noted, I think the content was very appropriate to cover the concepts of this module as they are quite difficult. I really benefited from completing some of the Regex crosswords while working through this chapter. \n  \n  \n    I think all modules need to be adjusted. \n    Considering we have only done the first half of the class and the shortest amount of time I have spent in one week on this class was the first module and I still did more than 9 hours of work for what should have been a half week seems unreasonable. For all other modules I spend over 12 hours each week on this class sometimes upwards of 25 hours in a single week. \n    I don't think the modules themselves are difficult just the content is being squeezed in and the time it takes doesn't correlate to the amount I feel like I'm learning, where I should be learning/proficient in much more than I currently am based on the amount of time spent. \n  \n  \n    The visualization module is pretty hefty in terms of how much time is spent. But it's all the same topic so I don't know how that would be split up. \n    see above \n    The modules are just difficult enough that the first couple times I try I struggle and it's hard, but after giving it a go it becomes much easier and I get it. So its enjoyably difficult. \n  \n  \n    I feel like I like the order and pace of the course materials. We can learn all of the material in the module in one week - it's just exhausting to do so. \n    Trim the reading in the modules and add more exploratory exercises. \n    I feel like the modules scale up in difficulty each week, but proportionally to the growth in our skills. I feel like I am really learning R for the first time! (After three other courses in R...) \n  \n  \n    In the tidyverse module, we used ggplot, but then we actually learned about how to do ggplot which would have been helpful before tidyverse. The R module was definitely a lot of material and the coding exercise was a steep learning curve for the first true coding exercise of the course. I understand sometimes the best way to learn something is struggling through, but I think it can be difficult for new students to make the sudden leap from follow a specific script in a book chapter to make up your own code. Maybe something as in altering existing scripts first? \n    While now there is a distinction on what is going to be on the quiz, there are multiple chapters and tutorials I haven‚Äôt read through yet. I‚Äôm actively searching to see what to prioritize first in a module. Module 4/5 also had a lot of (useful) material to work through. \n    I think working through the material for the quizzes themselves is just the right difficulty but the assessments range from not so difficult (Tidyverse) to spending a couple of hours googling and troubleshooting (first R and Visualization). While you said to spend no more than 1 or 2 hours on Visualization, it still took me 4 hours to figure out. \n  \n  \n    Not that I see so far. \n    NA \n    NA \n  \n  \n    NA \n    Most modules waver between just the right amount and a little too much. However, this goes hand in hand with what I have identified also as a strength of the course (great curation of resources), so perhaps adjustment isn't necessary... maybe it would be useful to have a more organized list of required and optional readings. \n    NA \n  \n\n\n\n\n\nConclusions from anwers to those questions: Overall too much material (see above), level of difficulty overall ok but too fast/crowded. Again, solution is to reduce (required) material.\nNext, let’s look at “whats working/not working” questions.\n\nd2 <- d %>% dplyr::select( Q1, Q2)\nnames(d2) <- c('good','bad')\nknitr::kable(d2) %>% kable_styling() \n\n\n\n \n  \n    good \n    bad \n  \n \n\n  \n    The positive impact would be that I am learning completely new things I have had no exposure too. \n    This class is extremely overwhelming. It is online and the professor is the worst time estimator I have ever seen. He assigns way to much with absolutely no emphasis on what is important. As he is the ‚Äúexpert‚Äù in this subject he should be able to narrow it down and make this course more reasonable. I should not be spending 20-25 hours a week on one course that is 3 credits. \n  \n  \n    I really enjoy the exercises assigned for each module. I am receiving a good amount of background information from the modules/reading, however I feel I really start to understand the material once I have used it in practice. Additionally, a good portion of the exercises thus far are directly applicable to the analysis I will be using for my personal research which is excellent practice.\n\nI also really appreciate that the structure of the course. I really struggled with the basic work flow of R scripts and GitHub initially as I am newer to the program. However, the progressive flow that this course has created has allowed me to learn the process step-by-step and frequently connected back to information that was previously used in an earlier module. As a result, I have become very comfortable with the basic usage/flow of R and I am excited to move into the more detailed functions of the program in the latter half of the semester! \n    The only small issue I have encountered thus far with this course is with the quiz time limits. I am sometimes having trouble sifting through all of my notes quickly enough to properly answer the quiz questions. I take approximately 10-20 pages of notes on each module (depending on the size and amount of exercises) and I can sometime struggle to locate the specific material in my notes within these time constraints. I absolutely understand the need for a time limit with an online class structure, however 20 minutes can be a bit tight for some of the quizzes. A simple bump from a 20-30 minute time range to 30-40 minute time range would be more than enough to address all questions adequately, while still ensuring students review the module content beforehand. \n  \n  \n    I love the tidyverse package and thanks to instructions on R primer.\n\nI love R primer, because it raise the problem and then have a space to write code to try, and available solution. \n    Some materials (R data science, IDS) are useful too. However, I do not like them. The reason is that they give me the knowledge, and explain some simple code. Then, I have exercise part with more complicated questions. I do not know how to do it sometimes and got stuck. One example is chapter about regular expressions.\n\nAnother thing is that this class took me so much time per week. It is one third of my week. I still have other three classes, and research duties.\n\nI expect that taking class is the quicker and better way to learn than learning by myself. This class is not what I expect. It is too time-consuming because I mainly have to learn by myself. \n  \n  \n    The R primers are very useful for understanding the material, the exercises you give us, and some of the exercises in the IDS book (however, they need to be narrowed down to what is actually important). \n    Having 5+ chapters of reading each week isn't useful. The readings in general aren't useful for coding, at least for me. They should supplement and be a reference for the actual coding we are doing but not be the entire basis of the quizzes. I spend so much time reading and I don't actually understand any of it unless I'm doing the coding. However, expecting to do every exercise that comes with some of the reading isn't working because I currently spend at least 12 hours every week just doing the exercises that go with readings and I end up not retaining much because I'm overwhelmed. \n  \n  \n    I really enjoy having a structured way to learn r for data analysis instead of just learning it on my own. It makes it much more manageable and mentally-forgiving when there are other people learning/struggling at the same time as you. All of the data/resources are in one place and present in a timeline that makes it easier to understand and learn. \n\nI also really enjoyed how he makes us go back to the other student's repositories and work with them. It helps foster the feeling/attitude that we're classmates and get to know each other more even though it's an online course. \n    The resources are a lot to work through. I don't have previous R experience but this class is taking a lot of my time to work through the R for data science book. \n  \n  \n    The amount of information available. Strengths of the class are being able to work with other students to complete assignments and get ideas from one another. \n    The amount of work we have to do every week, including reading, quizzes, and lengthy assignments. Someone new to coding might find this extremely overwhelming considering they have other classes to work on as well as their own laboratory work. The quizzes make me feel like I have not learned anything because they are very weird, specific questions that I have to spend a long time going back through the reading to hunt for. \n  \n  \n    The course website is very thorough. It is very clear that Dr. Handel has put a lot of thought and effort into building the materials and course for this class. The writing is very clear and accessible. The site and materials are very consistent, which is helpful. \n\nThe RPrimers, IDS, and R4DS chapters are helpful (in that order, respectively). The class exercises are very helpful. I am finding myself coding more often and drafting unique codes, which was my goal from the start. This is the most well-rounded course in R I have taken. \n\nDr. Handel's exercises are by far the most helpful. We get to think through the material and use resources as-needed. These are exercises I will need in my own analysis. I have been using these codes as guidelines for working through my own data sets.\n\nI love the R-Primers as an introduction, and then the Exercises. I think we could take a stab at the exercises without the IDS and R4DS readings, or have those readings embedded into the exercise. \n\nSuch as...\"Try to make this figure\" If you are stuck, Chapter 4.5 might be a good a reference. \n    I'm lucky in that there are a five or six students in my department all taking this course. Since we see each other regularly, we can help each other along and make sense of the material. I can't imagine working through this course alone, with no face-to-face interaction. The group dynamic helps me read into what is important and helps trouble-shoot when there are problems. The eLC discussion board is not great for connecting with others...\n\nOutside of these few people that I am lucky to see in person, I think it is hard to connect with the other students to gauge if I am \"on the right track\" or \"falling behind\" or plain bad at coding. We don't engage much with other students, so it's hard to tell if they're succeeding or struggling as much as we are.  I keep thinking - \"am I the only one feeling this way?\" \n\nThe modules seem to be thick/dense for just one week's worth of work. As soon as I finish a module, I am exhausted by the material, but then have to start right back up again. I don't feel like I have enough time to play around with all of the functions we just learned before it's time to start learning new ones. I'd like more time to apply them to data sets in example exercises. \n\nI've been to a few meetings with my advisor now where I say that most of my week has been taken up with this course. Perhaps I need to work on my own time management and expectations for this course work. Of course, I do think the time investment will pay off! I am excited to start analyzing my own data set! \n\nTo get by, I feel like I have started skimming the readings, or reading them \"diagonally\" as some people may say. I have \"CTRL-F\" through the readings just for the quizzes. I don't think this is ideal, and I'm sure Dr. Handel wouldn't want us to resort to this. \n  \n  \n    I like the exercises in R. While my lack of previous experience means I have to spend more time trying to understand how to put complex displays together, what I learn from this stays with me a lot better. \n    I know that I review the material quite a bit for the quizzes yet continue to be surprised at what actually shows up on the quiz. This may be a personal problem as the class distribution shows that other individuals are doing well. However, I know that it continues to be frustrating. Also, the MADA course on GitHub is still a bit difficult to navigate and the info given for each module is a lot to absorb without there being an application element. \n  \n  \n    The supplementary readings are useful, but that's about all that's having a positive impact on my learning in this class. \n    The class being online is convenient, but if the class were in person and I could see step by step instructions, and everything would make more sense. Yes I understand you learn by making mistakes but taking 20+ hours every single time I do an assignment or reading is absolutely ridiculous due to the fact that all of my instructions are typed out, and there's no \"teaching\" in my opinion. This class structure is not conducive to my learning style. There aren't even any videos of the professor lecturing and walking us through lessons. I'm not absorbing any material by reading over 50+ pages then being thrown an assignment and an overly detailed quiz that doesn't even focus on main points but extremely detailed findings that DO NOT showcase what I've learned. \n  \n  \n    The class is quite well structured with nice use of different video resources. \n    Some of the assignments are very much like busy work and are time consuming without being particularly helpful and just end up putting one under pressure during the week whenever there are so many other draws on ones time at graduate school. \n  \n  \n    I do like the material we cover since it‚Äôs mostly relevant, I feel like I‚Äôm learning more of the ins and outs of R each week! I think the set up of the class with readings then an assessment works well. \n    There is a lot of material to cover each week. While I enjoy it and I‚Äôm interested, it is all mostly new to me and I feel like I have to rush through the readings to do the quiz by Friday. \n  \n  \n    This class requires a lot of coding, and I had no previous experience with coding, but we were provided with a lot of resources and exercises to start from the beginning. \n    There are too much reading assignments. It really takes a lot of time for this class. I cold barely finish these assignments every week, not alone to digest them and use them. \n  \n  \n    I think this course is set up very well. The modules are constructed in a manner that makes the content accessible and the exercises are well guided. \n    I would say that nothing related to the class is having a negative impact on my learning. \n  \n  \n    We receive a ton of excellent resources. Dr. Handel is very responsive to my feedback, which I greatly appreciate. I was very apprehensive about taking an online course, but my fears have been completely assuaged. This class has already been incredibly beneficial to me. \n    As I've already shared with Dr. Handel, the online quizzes are sometimes frustrating in that we are only asked 6 to 7 very specific questions on a great deal of material. I don't feel we're given enough time to complete it, and partial credit isn't awarded. However, as I mentioned above, Dr. Handel is very receptive to feedback and I have already spoken with him about my concerns with this issues. \n  \n\n\n\n\n\nConclusions from anwers to those questions: Overall too much material, especially too much reading. R primers are good. Other resources are hit or miss. Quizzes are not working, need to be ditched or altered. Maybe more exercises. Find better alternative to eLC.\nFinally, the 2 remaining questions are about improvements, phrased in 2 different ways. Let’s look at them together.\n\nd2 <- d %>% dplyr::select( Q3, Q11)\nd2 <- d2[-12,] #this student didn't provide answers to either question\nnames(d2) <- c('specific suggested changes','number one recommendation')\nknitr::kable(d2) %>% kable_styling() \n\n\n\n \n  \n    specific suggested changes \n    number one recommendation \n  \n \n\n  \n    Less information per module. Most of these modules should be split into two weeks. \n    You need to put yourself in the eyes of a graduate student who is new to R and has a minimal stats background. As a graduate student in general public health field, what are the most important things to prepare is for publishing papers, understanding data, etc. \nWe should have spent two weeks on data wrangling and two weeks on data visualization. These are EXTREMELY important topics, and they were rushed through in one week. I do not think my understanding of how to do these things is good, and instead we are moving onto other topics that are less important. \n  \n  \n    As mentioned above, just a bit more time on the quizzes would be great! \n    A small increase in the time allotted to take the weekly quizzes. \n  \n  \n    I suggest that you can give the solution for this chapter ahead. We can learn through solution if we get stuck.\n\nTo reduce time to learn by myself, I suggest that professor can create R scripts with coding and explain what is the purpose of the code by text or better having a video to show how the code works in purpose.I think that this way will be better than the way that students have to go through many chapters of reading. If going through many chapters, students have to learn the new things and be able to finalize the knowledge at the same. This process is hard. So, I suggest that professor finalize the knowledge, then show it to us. You can indicate book in case they are still confused or do not understand, and need to read further. \n    For the knowledge that I learn through the R primer, I am happy with them. However, for other chapters that have to read through R4DS, and IDS, it is too time consuming and not an efficient way to learn. If you can have a better resources for the knowledge covered in R4DS and IDS, I am happy with this course. \n  \n  \n    More homework assignments that mimic the material that is in the readings, where we can use the readings as a reference/starting point. I feel like if we replace the quizzes/readings with an assignment that goes over what is generally important from the chapters you assign it would be more worthwhile. \n    No quizzes - replace with short assignments that reinforce the material presented in the reading each week. Still makes it so you have to read but not that you have to spend hours upon hours doings so. \n\nAlso as a side note - maybe introduce folders in the class github so that its organized by assignment so we can easily find things. \n  \n  \n    Specifically not much. \n    Establishing a working discussion board from the beginning would probably be the best thing. \n  \n  \n    I think that there should be more time allotted for the quizzes, simply because of the complexity of the questions as well as the vast amount of reading required. \n    Be a little more lenient on quizzes and giving out massive assignments. For an online course in coding, you're asking a lot from us. \n  \n  \n    It might be helpful to have \"coding drop in sessions\" where students can meet in a room, bring snacks, and discuss the course as a group. \n\nIt would be helpful if each modules was spread out over two weeks. It would be helpful to have more of Dr. Handel's exercises (not necessarily the IDS R4DS ones). \n    Trim the modules \n  \n  \n    I know that I do significantly better on the exercise portions of the class as I can see where the material is applicable but, again, this may be a personal preference. One blanket recommendation would be improving the navigational ability of the GitHub course site and breaking down the sections within each module to highlight where outside links need to be used. \n    NA \n  \n  \n    If this class is going to remain online I think the professor should record himself lecturing and have a split screen showing him using R instead of reading everything to get instructions. I think the supplementary reading/interactive learnings are helpful, but they should be in addition to teaching, not the only mode of learning. At this point I don't even feel like my professor is teaching me, a website and book are (barely). \n    Either change back to in person instruction or have video lectures with a computer split screen to show examples, THEN supplement with readings and interactive learning. The entire course just can't be a written instruction list and a website with definitions. \n  \n  \n    Make the assignments more relevant rather than busy work or at least have a required part and then then optional parts because to have to do a time consuming assignment each week isn't beneficial \n    NA \n  \n  \n    I liked the quiz this week (Module 7) because you specifically said that the exercises were optional in the reading so it did take the pressure off of having to go through multiple chapters. If the purpose of the quizzes are to make sure we are reading the material, then it was nice to focus on the content of the chapters. (And not lose 30 minutes to solving one exercise) \n    exercises from chapters not be covered on quizzes. :) \n  \n  \n    Again, the class is great. The only improvements are needed by me. \n    I do think the tidyverse is so massive an undertaking for a beginner that it could be split into two modules. \n  \n  \n    Nothing in particular comes to mind at the moment, as I've felt very comfortable voicing recommendations to Dr. Handel as they arise :) \n    Alternative way of testing for our reading each week? As of right now, the quizzes are precarious (one wrong answer can doom a grade) and time-constrained, and a pretty significant source of stress for me. However, as mentioned earlier, Dr. Handel and I are in communication about this! \n  \n\n\n\n\n\nConlusions from these answers: Reduce content per module (or alternatively increase time). Adjust or drop quizzes. More exercises. Record some lectures or provide links to recordings."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#automated-text-analysis",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#automated-text-analysis",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Automated text analysis",
    "text": "Automated text analysis\nSo this is likely not too useful, but I wanted to play around with some automated text analysis. Maybe the computer can figure out things I can’t?\nI don’t actually know how to do text analysis, so I’ll have to peek at the tidytext tutorial. Getting some ideas from this tutorial and the Text Mining with R book.\nTurn all answers into a long dataframe of words\n\nd2 <- d %>% tidyr::pivot_longer(cols = starts_with('Q'), names_to =\"question\", values_to = \"answers\") %>% \n            drop_na() %>%\n            unnest_tokens(word, answers, token = \"words\")\n\nLook at most frequent words.\n\nd2 %>%  count(word, sort = TRUE) \n\n# A tibble: 835 × 2\n   word      n\n   <chr> <int>\n 1 the     215\n 2 i       134\n 3 to      126\n 4 and      90\n 5 of       81\n 6 a        75\n 7 is       59\n 8 in       57\n 9 that     48\n10 are      47\n# … with 825 more rows\n\n\nThe usual words are the most frequent."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#sentiment-analysis",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#sentiment-analysis",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Sentiment analysis",
    "text": "Sentiment analysis\nSentiment analysis, look at most frequent positive and negative words.\n\nbing <- get_sentiments(\"bing\")\npositive <- bing %>% filter(sentiment == \"positive\")\nd2 %>% semi_join(positive) %>% nrow()\n\n[1] 158\n\nnegative <- get_sentiments(\"bing\") %>% filter(sentiment == \"negative\")\nd2 %>% semi_join(negative) %>% nrow()\n\n[1] 72\n\nbing_word_counts <- d2 %>%\n  inner_join(bing) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  mutate(n = ifelse(sentiment == \"negative\", -n, n)) %>%\n  mutate(word = reorder(word, n)) \n\nPlot positive and negative words.\n\nbing_word_counts %>% ggplot(aes(word, n, fill = sentiment)) +\n  geom_bar( stat = \"identity\") +\n  coord_flip() +\n  labs(y = \"Counts\")\n\n\n\n\nAbout twice as many positive as negative words, i guess that’s good 😃. And the most frequent negative words do reflect that things are “too much”.\nLet’s look at sentiment per question. Higher values are more positive.\n\nquestion_sentiment <- d2 %>%\n      inner_join(bing) %>%\n      count(question, sentiment) %>%\n  spread(sentiment, n, fill = 0) %>%\n  mutate(sentiment = positive - negative)\n\nggplot(question_sentiment, aes(question, sentiment)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) \n\n\n\n\nNot surprising, the 1st question “what is working well” has lots of positive. Surprisingly, question 2, “what’s not working well” has fairly high positive sentiment. One problem could be that what I’m plotting is total counts, but I should probably normalize by total words written per question. Let’s try:\n\nwords_per_q <- d2 %>% group_by(question) %>% count()\nprint(words_per_q)\n\n# A tibble: 7 × 2\n# Groups:   question [7]\n  question     n\n  <chr>    <int>\n1 Q1         799\n2 Q11        393\n3 Q2        1304\n4 Q3         582\n5 Q5         193\n6 Q7         202\n7 Q9         240\n\n\nYep, looks like most words were written by far for Q2. Maybe not a good sign? But maybe ok, since this specifically solicited feedback on all aspects. So let’s replot sentiment, normalized by number of words.\n\nquestion_sentiment <- question_sentiment %>% mutate(sent_per_word = sentiment / words_per_q$n)\nggplot(question_sentiment, aes(question, sent_per_word)) +\n  geom_bar(stat = \"identity\", show.legend = FALSE) \n\n\n\n\nOk, changed things a bit but not a lot. Q2 drop (expected) is most noticable change. Still, even for the “what’s not good” section, positive words dominate. That either means the course is quite good, or students are very optimistic or polite, or it might mean nothing at all."
  },
  {
    "objectID": "posts/2020-02-15-MADA-survey-analysis/index.html#wordclouds",
    "href": "posts/2020-02-15-MADA-survey-analysis/index.html#wordclouds",
    "title": "Text analysis of a mid-semester course survey",
    "section": "Wordclouds",
    "text": "Wordclouds\nWhy not? Everyone loves a wordcloud, even if they are just fun to look at, right?\n\nd2 %>%\ninner_join(bing) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  acast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\n  wordcloud::comparison.cloud(colors = c(\"#F8766D\", \"#00BFC4\"),\n                   max.words = 100)\n\n\n\n\nAt this point, I ran out of ideas for further text analysis. I didn’t think analysis by word pairs, or sentences, or such alternatives would lead to any further interesting results. I looked in the Text Mining with R book for some more ideas of what kind of analyses might be useful, but can’t come up with anything else. Not that the above ones are that useful either, but it was fun to try some text analysis, which is a type of data analysis I’m not very familiar with. So, I’ll stop this here. Feel free to play around yourself, you have access to the raw data and this script in the GitHub repository."
  },
  {
    "objectID": "posts/2020-05-29-automate-conflict-of-interest-form/index.html",
    "href": "posts/2020-05-29-automate-conflict-of-interest-form/index.html",
    "title": "Generating a conflict of interest form automatically",
    "section": "",
    "text": "Required packages\n\nlibrary(dplyr)\nlibrary(knitr)\nlibrary(flextable)\n#remotes::install_github('massimoaria/bibliometrix')\nlibrary(bibliometrix)\n\n\n\nLoading data\nAs explained in a previous post, the currently best way to get all my papers is to download them from NIH’s “My Bibliography” and export it in MEDLINE format. Then read in the file with the code below.\n\n#read bib file, turn file of references into data frame\npubs <- bibliometrix::convert2df(\"medline.txt\", dbsource=\"pubmed\",format=\"pubmed\") \n\n\nConverting your pubmed collection into a bibliographic dataframe\n\nDone!\n\n\nGenerating affiliation field tag AU_UN from C1:  Done!\n\n\nEach row of the data frame created by the convert2df function is a publication, the columns contain information for each publication. For a list of what each column variable codes for, see the bibliometrix website.\n\n\nGetting the right time period\nThis specific funding agency I’m currently writing a COI for (NIFA) requires co-authors of the last 3 years, so let’s get them. I don’t know if they mean 3 full years. I’m doing this mid-2020, so to be on safe side, I go back to 2017.\n\nperiod_start = 2017\npubs_new = pubs[pubs$PY>=period_start,]\n\nI need the full names of the authors. They are stored for each publication in the AF field. This is the only information I need for the COI form. I pull it out, then do a bit of processing to get it in the right shape, then remove duplicates and sort.\n\nallauthors = paste0(pubs_new$AF,collapse = \";\") #merge all authors into one vector\nallauthors2 = unlist(strsplit(allauthors, split =\";\"))\nauthors = sort(unique(allauthors2)) #split vector of authors, get unique authors\n\nNote that I originally did the above steps using biblioAnalysis(pubs_new). However, this function/approach broke in a recent version of the package, and I realized that I can just use a few base R commands to get what I need, which is the approach shown above. If you use the biblioAnalysis() function, the Authors are in the Authors field of the returned object.\n\n\nGetting a table of co-authors\nHere is the full table of my co-authors in the specified time period. I made a tibble that looks similar to what the COI document requires.\n\n#removing the 1st one since that's me\nauthortable = dplyr::tibble(Name = authors, \n                            \"Co-Author\" = 'x', \n                            Collaborator = '', \n                            'Advisees/Advisors' = '', \n                            'Other – Specify Nature' = '')\n\nFinally, I’m using the flextable package to make a decent looking table and save it to a word document.\n\nft <- flextable::flextable(authortable)\nflextable::autofit(ft)\n\n\n\nNameCo-AuthorCollaboratorAdvisees/AdvisorsOther – Specify NatureAHMED, HASANxALIKHAN, MALIHA AxAMANNA, IAN JxANTIA, ALICExANTIA, RUSTOMxBOOM, W HENRYxBULUSHEVA, IRINAxCARLSON, NICHOLE ExCASTELLANOS, M ExCASTELLANOS, MARIAxCHAKRABURTY, SRIJITAxCHEN, ENFUxCHENG, WEIxCOATES, P TOBYxCROFT, NATHAN PxDALE, ARIELLA PERRYxDENHOLM, J TxDOBBIN, KEVINxDUDEK, NADINE LxEBELL, MARKxEBELL, MARK HxEGGENHUIZEN, PETER JxFOREHAND, RONALDxFUGGER, LARSxGAN, POH YxGARCIA-SASTRE, ADOLFOxGREGERSEN, JON WxGUAN, JINGxHALLORAN, M ELIZABETHxHANDEL, AxHANDEL, ANDREASxHECKMAN, TIMOTHY GxHOLDSWORTH, STEPHEN RxHOLT, STEPHEN GxHOUBEN, R M G JxHUANG, HAODIxHUDSON, BILLY GxHUO, XIANGxHUYNH, MEGANxJOLOBA, MOSES LxKAKAIRE, RxKIRIMUNDA, SxKITCHING, A RICHARDxKIWANUKA, NxLA GRUTA, NICOLE LxLI, CHANGWEIxLI, CHAOxLI, YANxLING, FENGxLOH, KHAI LxLONGINI, IRA MxMALONE, LASHAUNDA LxMANICASSAMY, BALAJIxMARTINEZ, LxMARTINEZ, LEONARDOxMCBRYDE, E SxMCKAY, BRIANxMOORE, JAMES RxMU, LANxOOI, JOSHUA DxPAWELEK, KASIA AxPETERSEN, JANxPOWER, DAVID AxPURCELL, ANTHONY WxQUACH, TxQUINN, FREDERICK DxRAGONNET, RxRAMARATHINAM, SRI HxREID, HUGH HxROSSJOHN, JAMIExSETTE, ALESSANDROxSHEN, YExSIDNEY, JOHNxSLIFKA, MARKxSNG, XAVIER Y XxSTEIN, CATHERINE MxSUMNER, TxTAN, YU HxTHOMAS, PAUL GxTRAUER, J MxTSCHARKE, DAVID CxWAKIM, LINDA MxWANG, XIAOXIAOxWATSON, KATHERINE AxWHALEN, C CxWHALEN, CHRISTOPHER CxWILLETT, ZOE JxWOLDU, HxWOLDU, HENOKxWU, TINGxZALWANGO, SxZALWANGO, SARAHxZARNITSYNA, VERONIKAxZARNITSYNA, VERONIKA IxZHU, LIMEIx\n\nflextable::save_as_docx(\"my table\" = ft, path = \"COItable.docx\")\n\nI notice a few duplicates in the table that need to be removed. Of course I also need to remove myself. And for some, the full name doesn’t show. I need to fill in a few of the other columns and potentially add a few individuals who were not captured. So it’s not fully automated, but I can copy this table into the COI statement and the remaining edits are still annoying but not that terrible.\n\n\nDiscussion\nThese kinds of COI documents that ask for all co-authors are in my opinion antiquated and should go away. In the meantime using a somewhat automated approach makes the problem not too bad. I will have to make a few manual adjustments to the table, but overall it’s not too bad. I’m still glad that NIH does not require this.\n\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Generating a Conflict of Interest Form Automatically},\n  date = {2020-05-29},\n  url = {https://www.andreashandel.com/posts/2020-05-29-automate-conflict-of-interest-form},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Generating a Conflict of Interest Form\nAutomatically.” May 29, 2020. https://www.andreashandel.com/posts/2020-05-29-automate-conflict-of-interest-form."
  },
  {
    "objectID": "posts/2020-10-07-custom-word-format/index.html",
    "href": "posts/2020-10-07-custom-word-format/index.html",
    "title": "Custom Word formatting using R Markdown",
    "section": "",
    "text": "Word template preparation\n\nCreate a new word document (either through RMarkdown -> Word, or just open Word and create a new empty one).\nOpen the word document. Write a bit of text. It doesn’t matter what you write, it’s just meant so you can create and apply new styles to it. For instance you can write Text for mystyle1.\nMark the text you wrote, click on the arrow to the left of the Styles box (see the red “circle” in the figure) and choose Create a style. Depending on your version of Word, this might be somewhere else. Create the formatting the way you want.\nRepeat to create as many custom styles as you want, save the word document into the folder of your RMarkdown file.\n\n\n\n\n\n\n\n\n\n\n\n\nRMarkdown setup\nStart a new Rmarkdown document. Your YAML header should look something like this:\n---\ntitle: \"An example of formatting text blocks in Word\"\nauthor: \"Andreas Handel\"\ndocumentclass: article\nsite: bookdown::bookdown_site\noutput:\n  bookdown::word_document2: \n    toc: false\n    reference_docx: wordstyletemplate.docx\n---\nNote that I’m using bookdown as the output format here, but any others that can produce word output, e.g. the standard R Markdown format, should work equally well. The important part is the last line, which specifies the word document with the custom styles you created in the previous step.\n\n\nRMarkdown input content\nYou can now assign text blocks in your R markdown file specific styles. Here I created 3 styles called mystyle1/mystyle2/mystyle3 in the Word doc, and assign them to specific parts of the text. This example markdown text shows how to use the styles.\n# A regular section\n\nThis text is not getting a special format.\n\n# A formatted Section\n\n:::{custom-style=\"mystyle1\"}\nThis is formatted according to the _mystyle1_ format.\n:::\n\n# Another formatted block of text\n\nSome more regular text.\n\n:::{custom-style=\"mystyle2\"}\nNow text formatted based on _mystyle2_.\n:::\n\nMore regular text.\n\n:::{custom-style=\"mystyle3\"}\nThis format includes a border and it also works with an equation.\n$$Y = bX + c$$\n:::\n\nRegular text again.\n\n::: {custom-style=\"mystyle1\"}\n# With a header\n\nNote that the header formatting is overwritten.\n:::\n\n\nWord output\nThe resulting word document looks like this:\n\n\n\n\n\n\n\n\n\n\n\nSome notes\nOne thing you see in this example is that your own styles overwrite all others, so headers inside your custom style will just be formatted like your custom style. Some other quirks I noticed is that you seem to need empty lines before and after your custom style block. I seem to remember that formatting of R chunks works ok, but I also seem to recall that sometimes manual intervention is required.\nOverall, this approach gives you a good deal of flexibility for applying styling to your Word documents when writing in R Markdown, but there might still be some kinks. As I mentioned in the beginning, I ended up not using it for the project I had intended to use it (a review paper I wrote), so I don’t have a lot of real world experience beyond what I’m describing here.\n\n\nFurther Resources\nYou can get the Word template and the example R Markdown file I used to illustrate this here and here.\nI recently saw that the new R Markdown cookbook has a section describing word styling. I expect that more content will be added to the cookbook, so it’s worth checking regularly.\n\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Custom {Word} Formatting Using {R} {Markdown}},\n  date = {2020-10-07},\n  url = {https://www.andreashandel.com/posts/2020-10-07-custom-word-format},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Custom Word Formatting Using R\nMarkdown.” October 7, 2020. https://www.andreashandel.com/posts/2020-10-07-custom-word-format."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "",
    "text": "Generating personalized documents is often useful. Since this is a very common task, programs like Word or similar software can do this. But I like to use R if I can. And the whole R Markdown system is perfectly suited for repeat generation of customized documents.\nI’m certainly not the first one to have the idea of using R, and in fact my initial approach is based on this prior post. Here, I describe a few ways of using R and R Markdown to auto-generate custom documents, and provide example code and explanations for anyone who might want to use this (including my future self)."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#latexpdf",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#latexpdf",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "LaTeX/PDF",
    "text": "LaTeX/PDF\nFor this approach, you start with a template file that contains some LaTeX commands, including the placeholders that will get personalized. Here is code for an example file, you can get the file here. We’ll look at the resulting output below.\n\n\n---\ntitle: \"\"\noutput: pdf_document\nclassoption: landscape\n---\n\n\\begin{center}\n\\includegraphics[height=4cm]{fig1.png}\n\\hfill\n\\includegraphics[height=4cm]{fig2.jpg} \\\\\n\\bigskip\n{\\Huge\\bf Certificate of Accomplishment } \\\\\n\\bigskip\n{\\Huge <<FIRSTNAME>> <<LASTNAME>> } \\\\\n\\bigskip\n{\\Large has successfully completed the course {\\it Generating Certificates with R} with a score of <<SCORE>> } \\\\\n\\bigskip\n{\\Huge Congratulations!}\n\\end{center}\n\n\nThis template places 2 images at the top, writes some text, and most importantly, adds some placeholder text that will be customized for each student with the script shown below. It doesn’t matter what placeholder text you write, as long as it’s unique such that when you do the replacement, only the instance you want replaced is actually changed. Enclosing with special characters such as << >> is a good option for this, but it’s not required.\nThe advantages of the LaTeX/PDF approach are that 1) LaTeX allows you to do a lot of formatting and customization of the template so it looks exactly the way you want it, 2) the end product is a PDF file, which is easy to print or share with those for whom they are meant. The disadvantage is that you need to know a bit of LaTeX to set up your template, or at least be willing to spend some time with Google until you found all the snippets of commands you need for the layout you want to have."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#word",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#word",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "Word",
    "text": "Word\nFor this approach, you start with a template file that contains commands that lead to a decent looking Word document. Again, it needs to include the placeholders that will get personalized. Here is code for an example file, you can get the file here. We’ll look at the end result below.\n\n\n---\ntitle: \"\"\noutput: \n  word_document:\n    reference_docx: wordstyletemplate.docx\n---\n\n![Image](fig1.png)\n\n# Certificate of Accomplishment\n\n:::{custom-style=\"mystyle1\"}\n<<FIRSTNAME>>  <<LASTNAME>>\n:::\n\nhas successfully completed the course \"Generating Certificates with R\" with a score of <<SCORE>>\n\n:::{custom-style=\"mystyle2\"}\nCongratulations!\n:::\n\n```{r fig2, echo=FALSE, out.width=\"50%\"}\nknitr::include_graphics(\"fig2.jpg\")\n```\n\n\nNote that by default, going from R Markdown to Word doesn’t give you much ability to apply formatting. However, it is possible to do a decent amount of formatting using a word style template. I have another blog post which describes this approach, and I’m using it here.\nEven with the word style formatting, some things can’t be controlled well. Placement and sizing of figures is the main problem, no matter if you include the figures with basic Markdown commands or use the include_graphics() function. You’ll see the problem if you try to run this example (code below). As such, for something that includes figures, using the LaTeX/PDF workflow seems almost always better. A scenario where the Word setup might be useful is if you want to produce customized letters. The one main advantage of a Word output (in addition to not having to figuring out LaTeX commands) is that the output can be further edited if needed."
  },
  {
    "objectID": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#other-options",
    "href": "posts/2020-10-15-customized-documents-with-Rmarkdown/index.html#other-options",
    "title": "Automatically generate personalized certificates and other documents with R",
    "section": "Other options",
    "text": "Other options\nI believe the PDF or Word outputs are best for most instances, but occasionally another format might be needed. You can use this overall approach to generate other outputs, for instance the standard R Markdown html output, or different versions of presentation slides (e.g. ioslides, Beamer, Powerpoint), etc. In principle, any R Markdown output format should work. You just need to alter your template file accordingly."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html",
    "href": "posts/2021-01-11-simple-github-website/index.html",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "",
    "text": "Update 2022-09-25: Hosting a website on GitHub is still a good idea. With the new Quarto framework available, I recommend using that setup. This page provides good instructions on how to start a Quarto website, and this page explains how to publish it on GitHub. Maybe I’ll get around to updating this post for Quarto soon. In the meantime, here is my short Hugo to Quarto transition guide.\nThe following blog post provides step-by-step instructions for creating a simple (and free) website using (R)Markdown and Github."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html#install-r-and-rstudio",
    "href": "posts/2021-01-11-simple-github-website/index.html#install-r-and-rstudio",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nIf you don’t already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you update (install a new R version).\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it’s a recent version. If you have an older verion of RStudio, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach)."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html#get-github-up-and-running",
    "href": "posts/2021-01-11-simple-github-website/index.html#get-github-up-and-running",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "Get Github up and running",
    "text": "Get Github up and running\nIf you are new to Github, you need to create an account. At some point, it would also be useful to learn more about what Git/Github is and how to use it, but for this purpose you actually don’t need to know much. If you want to read a bit about Git/Github, see e.g. this document, which I wrote for one of my courses.. But for now, you don’t need to know much about Git/Github."
  },
  {
    "objectID": "posts/2021-01-11-simple-github-website/index.html#install-gitkraken-optional-but-assumed",
    "href": "posts/2021-01-11-simple-github-website/index.html#install-gitkraken-optional-but-assumed",
    "title": "Create a simple Markdown/GitHub website in less than 30 minutes",
    "section": "Install Gitkraken (optional but assumed)",
    "text": "Install Gitkraken (optional but assumed)\nThere are many ways you can interact with Git/Github. I mosty use the fairly user-friendly and full-featured Gitkraken. You can get a basic version for free, if you are a student, you can get the Pro version through the Github developer pack, teachers can get it through the Github teacher toolbox. If you qualify for either, I highly recommend signing up. But you don’t need it for our purpose.\nI assume for the rest of the post that you are using Gitkraken. If you have your own preferred Git/Github client (e.g. the one that comes with RStudio), you can of course use that one too. Make sure you connect Gitkraken to your Github account."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html",
    "href": "posts/2021-03-21-simple-distill-website/index.html",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "",
    "text": "Update 2022-09-25: Distill is in my opinion at this point outdated and replaced by Quarto. Going forward, I recommend using the Quarto framework.\nThe following blog post provides step-by-step instructions for creating a website using R Markdown, the distill R package and GitHub."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#install-r-and-rstudio",
    "href": "posts/2021-03-21-simple-distill-website/index.html#install-r-and-rstudio",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Install R and RStudio",
    "text": "Install R and RStudio\nIf you don’t already have it on your computer, install R first. You can pick any mirror you like. If you already have R installed, make sure it is a fairly recent version. If yours is old, I suggest you install a new R version.\nOnce you have R installed, install the free version of RStudio Desktop. Again, make sure it’s a recent version. If you have an older version, you should update.\nInstalling R and RStudio should be fairly straightforward. If you want some more details or need instructions, see this page (which is part of an online course I teach)."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#get-github-up-and-running",
    "href": "posts/2021-03-21-simple-distill-website/index.html#get-github-up-and-running",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Get GitHub up and running",
    "text": "Get GitHub up and running\nIf you are new to GitHub, you need to create an account. At some point, it would also be useful to learn more about what Git/GitHub is and how to use it, but for this purpose you actually don’t need to know much. If you want to read a bit about Git/GitHub, see e.g. this document, which I wrote for one of my courses.."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#install-gitkraken-optional-but-assumed",
    "href": "posts/2021-03-21-simple-distill-website/index.html#install-gitkraken-optional-but-assumed",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Install Gitkraken (optional but assumed)",
    "text": "Install Gitkraken (optional but assumed)\nThere are many ways you can interact with Git/GitHub. I mostly use the fairly user-friendly and full-featured Gitkraken. You can get a basic version for free. If you are a student, you can get the Pro version through the Github developer pack, teachers can get it through the Github teacher toolbox. If you qualify for either, I highly recommend signing up. But you don’t need it for our purpose.\nOnce you have your GitHub account set up and Gitkraken installed, make sure you connect Gitkraken to your Github account.\nI assume for the rest of the post that you are using Gitkraken. If you have your own preferred Git/GitHub client (e.g. the one that comes with RStudio), you can of course use that one too."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#gitkraken",
    "href": "posts/2021-03-21-simple-distill-website/index.html#gitkraken",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Gitkraken",
    "text": "Gitkraken\n\nOpen GitKraken, go to File -> Init Repo -> Local Only. Give it the name of your main website directory, e.g. mywebsite. The Initialize In folder should be the folder above where you created the website, such that the Full path entry is the actual location of your website on your computer. For .gitignore Template you can choose R. The rest you can leave as is.\n\n\n\n\n\n\n\n\n\n\nOnce done, click Create repository. You should see a bunch of files ready for staging on the left. Click Stage all changes enter a commit message, commit. Then Click the Push button.\nAt this point, if you didn’t properly connect GitKraken and GitHub previously, you’ll likely get an error message. Follow the error message and the connect Gitkraken to your Github account information to get it to work.\nYou’ll see a message about no remote existing and if you want to add one. Say yes. A menu on the left should show up. Make sure the repository name is the same as your website folder name. Then click the green button. If things worked, your local website folder has been sent to GitHub and is ready to be turned into a website."
  },
  {
    "objectID": "posts/2021-03-21-simple-distill-website/index.html#github-website",
    "href": "posts/2021-03-21-simple-distill-website/index.html#github-website",
    "title": "Create a GitHub website with distill in less than 30 minutes",
    "section": "Github website",
    "text": "Github website\nFor the last step, go to your account on Github.com and find the repository for the website you just created. On the bar at the top, in the right corner there should be the Settings button. Click on it. Scroll down until you find the GitHub Pages section. Under Source, select Main and then choose /docs as the folder. Don’t choose a theme since we are using our own. Save those changes. If everything works (it could take a minute or so), your website is now live and public! Look right underneath the GitHub Pages section, there should be something like Your site is ready to be published at https://andreashandel.github.io/mywebsite/. Click on the link and your new site should show up.\nThat’s it. Now the hard part starts, creating good content. 😄"
  },
  {
    "objectID": "posts/2021-09-30-tidy-tuesday-exploration/index.html",
    "href": "posts/2021-09-30-tidy-tuesday-exploration/index.html",
    "title": "Analysis of economic papers",
    "section": "",
    "text": "This analysis was performed as part of an exercise for my Modern Applied Data Analysis course taught in fall 2021.\nOne of the weekly assignments for the students is to participate in Tidy Tuesday. I did the exercise as well, this is my product. You can get the R Markdown/Quarto file to re-run the analysis here.\n\nIntroduction\nIf you are not familiar with Tidy Tuesday, you can take a quick look at the TidyTuesday section on this page.\nThis week’s data was about an analysis of economic working papers catalogued by NBER. More on the data is here.\n\n\nLoading packages\nMake sure they are installed. Note: I don’t like loading meta-packages, such as the tidyverse. Doing so makes it really hard to figure our which packages are actually used. So I prefer to only load what I need.\n\nlibrary('ggplot2')\nlibrary('readr')\nlibrary('dplyr')\nlibrary('stringr')\nlibrary('tidytuesdayR')\n\n\n\nData loading\nApparently there is now a tidytuesdayR package that makes data loading very easy!\n\n#tuesdata &lt;- tidytuesdayR::tt_load('2021-09-28')\n\nWell, that command above failed, claiming that date didn’t exist. So loading the data manually after all.\n\npapers &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/papers.csv')\n\nRows: 29434 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): paper, title\ndbl (2): year, month\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nauthors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/authors.csv')\n\nRows: 15437 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): author, name, user_nber, user_repec\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nprograms &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/programs.csv')\n\nRows: 21 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): program, program_desc, program_category\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npaper_authors &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_authors.csv')\n\nRows: 67090 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): paper, author\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\npaper_programs &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2021/2021-09-28/paper_programs.csv')\n\nRows: 53996 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): paper, program\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nUnderstanding the data\nThe explanations on the Tidy Tuesday website were a bit confusing. Looks like each CSV file only has a few variables, and papers.csv does not contain all the variables listed in the readme.\n\ncolnames(papers)\n\n[1] \"paper\" \"year\"  \"month\" \"title\"\n\ncolnames(authors)\n\n[1] \"author\"     \"name\"       \"user_nber\"  \"user_repec\"\n\ncolnames(programs)\n\n[1] \"program\"          \"program_desc\"     \"program_category\"\n\ncolnames(paper_authors)\n\n[1] \"paper\"  \"author\"\n\ncolnames(paper_programs)\n\n[1] \"paper\"   \"program\"\n\n\nPapers seem to be linked to authors by the paper_authors file and to programs (areas of work) by the paper_programs file. Probably best to combine all into one data frame. The Tidy Tuesday website has some code for that already, let’s see if it works\n\ndf &lt;- left_join(papers, paper_authors) %&gt;% \n  left_join(authors) %&gt;% \n  left_join(paper_programs) %&gt;% \n  left_join(programs)%&gt;% \n  mutate(\n    catalogue_group = str_sub(paper, 1, 1),\n    catalogue_group = case_when(\n      catalogue_group == \"h\" ~ \"Historical\",\n      catalogue_group == \"t\" ~ \"Technical\",\n      catalogue_group == \"w\" ~ \"General\"\n    ),\n    .after = paper\n  ) \n\nJoining with `by = join_by(paper)`\n\n\nWarning in left_join(papers, paper_authors): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 8 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n\nJoining with `by = join_by(author)`\nJoining with `by = join_by(paper)`\n\n\nWarning in left_join(., paper_programs): Each row in `x` is expected to match at most 1 row in `y`.\nℹ Row 111 of `x` matches multiple rows.\nℹ If multiple matches are expected, set `multiple = \"all\"` to silence this\n  warning.\n\n\nJoining with `by = join_by(program)`\n\nglimpse(df)\n\nRows: 130,081\nColumns: 12\n$ paper            &lt;chr&gt; \"w0001\", \"w0002\", \"w0003\", \"w0004\", \"w0005\", \"w0006\",…\n$ catalogue_group  &lt;chr&gt; \"General\", \"General\", \"General\", \"General\", \"General\"…\n$ year             &lt;dbl&gt; 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973, 1973,…\n$ month            &lt;dbl&gt; 6, 6, 6, 7, 7, 7, 8, 9, 9, 9, 9, 9, 10, 10, 10, 10, 1…\n$ title            &lt;chr&gt; \"Education, Information, and Efficiency\", \"Hospital U…\n$ author           &lt;chr&gt; \"w0001.1\", \"w0002.1\", \"w0003.1\", \"w0004.1\", \"w0005.1\"…\n$ name             &lt;chr&gt; \"Finis Welch\", \"Barry R Chiswick\", \"Swarnjit S Arora\"…\n$ user_nber        &lt;chr&gt; \"finis_welch\", \"barry_chiswick\", \"swarnjit_arora\", NA…\n$ user_repec       &lt;chr&gt; NA, \"pch425\", NA, \"pli669\", \"psm28\", NA, NA, NA, \"pli…\n$ program          &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ program_desc     &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ program_category &lt;chr&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nThat seems to have worked, now all data is in one data frame.\n\n\nExploration 1\nMy first idea is very idiosyncratic. A friend of mine is an Econ/Finance professor at Bocconi University. I’m going to see if he has any papers in that dataset.\n\nsmalldf &lt;- df %&gt;% filter(grepl(\"Wagner\",name))\nprint(unique(smalldf$name))\n\n [1] \"Joachim Wagner\"       \"Alexander F Wagner\"   \"Todd H Wagner\"       \n [4] \"Stefan Wagner\"        \"Mathis Wagner\"        \"Ulrich J Wagner\"     \n [7] \"Wolf Wagner\"          \"Gernot Wagner\"        \"Zachary Wagner\"      \n[10] \"Katherine R H Wagner\" \"Myles Wagner\"         \"Kathryn L Wagner\"    \n\n\nNot seeing him in there. Just to make sure, a check on first name.\n\nsmalldf &lt;- df %&gt;% filter(grepl(\"Hannes\",name))\nprint(unique(smalldf$name))\n\n[1] \"Hannes Schwandt\"\n\n\nOk, nothing. Might be that his area, finance, is not indexed by NBER. I don’t know enough about the econ/business/finance fields enough to know what is and isn’t part of NBER. So I guess moving on.\n\n\nExploration 2\nIn most areas of science and when looking at publication records, one finds that most people publish very little (e.g. a student who is a co-author on a paper, then goes into the “real world” and never publishes again) and a few people publish a lot (super-star and/or old faculty). One usually sees an 80/20 pattern or a distribution that follows a power law. Let’s see what we find here.\nFirst, I’m doing a few more checks.\n\n#looking at missing data for each variable\nnas &lt;- colSums(is.na(df))\nprint(nas)\n\n           paper  catalogue_group             year            month \n               0                0                0                0 \n           title           author             name        user_nber \n               0                0                0             2112 \n      user_repec          program     program_desc program_category \n           47158              530              530             1516 \n\n\nMissing data pattern seems ok. To be expected that some users don’t have those NBER or REPEC IDs.\nLet’s look at number of unique papers and unique authors.\n\nn_authors = length(unique(df$author))\nn_papers = length(unique(df$title))\nprint(n_authors)\n\n[1] 15437\n\nprint(n_papers)\n\n[1] 29419\n\n\nComparing those numbers to the original data frames, we see that the number of authors is same as in original authors data frame, that’s good. Number of papers (or at least unique titles) is less. Seems like some papers have the same titles? Non-unique titles is confirmed by checking the ID for each paper, which is the same as the one in the original papers CSV file.\nLet’s look at those titles that show up more than once. Note that we need to do that with the original papers data frame, since the merged one contains many duplicates since each author gets their own row.\n\n#using base R here, can of course also do that with tidyverse syntax\ndfdup &lt;- papers[duplicated(papers$title),]\nprint(dfdup$title)\n\n [1] \"The Wealth of Cohorts: Retirement Saving and the Changing Assets of Older Americans\"        \n [2] \"Empirical Patterns of Firm Growth and R&D Investment: A Quality Ladder Model Interpretation\"\n [3] \"The Market for Catastrophe Risk: A Clinical Examination\"                                    \n [4] \"Taxation and Corporate Financial Policy\"                                                    \n [5] \"Asset Pricing with Heterogeneous Consumers and Limited Participation: Empirical Evidence\"   \n [6] \"Tax Incidence\"                                                                              \n [7] \"Liquidity Shortages and Banking Crises\"                                                     \n [8] \"Legal Institutions and Financial Development\"                                               \n [9] \"Inequality\"                                                                                 \n[10] \"Predictive Systems: Living with Imperfect Predictors\"                                       \n[11] \"Corruption\"                                                                                 \n[12] \"The Dynamic Properties of Financial-Market Equilibrium with Trading Fees\"                   \n[13] \"Forward Guidance\"                                                                           \n[14] \"Labor Market Integration Before the Civil War\"                                              \n[15] \"The Impact of Globalization on Pre-Industrial, Technologically Quiescent Economies\"         \n\n\nSome titles I can easily seen being used more than once, e.g. a paper called “Corruption”. Others sound very unique, so not sure why they show up as duplicates. If this were a serious analysis, I would look more closely into that. But for this exercise, and since it’s just a few titles, I’ll ignore and move on.\nI want to look at publications per author. Since names might not be unique but NBER ID should be, I’m just going to remove those authors that don’t have an NBER ID (likely most of them have co-authored very few papers) and focus on the remaining authors. For each, I’ll count their total papers by counting how often they show up.\n\ndfnew &lt;- df %&gt;% filter(!is.na(user_nber)) %&gt;% \n                group_by(user_nber) %&gt;% \n                summarise(n_papers = n() ) %&gt;%\n                arrange(desc(n_papers)) %&gt;%\n                mutate(allpapers = cumsum(n_papers)) \ndfnew$id = 1:nrow(dfnew) #add an ID variable for plotting\n\nLooking at the histogram of number of papers for each author.\n\np1 &lt;- dfnew %&gt;% ggplot(aes(n_papers)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nLooks like expected, most authors have published only a few papers, a few have a lot.\nAnother way to look at this is with a violin plot\n\np1a &lt;- dfnew %&gt;% ggplot(aes(x=1, y=n_papers)) + geom_violin() \nplot(p1a)\n\n\n\n\nThat’s a very flat violin plot, with almost all the density close to 1.\nDoing a quick numerical summary\n\nsummary(dfnew$n_papers)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   3.000   8.983   7.000 359.000 \n\n\nOf course one needs to have at least 1 paper published to be in there, so that’s the minimum. The median is 3, so half of individuals have published 3 or less. The mean is higher, as expected for skewed data, and the max is at 359 papers, confirming the histogram which shows a few individuals wrote a lot of papers.\nJust because, let’s look at the 10 most published authors\n\nhead(dfnew,n=10)\n\n# A tibble: 10 × 4\n   user_nber        n_papers allpapers    id\n   &lt;chr&gt;               &lt;int&gt;     &lt;int&gt; &lt;int&gt;\n 1 jonathan_gruber       359       359     1\n 2 james_heckman         331       690     2\n 3 daron_acemoglu        308       998     3\n 4 janet_currie          306      1304     4\n 5 michael_bordo         297      1601     5\n 6 edward_glaeser        291      1892     6\n 7 joshua_aizenman       284      2176     7\n 8 martin_feldstein      272      2448     8\n 9 andrei_shleifer       242      2690     9\n10 alan_taylor           239      2929    10\n\n\nSince this is not my field, I don’t know any of those individuals. But looks like the 1st one, Gruber, is somewhat famous and also worked at NBER, so maybe not surprising that his papers are in there. Not sure, I don’t know how exactly NBER works, but it’s a quick consistency check and no red flag.\nThis looks at the accumulation of papers for the first N authors, with number of authors on the x-axis and total papers on the y axis. If every author were to contribute the same number of papers, we’d see a straight line up the diagonal. The fact that some authors write more papers, and most just a few, pushes the curve to the upper left corner. I’m also plotting a few lines that show the 80/20 idea, i.e. the vertical line indicates 20% of authors, the horizontal indicates 80% of all published papers.\n\nnobs = nrow(dfnew)\ntotpapers = max(dfnew$allpapers)\np2 &lt;- dfnew %&gt;% ggplot(aes(x=id, y=allpapers)) + \n                geom_point() + \n                theme(legend.position=\"none\") + \n               geom_segment(aes(x = floor(nobs*0.2), y = 1, xend = floor(nobs*0.2), yend = floor(totpapers*0.8) ), colour = \"red\", linetype=\"dashed\", size=1) +\n geom_segment(aes(x = 1, y = floor(totpapers*0.8), xend = floor(nobs*0.2), yend = floor(totpapers*0.8) ), colour = \"red\", linetype=\"dashed\", size=1) \n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplot(p2)\n\n\n\n\nLooks like the NBER papers are fairly close to that 80/20 distribution, with few authors contributing the bulk, and most authors contributing little.\nNote that this does not account for co-authorship, just doing a per-author count.\n\n\nFurther explorations\nI’m going to leave it at this for now. In contrast to my 2019 Tidy Tuesday exploration I won’t try a fake statistical analysis here.\nBut I can think of a few other ideas and things to explore Here are a few:\n\nThe Tidy Tuesday website had a link to this article which looks at gender representation among the papers/authors. We could do that here too, e.g. follow their approach to try and guess gender for authors, then could stratify number of papers by gender of author.\nAnother possible exploration would be to look at the numbers of papers per author based on the area of research, i.e. the programs variable.\nYet another analysis one could do is to look at the pattern of publication for those that publish a good bit (say over 50 papers) and see how numbers of papers per year changes, or how number of co-authors changes over the years.\n\n\n\nSummary\nPatterns of authorship have been explored often. Sometimes they lead to useful information. At other times, one just needs more or less meaningful numbers for career advancement purposes. See e.g. an analysis of my own papers I did and wrote up in this post covering Google scholar data and this post using the bibliometrix package.\nMy exploration here was not too thorough, but some expected patterns showed up, namely the 80/20 skewed distribution in authorship.\n\n\n\n\nCitationBibTeX citation:@online{handel2021,\n  author = {Handel, Andreas},\n  title = {Analysis of Economic Papers},\n  date = {2021-09-30},\n  url = {https://www.andreashandel.com/posts/2021-09-30-tidy-tuesday-exploration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHandel, Andreas. 2021. “Analysis of Economic Papers.”\nSeptember 30, 2021. https://www.andreashandel.com/posts/2021-09-30-tidy-tuesday-exploration."
  },
  {
    "objectID": "posts/2022-06-11-flowdiagramr-exploration/index.html",
    "href": "posts/2022-06-11-flowdiagramr-exploration/index.html",
    "title": "Exploring the flowdiagramr R package",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2022,\n  author = {Handel, Andreas},\n  title = {Exploring the `Flowdiagramr` {R} Package},\n  date = {2022-06-11},\n  url = {www.cillianmchugh.com/posts/2022-06-11-flowdiagramr-exploration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHandel, Andreas. 2022. “Exploring the `Flowdiagramr` R\nPackage.” June 11, 2022. www.cillianmchugh.com/posts/2022-06-11-flowdiagramr-exploration."
  },
  {
    "objectID": "posts/2022-10-01-hugo-to-quarto-migration/index.html",
    "href": "posts/2022-10-01-hugo-to-quarto-migration/index.html",
    "title": "Migration from Hugo/blogdown/Wowchemy to Quarto",
    "section": "",
    "text": "Who this is (not) for\nThis most directly targets folks who want to switch from blogdown/Hugo to Quarto. I’m hosting things on Netlify, but it should work for other hosting platforms too. Some of the tips might also be useful for folks who plan to build a Quarto website from scratch.\nThis is not a detailed walk-through. For that, see the Quarto documentation or for instance this blog post. I’ll describe a lot of steps only briefly, and make comments on some topics that might be not yet commonly known.\n\n\nSetting up the website\nI started by creating a new Quarto website. Either the Quarto documentation or these blog posts by Danielle Navarro and Albert Rapp worked well.\nFor the main page, I simply used the about page template that is built into Quarto. (While my about.qmd page is just a regular page.)\nI structured the new website to be as similar to my old as possible. For me, that meant folders and subfolders for posts, presentations and projects, and all other files (e.g. about.qmd) in the top level.\nNote that while I have posts and presentations separately, and under the old setup those pages were somewhat different, with Quarto there is (currently) no separate styling for presentations, thus it is basically another collection of posts. Only in this case, each post just contains the basic information of the presentation and a link to the slides and other relevant material. The same is true for the projects, they follow again the same structure as the posts, just somewhat differently formatted.\nIt would be possible to tune more, and make the presentations entries display differently. But I wanted to keep it simple, I learned that too much customization is just a time suck for me 😁.\nFor the projects, I made a change. They are also set up like posts, but I wanted the image that’s shown as thumbnail to also show on each page explaining a project. That was the way it was on my old site. I was thinking of messing with the project pages, but realized I could just use one of the ‘about’ page layout templates and that would do the trick. So I just added the solona template into the YAML of each file and got a layout that looked good enough for what I wanted.\n\n\nConfiguring the site\nThe main file for setting configurations is the _quarto.yml file. Additional configurations can be done in _metadata.yml inside the posts and presentations folders. I followed a mix of the Quarto documentation and the blog posts mentioned above and below to configure those files. You can check out my setup on the GitHub repo of this page.\n\n\nConverting YAML sections\nI had to do a bunch of manual conversions of all the index.qmd files that go with each post and presentation. It was a bit tedious but not too bad. Basically, I had to remove anything from the YAML that was specific for Hugo/Wowchemy and format it to have the fields supported by Quarto. I found Danielle’s post to be helpful for a quick orientation. (Note that she converts from Distill, so her starting point is slightly different, but the new Quarto entries are the same.)\n\n\nSetting up a 404/not found page\nThis is not really required, but I liked this approach from Lucy D’Agostino McGowan and decided to mostly copy it and adjust a bit for my purposes.\n\n\nTurning on comments\nI used utterances before to allow folks to comment on certain sites. This can be done easily as explained in this Quarto documentation section. To prevent comments on certain pages, one can turn those off in the YAML, also also described on the Quarto documentation page. An alternative is to place the utterances information into the _metadata.yml files, which then means comments are only on for those specific files/folders, in my case the blog posts and the presentations.\n\n\nHTML form embedding\nUpdate 2023-01-09: At the end of 2022, I decided to stop doing my newsletter for the time being and turned off the newsletter subscription section. Leaving this here since it might still be of general interest to figure out how to embed HTML.\nFor my newsletter subscription page, I embed HTML code produced by Mailchimp. It took me a little bit to figure out how to do it. This tip explained it, basically the setup looks like this:\n```{=html}\nALL THE HTML STUFF FROM MAILCHIMP GOES HERE\n```\n\n\nRedirects 1\nMy previous website used slug: labels in the YAML to provide shortened URLs. I wanted to keep those old URLs so breaking links are minimized. First I was thinking of following this example by Danielle Navarro and adjusting the code such that it parses the YAML of all my index.qmd files, pulls out the slug part and builds the paths and redirect table. It should have worked for me since I’m also hosting this website on Netlify, like Danielle.\nBut then I found a simpler option, using the aliases: setting, here is a an explanation/example. I tried to do an automated search and replace, but it didn’t quite work, so I ended up doing it manually. I basically replaced this:\nslug: publications-analysis-1\nwith this:\naliases: \n  - ../publications-analysis-1/\nin the YAML of all my posts (my presentations didn’t use slugs).\nNote on this: I ran into some problems initially. Since I was mapping into the same file structure, just a different file name, I wrote this:\naliases: \n  - publications-analysis-1\nThat didn’t work. On contemplation, the alias is referenced to the current document. So if my file is https://www.andreashandel.com/posts/2020-02-01-publications-analysis-1/ I need to move up a folder and place the alias folder/file publications-analysis-1 there. That was confusing on first try. I only figured it out once I looked into the _site folder to see what Quarto was producing, and from that deduced the right setup.\n\n\nRedirects 2\nAfter I got the aliases bit to work, I realized that I needd further redirects. On my old blog, I had andreashandel.com/talk redirect to andreashandel.com/presentations. The first URL is on many of my presentation slides and I didn’t want to change them all. So I figured I should use Danielle’s approach after all, and make a small _redirects file that contained these. Basically it looks like this:\n/talk /presentations\n/talks /presentations\n/post /posts\nI followed her instructions of placing this bit of code into the main index.qmd file.\n\n\nWidgets\nOn my previous website, I had several sections (widgets) on the main page under the static welcome text. Those showed my Twitter feed and most recent posts and presentations. I was trying to see if I can reproduce that with Quarto. Based on this example it seems one could do something like that. I contemplated giving it a try. But then I decided to keep it simple, and let interested readers klick on my Twitter/Posts/Presentations sections if they want. No need to complicate things 😁.\n\n\nCustom footer\nI had a footer with copyright text that I wanted to keep. Albert Rapp’s post has an example of using an html file. I had that setup on my previous site, but I didn’t quite like the inclusion of html. I instead added the footer using the page-footer section in _quarto.yml as described here. I basically copied this statement from one of Andrew Heiss’ courses. To get the alignment right, I also had to grab a bit of code out of his custom.scss file and stick it int my custom.css file (I don’t really know the difference between scss and css, seemed easier to place it into the css file.)\n\n\nExtensions\nTo get the nice icons in the footer that Andrew has, one needs to install an extension. In this case, it’s the fontawsome extension. This is easily done, as described here. or here. I expect many more cool extensions to show up soon.\n\n\nOther settings\nI’m using Google Analytics, it is easy to include that in a Quarto website. I also turned on the Cookies setting notification (I don’t use any cookies directly, but Google Analytics likely does, and probably Mailchimp that I use for my newsletter too?).\n\n\nPublishing\nI host my website on Netlify. I followed the Quarto documentation. First I did a quarto publish netlify. That placed my website onto Netlify and gave it a temporary URL. That was great for testing it online (as opposed to testing locally with Quarto’s preview option, which of course I used a lot as well). Once online, I ran a link checker (I like using Dr Link Check but there are many others.) Of course there were broken internal links, so I went ahead and fixed them. I decided that I like the somewhat manual publishing of the site instead of doing it automatically with GitHub integration (also an option described in the Quarto documentation, and what I was using). The manual approach means I can mess around with new blog posts and sync with GitHub and don’t have to worry about using the main branch or not and then only once things look good do I publish with Quarto. So I changed my setup to that. That was done easily by setting the information in the _publish.yml file to point to my actual website URL, and changing the settings on the Netlify side as described in the Quarto documentation.\n\n\nSummary\nOverall, it wasn’t too hard. The one item that got me stuck for a bit was the aliases issue as described above. The conversion did require some manual changing, which I’m sure I could have written R scripts for it, but since my webpage isn’t that large, it seemed easier to just do things by hand. The new site is slightly different, some parts are simplified, but I got pretty much the same functionality and content back. And it is a much simpler setup compared to the - in my opinion - fairly convoluted setup of Hugo/blogdown/Wowchemy. Overall I’m happy with the results. My research group website is still using blogdown/Hugo/Wowchemy. That one has a few more custom layout features, which would likely require some fiddling before they work in Quarto. However, I have high hopes for those extensions and I’m pretty sure soon someone will have made new layouts, and then I might just be able to use some of them. So research group website conversion will happen, though probably not this year 😄.\n\n\nFurther resources\nIn addition to the main Quarto website and the blogs mentioned above, the a Quarto tip a day by Mine Çetinkaya-Rundel and Thomas Mock’s materials (look for his Quarto training repositories) are great resources. This repo has materials for one of his workshops, here is another one. This tutorial by Isabella Velásquez is another nice resource describing specifically Quarto blogs.\n\n\nAcknowledgments\nIt should be obvious that I owe a lot of these ideas to the blog posts and other resources I cite above. So thanks to those who tried it before me and wrote about it!\n\n\n\n\nCitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Migration from {Hugo/blogdown/Wowchemy} to {Quarto}},\n  date = {2022-10-01},\n  url = {https://www.andreashandel.com/posts/2022-10-01-hugo-to-quarto-migration},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “Migration from Hugo/Blogdown/Wowchemy to\nQuarto.” October 1, 2022. https://www.andreashandel.com/posts/2022-10-01-hugo-to-quarto-migration."
  },
  {
    "objectID": "presentations.html",
    "href": "presentations.html",
    "title": "Presentations",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\n\n\nA presentation for and discussion with our departmental PhD and MS students\n\n\n\n\n\n\nJan 17, 2023\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\n\n\nA presentation for our departmental PhD and MS students to kick off a semester-long IDP development\n\n\n\n\n\n\nJan 10, 2023\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\n\n\nA presentation given at the 12th European Conference on Mathematical and Theoretical Biology.\n\n\n\n\n\n\nSep 22, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWorkshop\n\n\nTeaching\n\n\nModeling\n\n\nImmunology\n\n\n\n\nFor the 14th time, and 3rd time online, my colleague Paul Thomas and I taught our annual SISMID workshop.\n\n\n\n\n\n\nJul 20, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInoculum Dose\n\n\nNorovirus\n\n\nInfluenza\n\n\n\n\nA (virtual) research presentation given at York University.\n\n\n\n\n\n\nApr 21, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAcademia\n\n\nCareer\n\n\nResearch\n\n\n\n\nA presentation for our departmental PhD/MS students on publishing (peer reviewed) papers.\n\n\n\n\n\n\nJan 15, 2022\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\nCOVID-19\n\n\n\n\nA presentation of some recent projects, given at GA Southern University.\n\n\n\n\n\n\nNov 1, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nData Science\n\n\n\n\nA presentation given to team members of Metrum RG\n\n\n\n\n\n\nOct 26, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nModeling\n\n\nR package\n\n\n\n\nAn introduction to infectious disease modeling and software to learn it.\n\n\n\n\n\n\nOct 25, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nData Science\n\n\n\n\nA presentation given at a Data Science & Business Intelligence Society of Atlanta (virtual) meeting\n\n\n\n\n\n\nSep 24, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWorkshop\n\n\nTeaching\n\n\nModeling\n\n\nImmunology\n\n\n\n\nFor the 13th time, and 2nd time online, my colleague Paul Thomas and I taught our annual SISMID workshop.\n\n\n\n\n\n\nJul 14, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR package\n\n\nR package\n\n\n\n\nA pre-recorded presentation given at useR! 2021 introducing one of our new R packages\n\n\n\n\n\n\nJul 7, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\nVaccines\n\n\nInoculum Dose\n\n\n\n\nA (virtual) presentation discussing the role of dose for Influenza vaccines.\n\n\n\n\n\n\nApr 28, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\n\n\nA presentation for our departmental PhD and MS students on mentorship\n\n\n\n\n\n\nApr 22, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAcademia\n\n\nCareer\n\n\nResearch\n\n\n\n\nA presentation for our departmental PhD and MS students on how to find and implement good (academic) projects\n\n\n\n\n\n\nApr 1, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\nVaccines\n\n\nInoculum Dose\n\n\n\n\nA (virtual) presentation of some work on understanding the impact of dose for infection and vaccination.\n\n\n\n\n\n\nFeb 22, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\nAcademia\n\n\n\n\nA presentation for our departmental PhD and MS students on how to create CVs and resumes\n\n\n\n\n\n\nFeb 18, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\nWebsite\n\n\n\n\nSlides for a discussion with our departmental PhD and MS students on how to build an online brand and presence\n\n\n\n\n\n\nJan 21, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\n\n\nA presentation for our departmental PhD and MS students to kick off a semester-long IDP development\n\n\n\n\n\n\nJan 14, 2021\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfluenza\n\n\nVaccines\n\n\n\n\nA (virtual) presentation of some work on understanding the impact of dose for Influenza vaccines.\n\n\n\n\n\n\nDec 2, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nModeling\n\n\nR package\n\n\n\n\nAn introduction ot infectious disease modeling and software to learn it.\n\n\n\n\n\n\nNov 23, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nModeling\n\n\nCOVID-19\n\n\n\n\nA (virtual) presentation of some recent COVID-19 modeling work.\n\n\n\n\n\n\nNov 18, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nModeling\n\n\nCOVID-19\n\n\nVaccines\n\n\n\n\nA (virtual) presentation and discussion covering COVID-19 vaccines and modeling.\n\n\n\n\n\n\nOct 22, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nModeling\n\n\nCOVID-19\n\n\n\n\nA (virtual) presentation at Virginia Tech discussing some recent projects related to COVID-19 modeling and analysis.\n\n\n\n\n\n\nOct 7, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nModeling\n\n\nCOVID-19\n\n\n\n\nA (virtual) presentation at the University of British Columbia discussing some recent COVID-19 modeling projects.\n\n\n\n\n\n\nSep 30, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nCOVID-19\n\n\n\n\nA short talk given as part of an webinar on Battling Dual Threats: Flu and COVID-19 Converge hosted by MJH life sciences.\n\n\n\n\n\n\nSep 15, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nCOVID-19\n\n\n\n\nA (virtual) presentation at UGA’s Global Health Institute discussing some recent COVID-19 work.\n\n\n\n\n\n\nAug 27, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWorkshop\n\n\nTeaching\n\n\nModeling\n\n\nImmunology\n\n\n\n\nFor the 12th time, and the first time online, my colleague Paul Thomas and I taught our annual SISMID workshop.\n\n\n\n\n\n\nJul 20, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nCOVID-19\n\n\n\n\nA (virtual) presentation at GA Southern discussing some recent COVID-19 projects.\n\n\n\n\n\n\nApr 20, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nTuberculosis\n\n\n\n\nA presentation on some TB superspreading work I did for the EIA research group at UGA.\n\n\n\n\n\n\nMar 4, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nAcademia\n\n\nCareer\n\n\n\n\nA presentation for our departmental PhD students on various topics related to (peer reviewed) papers.\n\n\n\n\n\n\nFeb 20, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nCareer\n\n\n\n\nA presentation for our departmental PhD students on how to build and manage their brand (aka online presence).\n\n\n\n\n\n\nJan 23, 2020\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nResearch\n\n\nInfectious Disease\n\n\n\n\nSome recent research exploring the role of dose for infection and vaccination.\n\n\n\n\n\n\nOct 23, 2019\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nTeaching\n\n\nInfectious Disease\n\n\nImmunology\n\n\n\n\nAn introductory workshop on Infectious Diseases, Immunology and Within-Host Models.\n\n\n\n\n\n\nJul 17, 2019\n\n\nAndreas Handel\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nWorkshop\n\n\nInfectious Disease\n\n\nResearch\n\n\n\n\nAn introductory workshop on infectious disease modeling.\n\n\n\n\n\n\nJul 10, 2019\n\n\nAndreas Handel\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "presentations/2019-07-hangzhou/index.html",
    "href": "presentations/2019-07-hangzhou/index.html",
    "title": "Introduction to Infectious Disease Modeling",
    "section": "",
    "text": "Outline\nThe following topics are covered in this workshop:\n\nIntroduction to infectious disease modeling\nSome example models\nHow to use simulation models\nTypes of models\nSources of uncertainty\nHow to build (good) models\nHow to assess modeling studies\nActive learning of infectious disease epidemiology\n\n\n\nPresentation Slides\nAll pdf slides as zip file\n\n\n\n\nCitationBibTeX citation:@online{handel2019,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Introduction to {Infectious} {Disease} {Modeling}},\n  date = {2019-07-10},\n  url = {https://www.andreashandel.com/presentations/2019-07-hangzhou},\n  langid = {en},\n  abstract = {An introductory workshop on infectious disease modeling.}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2019. “Introduction to Infectious Disease\nModeling.” July 10, 2019. https://www.andreashandel.com/presentations/2019-07-hangzhou."
  },
  {
    "objectID": "presentations/2019-07-sismid/index.html",
    "href": "presentations/2019-07-sismid/index.html",
    "title": "Introduction to within-host modeling",
    "section": "",
    "text": "Outline\nI cover the following topics in this workshop (my co-teacher Paul Thomas covers the Immunology part):\n\nIntroduction to modeling\nSome example models\nHow to use simulation models\nSources of uncertainty\nTypes of models\nHow to build (good) models\nHow to assess modeling studies\n\n\n\nPresentation Slides\nAll pdf slides as zip file\n\n\n\n\nCitationBibTeX citation:@online{handel2019,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Introduction to Within-Host Modeling},\n  date = {2019-07-17},\n  url = {https://www.andreashandel.com/presentations/2019-07-sismid},\n  langid = {en},\n  abstract = {An introductory workshop on Infectious Diseases,\n    Immunology and Within-Host Models.}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2019. “Introduction to Within-Host\nModeling.” July 17, 2019. https://www.andreashandel.com/presentations/2019-07-sismid."
  },
  {
    "objectID": "presentations/2019-10-paris/index.html",
    "href": "presentations/2019-10-paris/index.html",
    "title": "Model-based optimization of vaccine inoculum dose",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2019,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Model-Based Optimization of Vaccine Inoculum Dose},\n  date = {2019-10-23},\n  url = {https://www.andreashandel.com/presentations/2019-10-paris},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2019. “Model-Based Optimization of Vaccine\nInoculum Dose.” October 23, 2019. https://www.andreashandel.com/presentations/2019-10-paris."
  },
  {
    "objectID": "presentations/2020-01-your-brand/index.html",
    "href": "presentations/2020-01-your-brand/index.html",
    "title": "Building and curating your brand",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Building and Curating Your Brand},\n  date = {2020-01-23},\n  url = {https://www.andreashandel.com/presentations/2020-01-your-brand},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Building and Curating Your Brand.”\nJanuary 23, 2020. https://www.andreashandel.com/presentations/2020-01-your-brand."
  },
  {
    "objectID": "presentations/2020-02-reading-managing-publishing-papers/index.html",
    "href": "presentations/2020-02-reading-managing-publishing-papers/index.html",
    "title": "Reading, managing and publishing papers",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Reading, Managing and Publishing Papers},\n  date = {2020-02-20},\n  url = {https://www.andreashandel.com/presentations/2020-02-reading-managing-publishing-papers},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Reading, Managing and Publishing\nPapers.” February 20, 2020. https://www.andreashandel.com/presentations/2020-02-reading-managing-publishing-papers."
  },
  {
    "objectID": "presentations/2020-03-tb-superspreaders/index.html",
    "href": "presentations/2020-03-tb-superspreaders/index.html",
    "title": "Tuberculosis Superspreading",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Tuberculosis {Superspreading}},\n  date = {2020-03-04},\n  url = {https://www.andreashandel.com/presentations/2020-03-tb-superspreaders},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Tuberculosis Superspreading.” March\n4, 2020. https://www.andreashandel.com/presentations/2020-03-tb-superspreaders."
  },
  {
    "objectID": "presentations/2020-04-gasouthern-covid/index.html",
    "href": "presentations/2020-04-gasouthern-covid/index.html",
    "title": "Some recent analysis and modeling applied to COVID-19",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Some Recent Analysis and Modeling Applied to {COVID-19}},\n  date = {2020-04-20},\n  url = {https://www.andreashandel.com/presentations/2020-04-gasouthern-covid},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Some Recent Analysis and Modeling Applied\nto COVID-19.” April 20, 2020. https://www.andreashandel.com/presentations/2020-04-gasouthern-covid."
  },
  {
    "objectID": "presentations/2020-07-sismid/index.html",
    "href": "presentations/2020-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Infectious {Diseases,} {Immunology,} and {Within-Host}\n    {Models}},\n  date = {2020-07-20},\n  url = {https://www.andreashandel.com/presentations/2020-07-sismid},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Infectious Diseases, Immunology, and\nWithin-Host Models.” July 20, 2020. https://www.andreashandel.com/presentations/2020-07-sismid."
  },
  {
    "objectID": "presentations/2020-08-uga-ghi/index.html",
    "href": "presentations/2020-08-uga-ghi/index.html",
    "title": "Studying COVID-19 Spread and Control",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Studying {COVID-19} {Spread} and {Control}},\n  date = {2020-08-27},\n  url = {https://www.andreashandel.com/presentations/2020-08-uga-ghi},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Studying COVID-19 Spread and\nControl.” August 27, 2020. https://www.andreashandel.com/presentations/2020-08-uga-ghi."
  },
  {
    "objectID": "presentations/2020-09-mjh/index.html",
    "href": "presentations/2020-09-mjh/index.html",
    "title": "Population-level patterns of COVID-19 and Flu: What should we expect?",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Population-Level Patterns of {COVID-19} and {Flu:} {What}\n    Should We Expect?},\n  date = {2020-09-15},\n  url = {https://www.andreashandel.com/presentations/2020-09-mjh},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Population-Level Patterns of COVID-19 and\nFlu: What Should We Expect?” September 15, 2020. https://www.andreashandel.com/presentations/2020-09-mjh."
  },
  {
    "objectID": "presentations/2020-09-ubc/index.html",
    "href": "presentations/2020-09-ubc/index.html",
    "title": "Modeling COVID-19",
    "section": "",
    "text": "The presentation slides are here.\nThe presentation slides are here.\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Modeling {COVID-19}},\n  date = {2020-09-30},\n  url = {https://www.andreashandel.com/presentations/2020-09-ubc},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Modeling COVID-19.” September 30,\n2020. https://www.andreashandel.com/presentations/2020-09-ubc."
  },
  {
    "objectID": "presentations/2020-10-uga-pha/index.html",
    "href": "presentations/2020-10-uga-pha/index.html",
    "title": "An overview of COVID-19 vaccines and modeling",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {An Overview of {COVID-19} Vaccines and Modeling},\n  date = {2020-10-22},\n  url = {https://www.andreashandel.com/presentations/2020-10-uga-pha},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “An Overview of COVID-19 Vaccines and\nModeling.” October 22, 2020. https://www.andreashandel.com/presentations/2020-10-uga-pha."
  },
  {
    "objectID": "presentations/2020-10-vt/index.html",
    "href": "presentations/2020-10-vt/index.html",
    "title": "COVID-19: Modeling, Visualization and Data Analysis",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {COVID-19: {Modeling,} {Visualization} and {Data} {Analysis}},\n  date = {2020-10-07},\n  url = {https://www.andreashandel.com/presentations/2020-10-vt},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “COVID-19: Modeling, Visualization and Data\nAnalysis.” October 7, 2020. https://www.andreashandel.com/presentations/2020-10-vt."
  },
  {
    "objectID": "presentations/2020-11-fyos/index.html",
    "href": "presentations/2020-11-fyos/index.html",
    "title": "User-friendly software for simulation modeling of infectious diseases",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {User-Friendly Software for Simulation Modeling of Infectious\n    Diseases},\n  date = {2020-11-23},\n  url = {https://www.andreashandel.com/presentations/2020-11-fyos},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “User-Friendly Software for Simulation\nModeling of Infectious Diseases.” November 23, 2020. https://www.andreashandel.com/presentations/2020-11-fyos."
  },
  {
    "objectID": "presentations/2020-11-pudong/index.html",
    "href": "presentations/2020-11-pudong/index.html",
    "title": "Simulation modeling to inform COVID-19 control and vaccination strategies",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Simulation Modeling to Inform {COVID-19} Control and\n    Vaccination Strategies},\n  date = {2020-11-18},\n  url = {https://www.andreashandel.com/presentations/2020-11-pudong},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “Simulation Modeling to Inform COVID-19\nControl and Vaccination Strategies.” November 18, 2020. https://www.andreashandel.com/presentations/2020-11-pudong."
  },
  {
    "objectID": "presentations/2020-12-isv/index.html",
    "href": "presentations/2020-12-isv/index.html",
    "title": "The Role of Influenza Vaccine Dose Towards Homologous and Heterologous Protection",
    "section": "",
    "text": "The organizers also recorded the event, the video recording can be found here.\n\n\n\nCitationBibTeX citation:@online{handel2020,\n  author = {Andreas Handel},\n  editor = {},\n  title = {The {Role} of {Influenza} {Vaccine} {Dose} {Towards}\n    {Homologous} and {Heterologous} {Protection}},\n  date = {2020-12-02},\n  url = {https://www.andreashandel.com/presentations/2020-12-isv},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2020. “The Role of Influenza Vaccine Dose Towards\nHomologous and Heterologous Protection.” December 2, 2020. https://www.andreashandel.com/presentations/2020-12-isv."
  },
  {
    "objectID": "presentations/2021-01-building-your-brand/index.html",
    "href": "presentations/2021-01-building-your-brand/index.html",
    "title": "Building and curating your brand (online presence)",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Building and Curating Your Brand (Online Presence)},\n  date = {2021-01-21},\n  url = {https://www.andreashandel.com/presentations/2021-01-building-your-brand},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Building and Curating Your Brand (Online\nPresence).” January 21, 2021. https://www.andreashandel.com/presentations/2021-01-building-your-brand."
  },
  {
    "objectID": "presentations/2021-01-idp/index.html",
    "href": "presentations/2021-01-idp/index.html",
    "title": "Introduction to Individual Development Plans and the AAAS myIDP",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Introduction to {Individual} {Development} {Plans} and the\n    {AAAS} {myIDP}},\n  date = {2021-01-14},\n  url = {https://www.andreashandel.com/presentations/2021-01-idp},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Introduction to Individual Development\nPlans and the AAAS myIDP.” January 14, 2021. https://www.andreashandel.com/presentations/2021-01-idp."
  },
  {
    "objectID": "presentations/2021-02-U01/index.html",
    "href": "presentations/2021-02-U01/index.html",
    "title": "The Role of Inoculum Dose Following Infection or Vaccination",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {The {Role} of {Inoculum} {Dose} {Following} {Infection} or\n    {Vaccination}},\n  date = {2021-02-22},\n  url = {https://www.andreashandel.com/presentations/2021-02-U01},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “The Role of Inoculum Dose Following\nInfection or Vaccination.” February 22, 2021. https://www.andreashandel.com/presentations/2021-02-U01."
  },
  {
    "objectID": "presentations/2021-02-cv-resume/index.html",
    "href": "presentations/2021-02-cv-resume/index.html",
    "title": "Tips for writing a (hopefully good) CV or resume",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Tips for Writing a (Hopefully Good) {CV} or Resume},\n  date = {2021-02-18},\n  url = {https://www.andreashandel.com/presentations/2021-02-cv-resume},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Tips for Writing a (Hopefully Good) CV or\nResume.” February 18, 2021. https://www.andreashandel.com/presentations/2021-02-cv-resume."
  },
  {
    "objectID": "presentations/2021-04-CIVIC-meeting/index.html",
    "href": "presentations/2021-04-CIVIC-meeting/index.html",
    "title": "A Comparison of High-Dose and Regular-Dose Seasonal Influenza Vaccines Toward Eliciting Homologous and Heterologous Immunity",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {A {Comparison} of {High-Dose} and {Regular-Dose} {Seasonal}\n    {Influenza} {Vaccines} {Toward} {Eliciting} {Homologous} and\n    {Heterologous} {Immunity}},\n  date = {2021-04-28},\n  url = {https://www.andreashandel.com/presentations/2021-04-CIVIC-meeting},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “A Comparison of High-Dose and Regular-Dose\nSeasonal Influenza Vaccines Toward Eliciting Homologous and Heterologous\nImmunity.” April 28, 2021. https://www.andreashandel.com/presentations/2021-04-CIVIC-meeting."
  },
  {
    "objectID": "presentations/2021-04-good-projects/index.html",
    "href": "presentations/2021-04-good-projects/index.html",
    "title": "Coming up with good (research) projects",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Coming up with Good (Research) Projects},\n  date = {2021-04-01},\n  url = {https://www.andreashandel.com/presentations/2021-04-good-projects},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Coming up with Good (Research)\nProjects.” April 1, 2021. https://www.andreashandel.com/presentations/2021-04-good-projects."
  },
  {
    "objectID": "presentations/2021-04-mentorship/index.html",
    "href": "presentations/2021-04-mentorship/index.html",
    "title": "How to be a good mentee and mentor",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {How to Be a Good Mentee and Mentor},\n  date = {2021-04-22},\n  url = {https://www.andreashandel.com/presentations/2021-04-mentorship},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “How to Be a Good Mentee and Mentor.”\nApril 22, 2021. https://www.andreashandel.com/presentations/2021-04-mentorship."
  },
  {
    "objectID": "presentations/2021-07-sismid/index.html",
    "href": "presentations/2021-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Infectious {Diseases,} {Immunology,} and {Within-Host}\n    {Models}},\n  date = {2021-07-14},\n  url = {https://www.andreashandel.com/presentations/2021-07-sismid},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Infectious Diseases, Immunology, and\nWithin-Host Models.” July 14, 2021. https://www.andreashandel.com/presentations/2021-07-sismid."
  },
  {
    "objectID": "presentations/2021-07-useR/index.html",
    "href": "presentations/2021-07-useR/index.html",
    "title": "An R package to flexibly generate simulation model flow diagrams",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {An {R} Package to Flexibly Generate Simulation Model Flow\n    Diagrams},\n  date = {2021-07-07},\n  url = {https://www.andreashandel.com/presentations/2021-07-useR},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “An R Package to Flexibly Generate\nSimulation Model Flow Diagrams.” July 7, 2021. https://www.andreashandel.com/presentations/2021-07-useR."
  },
  {
    "objectID": "presentations/2021-09-DSATL/index.html",
    "href": "presentations/2021-09-DSATL/index.html",
    "title": "Adventures in Public Health Data Analytics - COVID-19 and beyond",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Adventures in {Public} {Health} {Data} {Analytics} -\n    {COVID-19} and Beyond},\n  date = {2021-09-24},\n  url = {https://www.andreashandel.com/presentations/2021-09-DSATL},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Adventures in Public Health Data Analytics\n- COVID-19 and Beyond.” September 24, 2021. https://www.andreashandel.com/presentations/2021-09-DSATL."
  },
  {
    "objectID": "presentations/2021-10-Metrum/index.html",
    "href": "presentations/2021-10-Metrum/index.html",
    "title": "Modeling the role of dose for vaccines & some other projects",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Modeling the Role of Dose for Vaccines \\& Some Other\n    Projects},\n  date = {2021-10-26},\n  url = {https://www.andreashandel.com/presentations/2021-10-Metrum},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Modeling the Role of Dose for Vaccines\n& Some Other Projects.” October 26, 2021. https://www.andreashandel.com/presentations/2021-10-Metrum."
  },
  {
    "objectID": "presentations/2021-10-fyos/index.html",
    "href": "presentations/2021-10-fyos/index.html",
    "title": "Introduction to infectious disease modeling",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Introduction to Infectious Disease Modeling},\n  date = {2021-10-25},\n  url = {https://www.andreashandel.com/presentations/2021-10-fyos},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Introduction to Infectious Disease\nModeling.” October 25, 2021. https://www.andreashandel.com/presentations/2021-10-fyos."
  },
  {
    "objectID": "presentations/2021-11-GASouthern/index.html",
    "href": "presentations/2021-11-GASouthern/index.html",
    "title": "Adventures in Data Analytics and Modeling - COVID-19 and Influenza",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2021,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Adventures in {Data} {Analytics} and {Modeling} - {COVID-19}\n    and {Influenza}},\n  date = {2021-11-01},\n  url = {https://www.andreashandel.com/presentations/2021-11-GASouthern},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2021. “Adventures in Data Analytics and Modeling -\nCOVID-19 and Influenza.” November 1, 2021. https://www.andreashandel.com/presentations/2021-11-GASouthern."
  },
  {
    "objectID": "presentations/2022-01-tips-for-publishing-papers/index.html",
    "href": "presentations/2022-01-tips-for-publishing-papers/index.html",
    "title": "Tips for publishing academic papers",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Tips for Publishing Academic Papers},\n  date = {2022-01-15},\n  url = {https://www.andreashandel.com/presentations/2022-01-tips-for-publishing-papers},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “Tips for Publishing Academic\nPapers.” January 15, 2022. https://www.andreashandel.com/presentations/2022-01-tips-for-publishing-papers."
  },
  {
    "objectID": "presentations/2022-04-York-University/index.html",
    "href": "presentations/2022-04-York-University/index.html",
    "title": "Assessing the impact of dose on infection and vaccination outcomes",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Assessing the Impact of Dose on Infection and Vaccination\n    Outcomes},\n  date = {2022-04-21},\n  url = {https://www.andreashandel.com/presentations/2022-04-York-University},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “Assessing the Impact of Dose on Infection\nand Vaccination Outcomes.” April 21, 2022. https://www.andreashandel.com/presentations/2022-04-York-University."
  },
  {
    "objectID": "presentations/2022-07-sismid/index.html",
    "href": "presentations/2022-07-sismid/index.html",
    "title": "Infectious Diseases, Immunology, and Within-Host Models",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Infectious {Diseases,} {Immunology,} and {Within-Host}\n    {Models}},\n  date = {2022-07-20},\n  url = {https://www.andreashandel.com/presentations/2022-07-sismid},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “Infectious Diseases, Immunology, and\nWithin-Host Models.” July 20, 2022. https://www.andreashandel.com/presentations/2022-07-sismid."
  },
  {
    "objectID": "presentations/2022-09-ECMTB/index.html",
    "href": "presentations/2022-09-ECMTB/index.html",
    "title": "The impact of seasonal Influenza vaccine dose on homologous and heterologous immunity",
    "section": "",
    "text": "Note: I had to cancel the trip at the last minute and thus was not able to deliver the presentation. I decide to leave the slides here anyway.\n\n\n\nCitationBibTeX citation:@online{handel2022,\n  author = {Andreas Handel},\n  editor = {},\n  title = {The Impact of Seasonal {Influenza} Vaccine Dose on Homologous\n    and Heterologous Immunity},\n  date = {2022-09-22},\n  url = {https://www.andreashandel.com/presentations/2022-09-ECMTB},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2022. “The Impact of Seasonal Influenza Vaccine\nDose on Homologous and Heterologous Immunity.” September 22,\n2022. https://www.andreashandel.com/presentations/2022-09-ECMTB."
  },
  {
    "objectID": "presentations/2023-01-academia-vs-industry/index.html",
    "href": "presentations/2023-01-academia-vs-industry/index.html",
    "title": "Some thoughts on academic versus industry positions",
    "section": "",
    "text": "CitationBibTeX citation:@online{handel2023,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Some Thoughts on Academic Versus Industry Positions},\n  date = {2023-01-17},\n  url = {https://www.andreashandel.com/presentations/2023-01-academia-vs-industry},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2023. “Some Thoughts on Academic Versus Industry\nPositions.” January 17, 2023. https://www.andreashandel.com/presentations/2023-01-academia-vs-industry."
  },
  {
    "objectID": "presentations/2023-01-idp/index.html",
    "href": "presentations/2023-01-idp/index.html",
    "title": "Introduction to Individual Development Plans and the AAAS myIDP",
    "section": "",
    "text": "This is a slightly updated version of a similar presentation from a few years ago.\n\n\n\nCitationBibTeX citation:@online{handel2023,\n  author = {Andreas Handel},\n  editor = {},\n  title = {Introduction to {Individual} {Development} {Plans} and the\n    {AAAS} {myIDP}},\n  date = {2023-01-10},\n  url = {https://www.andreashandel.com/presentations/2023-01-idp},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nAndreas Handel. 2023. “Introduction to Individual Development\nPlans and the AAAS myIDP.” January 10, 2023. https://www.andreashandel.com/presentations/2023-01-idp."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Click on any of the Projects below to participate in my current research:\n\n\n  Moral Characters\n  \n    Complete a 3-5 min survey\n  \n  \n\n\n  \n\n  Individual Differences\n  \n    Complete a longer survey\n  \n  \n  \n\n\n\n  Moral Judgements\n  \n    Make a series of moral decisions (20 min)\n  \n  \n  \n\n\n\n\n\n\n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/IDEMAbook/index.html",
    "href": "projects/IDEMAbook/index.html",
    "title": "Infectious Disease Epidemiology - a Model-based Approach",
    "section": "",
    "text": "An online book (perpetually under construction) convering infectious disease epidemiology using a model-based approach. Some parts of the book are fairly readable and complete enough that I use it when I teach a course on that topic. Other sections are currently only templates/outlines. While I try to ensure that what I write is correct, the whole book is not thoroughly fact-checked, error-corrected, properly referenced, etc. While I have been considering the idea of turning this into a full/real book, in my current thinking the cost-benefit analysis doesn’t pan out. I thus plan to leave it online for free as is, with occasional updates and fixes, but without an attempt to make it polished and complete enough for a printed book."
  },
  {
    "objectID": "projects/IDEMAcourse/index.html",
    "href": "projects/IDEMAcourse/index.html",
    "title": "Infectious Disease Epidemiology - a Model-based Approach",
    "section": "",
    "text": "The course also makes heavy use of my IDEMA online book and the DSAIDE R package."
  },
  {
    "objectID": "projects/MADAcourse/index.html",
    "href": "projects/MADAcourse/index.html",
    "title": "Modern Applied Data Analysis",
    "section": "",
    "text": "Modern Applied Data Analysis (MADA) is a course I regularly teach online. All course materials are available in the form of a simple GitHub website and can be used by anyone for self-learning. You can find the course on this site."
  },
  {
    "objectID": "projects/SMIcourse/index.html",
    "href": "projects/SMIcourse/index.html",
    "title": "Simulation Modeling in Immunology",
    "section": "",
    "text": "The course makes heavy use of my DSAIRM R package."
  },
  {
    "objectID": "projects/dsaide/index.html",
    "href": "projects/dsaide/index.html",
    "title": "DSAIDE - Dynamical Systems Approach to Infectious Disease Epidemiology",
    "section": "",
    "text": "We developed an R package that teaches a modern, model-based approach to infectious disease epidemiology, without the need to write computer code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/dsairm/index.html",
    "href": "projects/dsairm/index.html",
    "title": "DSAIRM - Dynamical Systems Approach to Immune Response Modeling",
    "section": "",
    "text": "We developed an R package that provides immunologists and other bench scientists a user-friendly introduction to simulation modeling of within-host infection, without the need to write computer code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/flowdiagramr/index.html",
    "href": "projects/flowdiagramr/index.html",
    "title": "Flowdiagramr",
    "section": "",
    "text": "We developed the R package flowdiagramr to allow anyone to easily create high-quality ggplot2 based flow diagrams of simulation models (and other flowcharts) with just a few lines of R code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/iblir/index.html",
    "href": "projects/iblir/index.html",
    "title": "Introduction to Biostatistics Labs in R (iblir)",
    "section": "",
    "text": "iblir is an R package that contains several Shiny/learnr based tutorials that teach introductory aspects of biostatistics in an interactive way. Learn more about it on the package website."
  },
  {
    "objectID": "projects/modelbuilder/index.html",
    "href": "projects/modelbuilder/index.html",
    "title": "Modelbuilder",
    "section": "",
    "text": "We are developing an R package called modelbuilder that allows users to build and analyze compartmental, dynamical, mechanistic models (implemented as differential equations, discrete-time or stochastic), without the need to write computer code. Learn more about it on the package website."
  },
  {
    "objectID": "projects/quizgrader/index.html",
    "href": "projects/quizgrader/index.html",
    "title": "Quizgrader",
    "section": "",
    "text": "quizgrader is an R package allows teachers to administer and auto-grade quizzes that students submit online. It replaces the functionality often found in learning management systems (LMS). Student submissions can be fully analyzed to gain insights into problem areas. It is functional, but not yet fully tested. Learn more about it on the package website."
  },
  {
    "objectID": "projects/resourcelist/index.html",
    "href": "projects/resourcelist/index.html",
    "title": "Resource list website",
    "section": "",
    "text": "A simple website with a collection of lists with links to various resources that are related to my research and teaching, as well as some general (academic) career content."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "",
    "text": "This is a tutorial and worked example, to illustrate how one can use the brms and rethinking R packages to perform a Bayesian analysis of longitudinal data in a multilevel/hierarchical/mixed-effects setup.\nI wrote it mainly for my own benefit/learning (nothing forces learning a concept like trying to explain it.) Hopefully, others find it useful too.\nIt started out as a single post, then became too large and is now a multi-part series. It currently has the following parts:\nI generally place further resources and acknowledgments sections at the end. However, since this series seems to be expanding and there is no clear order, I decided to get it out of the way and place these items right here at the beginning, before starting with the tutorial sequence."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-outcome-the-likelihood",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-outcome-the-likelihood",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model for outcome (the likelihood)",
    "text": "Model for outcome (the likelihood)\nFor our scenario, the outcome of interest (the log of the virus load) is continuous, which we assume to be normally distributed. Note that this is technically never fully correct, since there is some lower limit of detection for the virus load, which would lead to a truncation at low values. (A similar upper limit of detection does at times also occur.) If you have such censored data, you have to decide what to do about them. Here, we assume for simplicity that all values are far away from any limits, such that a normal distribution is a good approximation.\nMaking this normal distribution assumption, the equation describing the outcome (the likelihood model) is\n\\[\nY_{i,t}  \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right)\n\\]\nThe \\(Y_{i,t}\\) are the measured outcomes (log virus load here) for each individual \\(i\\) at time \\(t\\). This is shown as symbols in the (simulated) data you can see in the figure above. The deterministic time-series trajectory for each individual is given by \\(\\mu_{i,t}\\) (shown as lines in the figure above). \\(\\sigma\\) captures variation in the data that is not accounted for by the deterministic trajectories.\nNote that you could assume a different distribution, based on the specifics of your data. For instance, if you had a time-series of counts, you could use a Poisson distribution. Some of the details would then change, e.g., you wouldn’t have a mean and standard deviation in your model, but instead a rate. But the overall setup described here will still work the same way."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-deterministic-trajectories",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-for-deterministic-trajectories",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model for deterministic trajectories",
    "text": "Model for deterministic trajectories\nNext, we need to describe the underlying deterministic time-series model for the outcome.\nThere are different ways for choosing this part of the model. If you have enough information about your system to propose a mechanistic/process model, it is generally the best idea to go with such a model. Unfortunately, this is rare. Further, process models for time-series data are often implemented as differential equations, and those can take a very long time to fit.\nA more common approach is to model the overall pattern in the data with some type of phenomenological/heuristic function, chosen to match the data reasonably well. Generalized linear models (GLM), such as linear or logistic models, fall into that category. Here, we use such a phenomenological function, but a GLM won’t describe the pattern in our data (rapid virus growth, followed by decline). Therefore, we use an equation that gets us the shape we are looking for. For our simple example here, I choose a function that grows polynomially and declines exponentially with time. To be clear, this function doesn’t try to represent any real processes or mechanisms, it is simply chosen as an easy way to capture the general pattern seen in the virus load time-series. This is very similar to the use of GLMs or other standard models, which often work well at describing the overall pattern, without assuming a mechanistic process leading to the relation between predictor and outcome assumed by the GLM.\nThe equation for our model is given by\n\\[\n\\mu_{i,t} = \\log\\left( t_i^{\\alpha_i} e^{-\\beta_i t_i} \\right)  \n\\] In the model, \\(t_i\\) are the times for each individual \\(i\\) at which their outcome \\(Y_{i,t}\\) was measured. Those could be the same for each individual, which we’ll do here for simplicity, but they could also be all at different times and things won’t change. The model parameters are \\(\\alpha_i\\) and \\(\\beta_i\\), and their values describe the trajectory for each individual.\nYou can convince yourself with the following bit of code that this function, for the right values of \\(\\alpha\\) and \\(\\beta\\), gives you “up, then down” curves as a function of time. Note that since we are modeling the log of the virus load, I already log-transformed the equation.\n\nt = seq(0.1,30,length=100) #simulating 30 days, don't start at 0 to avoid 0/inf in plot\nalpha = 20; beta = 2; #just some values to show shape\nmu = log(t^alpha*exp(-beta*t)) #log virus load\nplot(t,mu, type = \"l\",ylim=c(0,30)) #looks somewhat like virus load in acute infections\n\n\n\n\n(Log) virus load time series for the heuristic model we will use.\n\n\n\n\nThe simple function I use here is in fact not that great for most real data, and better functions exist. See part 4 of this tutorial, where I show a more complex 4-parameter function, the one we actually used for our research problem. But to keep things somewhat simple here, I’m sticking with the 2-parameter function. It is fully sufficient to illustrate all the conceptual ideas I want to discuss. It can give us time-series which look somewhat similar to real virus load data often seen in acute infections. Of course, you need to pick whatever function describes your data reasonably well."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#numerical-trickeries",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#numerical-trickeries",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Numerical trickeries",
    "text": "Numerical trickeries\nLet’s go on a brief detour and discuss an important topic that comes up often.\nIn the equation for \\(\\mu_{i,t}\\) I just introduced, only positive values of \\(\\alpha\\) and \\(\\beta\\) produce reasonable trajectories. It is common to have parameters that can only take on certain values (e.g., positive, between 0-1). The problem is that by default, most fitting routines assume that the parameters that need to be estimated can take on any value. It turns out that the fitting software we will use (Stan through rethinking and brms) can be told that some parameters are only positive. You’ll see that in action later. But with different software, that might not be possible. Further, as you’ll also see below, we don’t actually fit \\(\\alpha\\) and \\(\\beta\\) directly, and it is tricky to enforce them to be positive using the built-in parameter constraint functionality of Stan.\nA general trick is to redefine parameters and rewrite the model to ensure positivity. Here, we can do that by exponentiating the parameters \\(\\alpha\\) and \\(\\beta\\) like this\n\\[\n\\mu_{i,t}  = \\log\\left( t_i^{\\exp(\\alpha_{i})} e^{-\\exp(\\beta_{i}) t_i} \\right)\n\\] Now, \\(\\alpha_i\\) and \\(\\beta_i\\) themselves can take any value without the danger of getting a nonsensical shape for \\(\\mu_{i,t}\\). It is likely possible to fit the model without taking those exponents and hoping that during the fitting process, the fitting routine “notices” that only positive values make sense. However, it might make the numerical procedures less robust.\nAnother alternative would be to enforce positive \\(\\alpha_i\\) and \\(\\beta_i\\) by setting up the rest of the model such that they can only take positive values. I’ll show a version for that in part 4.\nOne issue with that exponentiation approach is that it can sometimes produce very large or very small numbers and lead to numerical problems. For instance, if during the fitting the solver tries \\(\\beta_i = 10\\) and time is 10, then the exponent in the second term becomes \\(e^{-10 exp(10)}\\), and that number is so small that R sets it to 0. Similarly, if the solver happens to explore \\(\\alpha_i = 10\\) at time 10, we would end up with \\(10^{exp(10)}\\) in the first term, which R can’t handle and sets to Infinity. (Try by typing exp(-10 * exp(10)) or 10^exp(10) into the R console). In both cases, the result will not make sense and can lead to the numerical routine either completely failing and aborting with an error, or at a minimum wasting computational time by having to ignore those values. (Stan is good at usually not completely breaking and instead ignoring such nonsensical results, but one can waste a lot of computational time.)\nNumerical issues like this one are not uncommon and something to always be aware of. To minimize such problems with very large/small numbers, one can often use algebraic (logarithm, exponent, etc.) rules and rewrite the equations. In our case, we can rewrite as\n\\[\n\\mu_{i,t}  =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i}\n\\] Using \\(\\mu_{i,t}\\) in this form in the code seems to work fine, as you’ll see. Note that this is exactly the same equation as the one above, just rewritten for numerical convenience. Nothing else has changed."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#modeling-the-main-model-parameters",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#modeling-the-main-model-parameters",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Modeling the main model parameters",
    "text": "Modeling the main model parameters\nOk, now let’s get back to building the rest of our model. So far, we specified an equation for the virus load trajectory \\(\\mu_{i,t}\\). We assume that every individual has their own virus-load trajectory, specified by parameters \\(\\alpha_i\\) and \\(\\beta_i\\). We need to define those. We allow each individual to have their own individual-level contribution to the parameters, and also assume there is a potential population-level effect of dose.\nThe latter assumption is in fact our main scientific question. We want to know if the dose someone receives has a systematic impact on the virus load trajectories. At the same time, we want to allow for variation between individuals. We could also consider a model that allows the impact of dose to be different for every individual. With enough data, that might seem feasible. But here, we assume we have limited data. (Of course, this is just simulated data, so it is as large as we want it to be. But for the real research project which motivates this tutorial, we only have data on 20 individuals.) We also really want to focus on the overall, population-level, effect of dose, and are less interested to see if there is variation of dose effect among individuals.\nIt is not clear how to best model the potential impact of inoculum dose. We really don’t have much biological/scientific intuition. Without such additional insight, a linear assumption is generally a reasonable choice. We thus model the main parameters \\(\\alpha_i\\) and \\(\\beta_i\\) as being linearly related to the (log of) the dose. This assumption relating the parameter to the log of the dose is mostly heuristic. But it does make some biological sense as often in systems like this, outcomes change in response to the logarithm of some input.\nIn addition to the dose component, every individual can have their unique contribution to \\(\\alpha_i\\) and \\(\\beta_i\\).\nWriting this in equation form gives\n\\[\n\\begin{aligned}\n\\alpha_{i} &  =  a_{0,i} + a_1 \\log (D_i) \\\\\n\\beta_{i}  & =  b_{0,i} + b_1 \\log (D_i)\n\\end{aligned}\n\\] Here, \\(a_{0,i}\\) and \\(b_{0,i}\\) are the individual-level contributions of each person to the main parameters, and \\(a_1\\) and \\(b_1\\) quantify how the dose each individual receives, \\(D_i\\), impacts the overall time-series trajectories. \\(a_1\\) and \\(b_1\\) do not vary between individuals, thus we are only estimating the overall/mean/population-level impact of dose, and won’t try to see if different individuals respond differently to dose. If, after fitting the data, we find that the distributions for \\(a_1\\) and \\(b_1\\) are mostly around zero, we could conclude that dose does not have an impact. In contrast, if the distributions for those parameters are away from zero, we conclude that dose seems to impact the time-series trajectories.\nNote that if we were to fit this model in a frequentist framework, we would overfit (trying to estimate too many parameters). That is because if every individual has their own \\(a_{0,i}\\) and \\(b_{0,i}\\), the model can take any shape without needing the dose-related parameters to play a role. Thus we would have non-identifiability of parameters. As you’ll see in the next post of this series, this feature of potential overfitting/non-identifiability can also be seen in the Bayesian approach, but we are still able to obtain reasonable fits and parameter estimates. We’ll discuss that topic in more detail in the next post."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#some-rescaling",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#some-rescaling",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Some rescaling",
    "text": "Some rescaling\nAlright, time for another brief detour.\nThe model we have is ok. But as it is written right now, the parameters \\(a_{i,0}\\) and \\(b_{i,0}\\) describe the virus-load trajectory for an individual with a dose of 1 (log(1)=0). In our made-up example, individuals receive doses of strength 10/100/1000 but not 1. If we didn’t work with dose on a log scale, the \\(a_{i,0}\\) and \\(b_{i,0}\\) parameters would represent trajectories for individuals who received a dose of 0. That doesn’t make sense, since anyone not being challenged with virus will not have any virus trajectory.\nIt doesn’t mean the model is wrong, one can still fit it and get reasonable estimates. But interpretation of parameters, and thus choices for priors, might get trickier. In such cases, some transformation of the data/model can be useful.\nA common approach is to adjust predictor variables by standardizing (subtracting the mean and dividing by the standard deviation). Here we do something slightly different. We subtract the log of the middle dose. We call that dose \\(D_m\\). In our example it takes on the value of 100. In general, you can do any transformation that you think makes the setup and problem easier to interpret.\nThe equations then become\n\\[\n\\begin{aligned}\n\\alpha_{i} &  =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}  & =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right)\n\\end{aligned}\n\\] Now the intercept parameters \\(a_{i,0}\\) and \\(b_{i,0}\\) describe the main model parameters \\(\\alpha_i\\) and \\(\\beta_i\\), and thus the trajectory for the virus, if the dose is at the intermediate level. Thus, these parameters are now easy to interpret."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#quick-summary",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#quick-summary",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Quick summary",
    "text": "Quick summary\nAt this stage in the model building process, our model as the following parts\n\\[\n\\begin{aligned}\n\\textrm{Outcome} \\\\\nY_{i,t}   \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{Deterministic time-series trajectory} \\\\\n\\mu_{i,t}   =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\n\\alpha_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}   =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\end{aligned}\n\\]\nThe model parameters are \\(\\sigma\\), \\(a_1\\), \\(b_1\\), \\(a_{i,0}\\) and \\(b_{i,0}\\). The latter two consist of as many parameters as there are individuals in the data. So if we have \\(N\\) individuals, we have a total of \\(3+2N\\) parameters.\nAt this point, we could fit the model in either a Bayesian or frequentist framework. For either approach, we need to determine what (if any) additional structure we want to impose on the model parameters.\nOne approach is to not impose any further structure. We could make the assumption that every individual is completely different from each other, such that their outcomes do not inform each other. That would mean we allow values of \\(a_{i,0}\\) and \\(b_{i,0}\\) to be different for each individual, and let them be completely independent from each other. Such a model is (in McElreath’s terminology) a no pooling model. Such a model is expected to fit the data for each individual well. But it would lead to overfitting, trying to estimate too many parameters given the data. That means the estimates for the parameters will be uncertain, and thus our question of interest, the possible impact of dose, will be hard to answer. Also, it won’t be very good at predicting future individuals.\nOn the other extreme, we could instead assume that all individuals share the same parameter values, i.e. \\(a_{i,0} = a_0\\) and \\(b_{i,0} = b_0\\). This is called a full pooling model. You see this model often in data analyses, when investigators take the mean of some measurements and then just model the means. For our example, it we would be modeling the mean virus load time-series trajectory for all individuals in a given dose group. This type of model can extract the population level (in our case dose) effect, but by ignoring the variation among individuals for the same dose, the model is likely overly confident in its estimates, and it leads to underfitting of the data. By not allowing differences between individuals, the model is likely too restrictive and thus is not that great at capturing the patterns seen in the data. We’ll explore that when we fit the models.\nAn intermediate model - and usually the best approach - is one that neither allows the \\(a_{i,0}\\) and \\(b_{i,0}\\) to be completely independent or forces them to be exactly the same. Instead, it imposes some correlation between the parameters. This leads to the mixed/hierarchical/multilevel modeling approach. Such an approach can be implemented in both a frequentist or Bayesian framework. Here, we focus on the Bayesian approach, which I personally find more intuitive since everything is explicit and spelled out."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#specifying-priors",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#specifying-priors",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Specifying priors",
    "text": "Specifying priors\nSince we are working in a Bayesian framework, our parameters need priors. We assume that for all models we discuss below, the parameters \\(a_{0,i}\\), \\(b_{0,i}\\), \\(a_1\\) and \\(b_1\\) have priors described by Normal distributions. The standard deviation \\(\\sigma\\) is modeled by a Half-Cauchy distribution (a Cauchy distribution that’s only defined for positive values, since standard deviations need to be positive). Those choices are a mix of convention, numerical usefulness and first principles. See for instance the Statistical Rethinking book for more details. You can likely choose other prior distributions and results might not change much. If they do, it means you don’t have a lot of data to inform your results and need to be careful about drawing conclusions.\nThe equations for our priors are\n\\[\n\\begin{aligned}\n\\sigma  \\sim \\mathrm{HalfCauchy}(0, 1)  \\\\\na_1  \\sim \\mathrm{Normal}(0.1, 0.1) \\\\\nb_1  \\sim \\mathrm{Normal}(-0.1, 0.1) \\\\\na_{0,i}  \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i} \\sim \\mathrm{Normal}(\\mu_b, \\sigma_a) \\\\\n\\end{aligned}\n\\] I gave the prior distributions for \\(\\sigma\\), \\(a_1\\) and \\(b_1\\) fixed values. I chose those values to get reasonable simulation results (as you’ll see below). We will use those same values for all models. The priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) are more interesting. They depend on parameters themselves. In the next sections, we will explore different choices for those parameters \\(\\mu_a\\), \\(\\mu_b\\), \\(\\sigma_a\\) and \\(\\sigma_b\\), based on the different modeling approaches described in the previous section."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-1",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 1",
    "text": "Model 1\nOur first model is one that replicates the no pooling approach. In a Bayesian framework, such a no-pooling model can be implemented by making the priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) very wide, which essentially assumes that any values are allowed, and there is (almost) no shared commonality/information among the parameters for each individual.\nIn our example, we can accomplish this by ensuring \\(\\sigma_a\\) and \\(\\sigma_b\\) are large, such that the normal distributions for \\(a_{i,0}\\) and \\(b_{i,0}\\) become very wide. In that case, the values for the mean, \\(\\mu_a\\) and \\(\\mu_b\\) don’t matter much since we allow the model to take on any values, even those far away from the mean. Therefore, we can just set \\(\\mu_a\\) and \\(\\mu_b\\) to some reasonable values, without paying too much attention.\nThis choice for the priors leads to a Bayesian model similar to a frequentist no-pooling model."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-2",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-2",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 2",
    "text": "Model 2\nNow we’ll try to reproduce the full pooling model in a Bayesian framework. We could remove the individual-level variation by setting \\(a_{i,0} = a_0\\) and \\(b_{i,0} = b_0\\) (and we’ll do that below). But if we want to keep the structure we have above, what we need to do is to ensure the priors for those parameters are very narrow, such that every individual is forced to have more or less the same value. We can accomplish that by setting values for \\(\\sigma_a\\) and \\(\\sigma_b\\) very close to zero.\nIf we set the \\(\\mu_a\\) and \\(\\mu_b\\) parameters to some fixed values, we would enforce \\(a_{i,0}\\) and \\(b_{i,0}\\) to take specific values too. We don’t want that, we want them to be estimated, we just want to make sure all individuals get pretty much the same estimate. To do so, we can give \\(\\mu_a\\) and \\(\\mu_b\\) their own distributions and make those wide/flat. A normal distribution for each parameter with a wide variance should work.\nWith these choices, we have a model that can find the mean for \\(a_{i,0}\\) and \\(b_{i,0}\\), but the spread in those parameters is minimal."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-3",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 3",
    "text": "Model 3\nOk, so we discussed that the no pooling model 1 is too flexible and thus likely overfits, the full pooling model 2 is too rigid and likely underfits. Why not build a model that has reasonable priors in between those two? That’s a good idea and it leads us to a partial pooling model.\nWe want priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) that are not too flat/wide (model 1) or too narrow (model 2). They should allow some variation, but still ensure that there is shared information among the parameters. With that, we might be able to find a happy medium between underfitting and overfitting. Such priors are known as regularizing priors. They allow some variability for the parameters among individuals, while implementing the notion that the individuals share some commonality, and therefore their parameters should also share some common features, as indicated by belonging to the same prior distributions.\nThe question is, how to set the priors? One option is to pick some values for \\(\\mu_a\\), \\(\\mu_b\\), \\(\\sigma_a\\) and \\(\\sigma_b\\) (either by specifying their distributions or by directly setting values), then do prior predictive simulations, see if results look reasonable (no crazy outcomes, but still a good bit of variability) and then iterate until one found good priors. One can also explore the impact of the priors on the posterior. If they have a strong impact, it suggests there is not enough data to fully determine the posterior.\nThis approach of trial and error is reasonable, and we’ll use it here for our model 3. But it also feels a bit like ad-hoc. One might want to ask the question if there is another way to pick the priors. The answer is yes, which brings us to our next model."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-4",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#model-4",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Model 4",
    "text": "Model 4\nInstead of trying to pick values for the priors manually (again, nothing wrong with that, but maybe not always optimal), one can let the data determine the priors. That approach involves estimating each of the parameters that specify the priors for \\(a_{i,0}\\) and \\(b_{i,0}\\).\nThe parameters \\(\\mu_a\\), \\(\\mu_b\\), \\(\\sigma_a\\) and \\(\\sigma_b\\) now get their own distributions, with their own priors (often called hyper-priors). The values for the hyper-priors are picked such that resulting simulations from the model produce (somewhat) reasonable trajectories, as you’ll see below. In principle, one can further specify them as functions of other priors (turtles, I mean priors, all the way down!). But in most cases, including our example, not much is gained from that.\nWhat now happens is that as we fit the model, our priors for \\(a_{i,0}\\) and \\(b_{i,0}\\) share some information, and the amount of sharing is controlled by the hyper-prior parameters, which are determined by the data fitting process itself. It sounds a bit like magic, and I admit that on some deep level, I still don’t fully understand this magic, even though I can follow the steps. Maybe at some point in the future I will fully get it. For now I’m content with the level of understanding I have, and knowing that it works 😄.\nThis model is a partial pooling model like model 3, but now the pooling is determined adaptively by the data. This leads to a happy intermediate between the too-rigid full-pooling model and the too-flexible no-pooling model. This kind of model is often the best choice."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#recap",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#recap",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Recap",
    "text": "Recap\nOk, those were a lot of steps, so to have it all in one place, here are the models again, now shown with equations and with all components in one place.\nAll models have these parts:\n\\[\n\\begin{aligned}\n\\textrm{Outcome} \\\\\nY_{i,t}   \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{Deterministic time-series trajectory} \\\\\n\\mu_{i,t}   =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\n\\alpha_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i}   =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\\\\n\\textrm{population-level priors} \\\\\n\\sigma  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\na_1 \\sim \\mathrm{Normal}(0.1, 0.1) \\\\\nb_1 \\sim \\mathrm{Normal}(-0.1, 0.1) \\\\\na_{0,i} \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i}  \\sim \\mathrm{Normal}(\\mu_b, \\sigma_a) \\\\\n\\end{aligned}\n\\]\nFor model 1, we set the parameters describing the distribution for \\(a_{0,i}\\) and \\(b_{0,i}\\) to produce un-informative/flat priors. For our example, these values work:\n\\[\n\\begin{aligned}\n\\mu_a & = 3  \\\\\n\\mu_b & = 1  \\\\\n\\sigma_a & = 10  \\\\\n\\sigma_b & = 10   \n\\end{aligned}\n\\]\nFor model 2, we set the standard deviations to a very small value and give the mean parameters somewhat flexible distributions. These work:\n\\[\n\\begin{aligned}\n\\mu_a & \\sim \\mathrm{Normal}(3, 1) \\\\\n\\mu_b & \\sim \\mathrm{Normal}(1, 1) \\\\\n\\sigma_a & = 0.001  \\\\\n\\sigma_b & = 0.001  \n\\end{aligned}\n\\]\nAs mentioned, an alternative for model 2, which I’ll call model 2a, is to reduce the parameters from 2N to 2 and specify these priors for what are now population-level only parameters, like this:\n\\[\n\\begin{aligned}\na_{0} &  \\sim \\mathrm{Normal}(3, 1) \\\\\nb_{0} & \\sim \\mathrm{Normal}(1, 1)\n\\end{aligned}\n\\]\nFor model 3, we set values that lead to priors that are reasonably intermediate between the model 1 too flat and model 2 too narrow priors. These work:\n\\[\n\\begin{aligned}\n\\mu_a & = 3  \\\\\n\\mu_b & = 1  \\\\\n\\sigma_a & = 1  \\\\\n\\sigma_b & = 1   \n\\end{aligned}\n\\]\nFinally, model 4 has distributions for all 4 parameters. These work for our example\n\\[\n\\begin{aligned}\n\\mu_a & \\sim \\mathrm{Normal}(3, 1) \\\\\n\\mu_b & \\sim \\mathrm{Normal}(1, 1) \\\\\n\\sigma_a & \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_b & \\sim \\mathrm{HalfCauchy}(0,1)  \n\\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#comment-on-terminology",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#comment-on-terminology",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Comment on terminology",
    "text": "Comment on terminology\nI have been talking about 4 different models (or 5 if you count model 2a). As I’m sure you realized, some models are structurally the same, just with different choices for the priors. In a Bayesian framework, the priors (which includes choices for both the distribution and values) are part of the model, thus in that sense, model 1 and 3 can be considered different models, even if we only change the values for the variance priors. For the purpose of this tutorial I’ll take that perspective and consider them separate models. It also makes it easier to talk about them by giving each their own label/number."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#general-settings",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#general-settings",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "General settings",
    "text": "General settings\n\n## General settings\nset.seed(123) #for reproducibility\n# days at which we assume outcome is measured\ntimevec &lt;- c(0.1,1,3,5,7,10,14,21,28,35,42)\n\n#different number of individuals per dose to make it clearer which is which\n#also, that's the structure of the data which motivated the tutorial\nNlow = 7; Nmed = 8; Nhigh = 9; filename = \"simdat.Rds\"\n#if you want to explore how model fitting changes if you increase sample size\n#turn on this line of code\n#this is used in part 4 of the tutorial\n#Nlow = 70; Nmed = 80; Nhigh = 90; filename = \"simdat_big.Rds\"\n\nNtot = Nlow + Nmed + Nhigh; #total number of individuals\n\n# Set values for dose\n# since we only consider dose on a log scale\n# we'll log transform right here and then always use it in those log units\nhigh_dose = log(1000)\nmed_dose = log(100)\nlow_dose = log(10)\ndosevec = c(rep(low_dose,Nlow),rep(med_dose,Nmed),rep(high_dose,Nhigh))\n# we are also creating a version of the dose variable\n# that consists of ordered categories instead of numeric values\n# we'll use that mostly for plotting\ndosevec_cat = ordered(c(rep(\"low\", Nlow),\n                        rep(\"medium\",Nmed),\n                        rep(\"high\",Nhigh)),\n                      levels=c(\"low\",\"medium\",\"high\"))\n\n\n## Setting parameter values\n\nI chose fairly low sample sizes, with less than 10 individuals for each dose group. This is motivated by the real data I have in mind, which has similar sample sizes. Of course, more data is generally better. In part 4 of the tutorial I play around a bit with fitting larger samples."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#setting-parameter-values",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#setting-parameter-values",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Setting parameter values",
    "text": "Setting parameter values\nThe parameters \\(\\sigma\\), \\(a_1\\) and \\(b_1\\) show up in all models. For easy comparison between models, I’m setting them to the same value for all models.\nFor the estimation procedure (see part 2), we assume that those parameters follow the distributions shown above. We could sample a single value for each of them from such a distribution. But to reduce variability and to more easily compare estimated parameters to those used to simulate the data, I’m setting them to specific values, which you can conceptually think of as being a single sample from the distributions we discussed above. It makes sense to chose their means as the value to use.\n\nsigma = 1\na1 = 0.1\nb1 = -0.1\n\nNow well get values for the other parameters. For model 1, we have \\(N\\) parameters for \\(a_{i,0}\\) and \\(b_{i,0}\\), with priors that are very wide. We set them as follows\n\nm1_mua = 3\nm1_mub = 1\nm1_sigmaa = 1\nm1_sigmab = 1\nm1_a0 = rnorm(n=Ntot, m1_mua, m1_sigmaa)\nm1_b0 = rnorm(n=Ntot, m1_mub, m1_sigmaa)\n# saving main parameters\nm1pars = c(sigma = sigma, a1 = a1, b1 = b1,\n           a0_mu = m1_mua, b0_mu = m1_mub)\n\nNote a few things here. First, the priors are narrower than I specified above. As you will see in the figures below, even with these less wide priors, results for model 1 still look way too variable. We can use the wider priors when we fit the model, to allow the data to dominate the fits. But for data generation, going too wild/wide seems pointless.\nSecond, you noticed that I did sample from distributions for the \\(a_{i,0}\\) and \\(b_{i,0}\\) parameters. That’s not necessary, I could have also specified values for each of the parameters, like I did for \\(\\sigma\\), \\(a_1\\) and \\(b_1\\), as long as the values can be thought of as coming from the underlying distribution. If I sample, I need to make sure to set a random seed (which I did above) to ensure reproducibility.\nLastly, I’m saving the parameters in a vector which will be added to the generated data so we can keep track of the parameters that were used to generate the data, and compare later with the estimates from the models.\nOk, now for model 2. We have 2 versions, model 2a collapses the individual-level parameters into a single population one. We’ll explore that model when doing the fitting, for simulating the data I’m just going with model 2.\n\nm2_mua = 3\nm2_mub = 1\nm2_sigmaa = 0.0001\nm2_sigmab = 0.0001\nm2_a0 = rnorm(n=Ntot, m2_mua, m2_sigmaa)\nm2_b0 = rnorm(n=Ntot, m2_mub, m2_sigmab)\nm2pars = c(sigma = sigma, a1 = a1, b1 = b1,\n           a0_mu = m2_mua, b0_mu = m2_mub)\n\nFinally, model 3 with priors that have a width between those of model 1 and model 2.\n\nm3_mua = 3\nm3_mub = 1\nm3_sigmaa = 0.1\nm3_sigmab = 0.1\nm3_a0 = rnorm(n=Ntot, m3_mua, m3_sigmaa)\nm3_b0 = rnorm(n=Ntot, m3_mub, m3_sigmaa)\nm3pars = c(sigma = sigma, a1 = a1, b1 = b1,\n           a0_mu = m3_mua, b0_mu = m3_mub)\n\nNote that for the purpose of simulating data, model 4 is basically the same as model 3. We would need to sample (or pick) values for the parameters \\(\\mu_a\\), \\(\\sigma_a\\), \\(\\mu_b\\), and \\(\\sigma_b\\) and then use them to sample (or set) values for \\(a_{i,0}\\) and \\(b_{i,0}\\). This setup makes sense during fitting, but for generating data, it isn’t really different than what er already did. You can conceptually assume we did sample parameters and happen to get the values shown for model 3. Thus, model 4 collapses to the models we already specified.\nOverall, when generating data, we can go through all steps of sampling from each specified distribution to get values. Nothing wrong with that. But if we change the random seed, values change. And it is harder to compare the parameters used to generate the data with those that are estimated. Thus, it is generally easier during the data generation process to assume conceptually that values correspond to samples from distributions, but then set the values manually. Above, we used that approach for most parameters. We did sample the \\(a_{0,i}\\) and \\(b_{0,i}\\) to show explicitly the sampling steps involved in generating simulated data from our Bayesian models."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#creating-simulated-data",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#creating-simulated-data",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Creating simulated data",
    "text": "Creating simulated data\nNow we can combine the parameters as specified in the equations above to get simulated trajectories for each individual, for each of the models. We just need to add the parameters together in the way prescribed by the model to get to the outcome. This is a nice feature of Bayesian models, that you can run them both “forward” to generate data given parameter values, and “backward” to estimate parameter values, given the data. Because of that feature, Bayesian models are generative models.\nHere is the code that computes the mean and the outcome for model 1, the wide model.\n\nm1_alpha = m1_a0 + a1*(dosevec - med_dose)\nm1_beta = m1_b0 + b1*(dosevec - med_dose)\n#doing matrix multiplication to get time-series for each individual\n#for that to work, the timevec vector needs to be transposed\nm1_mu =  exp(m1_alpha) %*% t(log(timevec)) - exp(m1_beta) %*% t(timevec)\n# apply variation following a normal distribution to each data point\nm1_y = rnorm(length(m1_mu),m1_mu, sigma)\n# in a final step, we reorganize the data into a long data frame with\n# columns id, time, dose, model,\n# the deterministic mean mu, and the normally distributed outcome.\n# We store dose in 3 versions, the original (log transformed one),\n# the one that has the middle value subtracted, and a categorical one.\n# Note that trick using sort to get time in the right order.\n# Not a robust way of doing things, but works here\nm1_dat &lt;- data.frame(id = rep(1:Ntot,length(timevec)),\n                     dose = rep(dosevec,length(timevec)),\n                     dose_adj = rep(dosevec,length(timevec))-med_dose,\n                     dose_cat =  rep(dosevec_cat,length(timevec)),\n                     time = sort(rep(timevec,Ntot)),\n                     mu = as.vector(m1_mu),\n                     outcome = as.vector(m1_y),\n                     model = \"m1\")\n\nNow we just repeat the same code again for the other models.\n\n#model 2\nm2_alpha = m2_a0 + a1*(dosevec - med_dose)\nm2_beta = m2_b0 + b1*(dosevec - med_dose)\nm2_mu =  exp(m2_alpha) %*% t(log(timevec)) - exp(m2_beta) %*% t(timevec)\nm2_y = rnorm(length(m2_mu),m2_mu, sigma)\nm2_dat &lt;- data.frame(id = rep(1:Ntot,length(timevec)),\n                     dose = rep(dosevec,length(timevec)),\n                     dose_adj = rep(dosevec,length(timevec))-med_dose,\n                     dose_cat =  rep(dosevec_cat,length(timevec)),\n                     time = sort(rep(timevec,Ntot)),\n                     mu = as.vector(m2_mu),\n                     outcome = as.vector(m2_y),\n                     model = \"m2\")\n\n#model 3\nm3_alpha = m3_a0 + a1*(dosevec - med_dose)\nm3_beta = m3_b0 + b1*(dosevec - med_dose)\nm3_mu =  exp(m3_alpha) %*% t(log(timevec)) - exp(m3_beta) %*% t(timevec)\nm3_y = rnorm(length(m3_mu),m3_mu, sigma)\nm3_dat &lt;- data.frame(id = rep(1:Ntot,length(timevec)),\n                     dose = rep(dosevec,length(timevec)),\n                     dose_adj = rep(dosevec,length(timevec))-med_dose,\n                     dose_cat =  rep(dosevec_cat,length(timevec)),\n                     time = sort(rep(timevec,Ntot)),\n                     mu = as.vector(m3_mu),\n                     outcome = as.vector(m3_y),\n                     model = \"m3\")"
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#plotting-the-simulated-data",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#plotting-the-simulated-data",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Plotting the simulated data",
    "text": "Plotting the simulated data\nTo ensure our simulated data makes sense, let’s plot what we produced. We’ll use ggplot2, so let’s load it first.\n\nlibrary('ggplot2')\n\nThese lines of code create plots for each model/simulated dataset.\n\np1 &lt;- ggplot(m1_dat) +\n  geom_line(aes(x=time, y=mu, col = dose_cat, group = id)) +\n  geom_point(aes(x=time, y=outcome, col = dose_cat)) +\n  scale_y_continuous(limits = c(-30,200)) +\n  labs(y = \"Outcome (log virus load)\",  x = \"Time (days)\") +\n  theme_minimal()\n\n\np2 &lt;- ggplot(m2_dat) +\n  geom_line(aes(x=time, y=mu, col = dose_cat, group = id)) +\n  geom_point(aes(x=time, y=outcome, col = dose_cat)) +\n  scale_y_continuous(limits = c(-30,50)) +\n  labs(y = \"Outcome (log virus load)\",  x = \"Time (days)\") +\n  theme_minimal()\n\np3 &lt;- ggplot(m3_dat) +\n  geom_line(aes(x=time, y=mu, col = dose_cat, group = id)) +\n  geom_point(aes(x=time, y=outcome, col = dose_cat)) +\n  scale_y_continuous(limits = c(-30,50)) +\n  labs(y = \"Outcome (log virus load)\",  x = \"Time (days)\") +\n  theme_minimal()\n\nNow, let’s plot the simulated data. For each plot, the lines show the deterministic mean trajectory, and the symbols show the outcomes, which have some extra variation on top, determined by the value of \\(\\sigma\\).\n\nplot(p1)\n\n\n\nplot(p2)\n\n\n\nplot(p3)\n\n\n\n\nAs you can see, the priors for model 1 are so wide that some of the resulting trajectories are not reasonable. The variation between individuals is so strong that the dose effect - which we programmed into the simulated data to exist - is swamped out. That could certainly be true for real data, sometimes there is just too much noise/variability to detect a pattern, even if it exists. But some of the trajectories produce virus load that’s just biologically unreasonable (note how high the y-values go.)\nOn the other extreme, the priors for model 2 allow so little variation that the individual-level variation is minimal and the only effect that is noticable is the dose dependence we assumed in our model (by setting \\(a_1\\) and \\(b_1\\) to non-zero values).\nModel 3 produces the most reasonable trajectories, with both the dose-effect showing, and some variation between individuals."
  },
  {
    "objectID": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#saving-the-simulated-data",
    "href": "posts/2022-02-22-longitudinal-multilevel-bayes-1/index.html#saving-the-simulated-data",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 1",
    "section": "Saving the simulated data",
    "text": "Saving the simulated data\nFinally, let’s combine all the simulated data into a single list containing all data frames, and save it. I’m also saving the parameters for each model, and sample sizes, so we can quickly retrieve them when we compare with the model estimates.\nWe’ll also save one of the plots (this is mainly so I can show it at the beginning of the tutorial).\n\n#save a plot so we can use it in the blog post\nsimdat &lt;- list(m1 = m1_dat, m2 = m2_dat, m3 = m3_dat, m1pars = m1pars, m2pars = m2pars, m3pars = m3pars, Nlow = Nlow, Nmed = Nmed, Nhigh = Nhigh)\nsaveRDS(simdat, file = filename)\nggsave(file = paste0(\"featured.png\"), p3, dpi = 300, units = \"in\", width = 6, height = 6)\n\nWarning: Removed 53 rows containing missing values (`geom_line()`).\n\n\nWarning: Removed 53 rows containing missing values (`geom_point()`).\n\n\nWe’ll load and use the simdat file in the next parts of the tutorial."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "",
    "text": "I assume you’ve read part 1, otherwise this post won’t make much sense. You might even want to open that first part in a separate tab for quick comparison.\n\nIntroduction\nIn the previous post, I showed a setup where some continuous outcome data (in our case, virus load) was collected over time for several individuals. Those individuals differed by some characteristic (in our case, dose at which they got infected). I specified several models that are useful for both fitting the data, and creating simulations. We’ve done the simulating part, now we’ll start fitting models to that data.\nThe advantage of fitting to simulated data is that we know exactly what model and what parameters produced the data, so we can compare our model estimates to the truth (i.e. the parameter and model settings that were used to generate the data) to see how our models perform. It is always good to do that to get some confidence that your models make sense, before you apply them to real data. For the latter, you don’t know what the “truth” is, so you have to trust whatever your model tells you.\nFitting Bayesian models can take a good bit of time (hours, depending on the settings for the fitting routine). It is generally advisable to place code that takes a while to run into its own R script, run that script and then save the results for further processing. This is in fact what I did here. I wrote 2 separate R scripts, one that does the fitting and one that does the exploration of the model fits. The code shown below comes from those 2 scripts. There is some value in re-coding yourself by copying and pasting the code chunks from this tutorial, but if you just want to get all the code from this post you can find it here and here.\n\n\nR Setup\n\n\n\n\nCitationBibTeX citation:@online{handel2022,\n  author = {Handel, Andreas},\n  title = {Bayesian Analysis of Longitudinal Multilevel Data Using Brms\n    and Rethinking - Part 2},\n  date = {2022-02-23},\n  url = {https://www.andreashandel.com/posts/2022-02-23-longitudinal-multilevel-bayes-2},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHandel, Andreas. 2022. “Bayesian Analysis of Longitudinal\nMultilevel Data Using Brms and Rethinking - Part 2.” February 23,\n2022. https://www.andreashandel.com/posts/2022-02-23-longitudinal-multilevel-bayes-2."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 1",
    "text": "Model 1\nThese lines of code specify the full set of equations for our model 1. Note how closely the R code resembles the mathematical notation. That close match between math and code is one of the nice features of rethinking/ulam. Also note the indexing of the parameters a0 and b0 by id, which indicates that each individual has their own values.\n\n#wide-prior, no-pooling model\n#separate intercept for each individual/id\n#2x(N+1)+1 parameters\nm1 <- alist(\n  # distribution of outcome\n  outcome ~ dnorm(mu, sigma),\n\n  # main equation for time-series trajectory\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n\n  #equations for alpha and beta\n  alpha <-  a0[id] + a1*dose_adj,\n  beta <-  b0[id] + b1*dose_adj,\n\n  #priors\n  a0[id] ~ dnorm(2,  10),\n  b0[id] ~ dnorm(0.5, 10),\n\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)\n\nYou might have noticed that I chose some of the values in the priors to be different than the values we used to generate the simulated data. I don’t want to make things too easy for the fitting routine 😁. We want to have the fitting routine “find” the right answer (parameter estimates). Hopefully, even if we don’t start at the right values, we’ll end up there."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-2",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-2",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 2",
    "text": "Model 2\nNow we’ll set up model 2 exactly as for model 1 but with some of the priors changed as discussed previously. Specifically, the priors now force the individual-level parameters to be essentially all the same. Note that - as you will see below - this model is not a good model, and if one wanted to not allow the \\(a_0\\) and \\(b_0\\) parameters to have any individual level variation, one should just implement and run the model 2 alternative I describe below. We’ll run this model anyway, to just illustration and to see what happens.\n\n#narrow-prior, full-pooling model\n#2x(N+2)+1 parameters\nm2 <- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n  alpha <-  a0[id] + a1*dose_adj,\n  beta <-  b0[id] + b1*dose_adj,\n  a0[id] ~ dnorm(mu_a,  0.0001),\n  b0[id] ~ dnorm(mu_b, 0.0001),\n  mu_a ~ dnorm(2, 1),\n  mu_b ~ dnorm(0.5, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-3",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 3",
    "text": "Model 3\nThis is the same as model 1 but with different values for the priors. These priors are somewhat regularizing and more reasonable. As we’ll see, the results are similar to those from model 1, but the model runs more efficiently and thus faster.\n\n#regularizing prior, partial-pooling model\nm3 <- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n  alpha <-  a0[id] + a1*dose_adj,\n  beta <-  b0[id] + b1*dose_adj,\n  a0[id] ~ dnorm(2,  1),\n  b0[id] ~ dnorm(0.5, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-4",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-4",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 4",
    "text": "Model 4\nThis is our adaptive pooling model. For this model, we specify a few extra distributions.\n\n#adaptive priors, partial-pooling model\n#2x(N+2)+1 parameters\nm4 <- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n  alpha <-  a0[id] + a1*dose_adj,\n  beta <-  b0[id] + b1*dose_adj,\n  a0[id] ~ dnorm(mu_a,  sigma_a),\n  b0[id] ~ dnorm(mu_b, sigma_b),\n  mu_a ~ dnorm(2, 1),\n  mu_b ~ dnorm(0.5, 1),\n  sigma_a ~ cauchy(0, 1),\n  sigma_b ~ cauchy(0, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0, 1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#a-few-model-alternatives",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#a-few-model-alternatives",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "A few model alternatives",
    "text": "A few model alternatives\nThere are a few model alternatives I also want to consider. The first one is a version of model 2 that gets rid of individual-level parameters and instead has only population-level parameters. I discussed this model in part 1 of the tutorial and called it 2a there. Here is the model definition\n\nModel 2a\n\n#full-pooling model, population-level parameters only\n#2+2+1 parameters\nm2a <- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n  alpha <-  a0 + a1*dose_adj,\n  beta <-  b0 + b1*dose_adj,\n  a0 ~ dnorm(2,  0.1),\n  b0 ~ dnorm(0.5, 0.1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0,1)\n)\n\nNote that a0 and b0 are not indexed by id anymore and are now single numbers, instead of \\(N\\) values as before.\n\n\nModel 4a\nAnother model I want to look at is a variant of model 4. This is in fact the same model as model 4, but written in a different way. A potential problem with model 4 and similar models is that parameters inside parameters can lead to inefficient or unreliable numerical results when running your Monte Carlo routine (in our case, this is Stan-powered Hamilton Monte Carlo). It is possible to rewrite the model such that it is the same model, but it looks different in a way that makes the numerics often run better. It turns out for our example, model 4 above runs ok. But it’s a good idea to be aware of the fact that one can re-write models if needed, therefore I decided to include this model alternative here.\nThe above model 4 is called a centered model and the re-write for model 4a is called a non-centered model. The trick is to pull out the parameters from inside the distributions for \\(a_{0,i}\\) and \\(b_{0,i}\\). The non-centered model looks like this:\n\n#adaptive priors, partial-pooling model\n#non-centered\nm4a <- alist(\n  outcome ~ dnorm(mu, sigma),\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n  #rewritten to non-centered\n  alpha <-  mu_a + az[id]*sigma_a + a1*dose_adj,\n  beta  <-  mu_b + bz[id]*sigma_b + b1*dose_adj,\n  #rewritten to non-centered\n  az[id] ~ dnorm(0, 1),\n  bz[id] ~ dnorm(0, 1),\n  mu_a ~ dnorm(2, 1),\n  mu_b ~ dnorm(0.5, 1),\n  sigma_a ~ cauchy(0, 1),\n  sigma_b ~ cauchy(0, 1),\n  a1 ~ dnorm(0.3, 1),\n  b1 ~ dnorm(-0.3, 1),\n  sigma ~ cauchy(0, 1)\n\n  )\n\nAgain, this model is mathematically the same as the original model 4. If this is confusing and doesn’t make sense (it sure wouldn’t to me if I just saw that for the first time 😁), check the Statistical Rethinking book. (And no, I do not get a commission for continuing to point you to the book, and I wish there was a free online version (or a cheap paperback). But it is a great book and if you want to learn this kind of modeling for real, I think it’s worth the investment.)\n\n\nModel 5\nAnother model, which I’m calling model 5 here, is one that does not include the dose effect. That means, parameters \\(a_1\\) and \\(b_1\\) are gone. Otherwise I’m following the setup of model 1. The reason I’m doing this is because on initial fitting of the above models, I could not obtain estimates for the dose parameters I used for the simulation. I noticed strong correlations between posterior distributions of the model parameters. I suspected an issue with non-identifiable parameters (i.e, trying to estimate more parameters than the data supports). To figure out what was going on, I wanted to see how a model without the dose component would perform. It turned out that the main reason things didn’t look right was because I had a typo in the code that generated the data, so what I thought was the generating model actually wasn’t 🤦. A helpful colleague and reader pointed this out. Once I fixed it, things made more sense. But I figured it’s instructive to keep this model anyway.\n\n#no dose effect\n#separate intercept for each individual/id\n#2xN+1 parameters\nm5 <- alist(\n  # distribution of outcome\n  outcome ~ dnorm(mu, sigma),\n\n  # main equation for time-series trajectory\n  mu <- exp(alpha)*log(time) - exp(beta)*time,\n\n  #equations for alpha and beta\n  alpha <-  a0[id],\n  beta <-  b0[id],\n\n  #priors\n  a0[id] ~ dnorm(2,  10),\n  b0[id] ~ dnorm(0.5, 10),\n\n  sigma ~ cauchy(0,1)\n)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#setting-starting-values",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#setting-starting-values",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Setting starting values",
    "text": "Setting starting values\nAny fitting routine needs to start with some parameter values and then from there tries to improve. Stan uses a heuristic way of picking some starting values. Often that works, sometimes it fails initially but then the routine fixes itself, and sometimes it fails all the way. In either case, I find it a good idea to specify starting values, even if they are not strictly needed. And it’s good to know that this is possible and how to do it, just in case you need it at some point. Setting starting values gives you more control, and you also know exactly what should happen when you look at for instance the traceplots of the chains.\n\n## Setting starting values\n#starting values for model 1\nstartm1 = list(a0 = rep(2,Ntot), b0 = rep(0.5,Ntot), a1 = 0.3 , b1 = -0.3, sigma = 1)\n#starting values for model 2\nstartm2 = list(a0 = rep(2,Ntot), b0 = rep(0.5,Ntot), mu_a = 2, mu_b = 1, a1 = 0.3 , b1 = -0.3, sigma = 1)\n#starting values for model 3\nstartm3 = startm1\n#starting values for models 4 and 4a\nstartm4 = list(mu_a = 2, sigma_a = 1, mu_b = 1, sigma_b = 1, a1 = 0.3 , b1 = -0.3, sigma = 1)\nstartm4a = startm4\n#starting values for model 2a\nstartm2a = list(a0 = 2, b0 = 0.5, a1 = 0.3, b1 = -0.3, sigma = 1)\n#starting values for model 5\nstartm5 = list(a0 = rep(2,Ntot), b0 = rep(0.5,Ntot), sigma = 1)\n\n#put different starting values in list\n#need to be in same order as models below\nstartlist = list(startm1,startm2,startm3,startm4,startm2a,startm4,startm5)\n\nNote that we only specify values for the parameters that are directly estimated. Parameters that are built from other parameters (e.g. \\(\\alpha\\) and \\(\\beta\\)) are computed and don’t need starting values.\nFor some more detailed discussion on starting values, see for instance this post by Solomon Kurz. He uses brms in his example, but the same idea applies with any package/fitting routine. He also explains that it is a good idea to set different starting values for each chain. I am not sure if/how this could be done with rethinking, it seems ulam does not support this? But it can be done for brms (and I’m doing it there)."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-fitting",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-fitting",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model fitting",
    "text": "Model fitting\nNow that we specified all models, we can loop through all models and fit them. First, some setup before the actual fitting loop.\n\n#general settings for fitting\n#you might want to adjust based on your computer\nwarmup = 6000\niter = warmup + floor(warmup/2)\nmax_td = 18 #tree depth\nadapt_delta = 0.9999\nchains = 5\ncores  = chains\nseed = 4321\n\n#stick all models into a list\nmodellist = list(m1=m1,m2=m2,m3=m3,m4=m4,m2a=m2a,m4a=m4a,m5=m5)\n# set up a list in which we'll store our results\nfl = vector(mode = \"list\", length = length(modellist))\n\n\n#setting for parameter constraints\nconstraints = list(sigma=\"lower=0\",sigma_a=\"lower=0\",sigma_b=\"lower=0\")\n\nThe first code block defines various settings for the ulam function. Look at the help file for details. Then we place all models into a list, set up an empty list for our fit results, and specify the data needed for fitting. The final command enforces some constraints on parameters. For our model, we want Half-Cauchy distributions for all variance parameters to ensure they are positive. Above, I specified them as Cauchy. There is no direct Half-Cauchy implementation. The way one achieves one is to tell ulam/Stan that the values for those parameters need to be positive. That’s what the constraints line in the code below does.\nLooping over each model and fitting it. In addition to the actual fitting call to ulam, I’m also printing a few messages and storing the model name and the time it took to run. That’s useful for diagnostic. It’s generally a good idea to do short runs/chains until things work, then do a large run to get the actual result. Recording the running time helps decide how long a real run can be and how long it might take.\n\n# fitting models\n#loop over all models and fit them using ulam\nfor (n in 1:length(modellist))\n{\n\n  cat('************** \\n')\n  cat('starting model', names(modellist[n]), '\\n')\n\n  tstart=proc.time(); #capture current time\n\n  #run model fit\n  fl[[n]]$fit <- ulam(flist = modellist[[n]],\n                          data = fitdat,\n                          start=startlist[[n]],\n                          constraints=constraints,\n                          log_lik=TRUE, cmdstan=TRUE,\n                          control=list(adapt_delta=adapt_delta,\n                                       max_treedepth = max_td),\n                          chains=chains, cores = cores,\n                          warmup = warmup, iter = iter,\n                          seed = seed\n  )# end ulam\n\n  #capture time taken for fit\n  tdiff=proc.time()-tstart;\n  runtime_minutes=tdiff[[3]]/60;\n\n  cat('model fit took this many minutes:', runtime_minutes, '\\n')\n  cat('************** \\n')\n\n  #add some more things to the fit object\n  fl[[n]]$runtime = runtime_minutes\n  fl[[n]]$model = names(modellist)[n]\n\n} #end fitting of all models\n\n# saving the list of results so we can use them later\n# the file is too large for GitHub\n# thus I am saving here to a local folder\n# adjust accordingly for your setup\nfilepath = fs::path(\"D:\",\"Dropbox\",\"datafiles\",\"longitudinalbayes\",\"ulamfits\", ext=\"Rds\")\nsaveRDS(fl,filepath)"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 1 and 3",
    "text": "Models 1 and 3\nNow I’ll look at bit more carefully at the different models. We start by comparing fits for models 1 and 3. Those two are essentially the same model, with the only difference being wider priors for the individual-level parameters in model 1. It is worth mentioning that when running the fitting routine, model 1 takes much longer to fit than model 3. With the settings I used, runtimes were 331 versus 57 minutes. The wide priors made the fitting efficiency poor. But let’s see how it impacts the results.\nFirst, we explore priors and posteriors. They are easy to extract from the models using the extract.prior() and extract.samples() functions from rethinking.\n\n#get priors and posteriors for models 1 and 3\nm1prior <- extract.prior(fl[[1]]$fit, n = 1e4)\nm1post <- extract.samples(fl[[1]]$fit, n = 1e4)\n\nm3prior <- extract.prior(fl[[3]]$fit, n = 1e4)\nm3post <- extract.samples(fl[[3]]$fit, n = 1e4)\n\nNow we can plot the distributions. Note that for the individual-level parameters \\(a_0\\) and \\(b_0\\), the plots show the distribution across all individuals. The dashed lines show the priors, the solid the posteriors. Black is model 1, blue is model 3.\n\n#showing density plots for a0\nplot(density(m1prior$a0), xlim = c (-20,20), ylim = c(0,2), lty=2)\nlines(density(m1post$a0), lty=1)\nlines(density(m3prior$a0), col = \"blue\", lty=2)\nlines(density(m3post$a0), col = \"blue\", lty=1)\n\n\n\n#showing density plots for b0\nplot(density(m1prior$b0), xlim = c (-20,20), ylim = c(0,2), lty=2)\nlines(density(m1post$b0), lty=1)\nlines(density(m3prior$b0), col = \"blue\", lty=2)\nlines(density(m3post$b0), col = \"blue\", lty=1)\n\n\n\n#showing density plots for a1\nplot(density(m1prior$a1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m1post$a1), lty=1)\nlines(density(m3prior$a1), col = \"blue\", lty=2)\nlines(density(m3post$a1), col = \"blue\", lty=1)\n\n\n\n#showing density plots for b1\nplot(density(m1prior$b1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m1post$b1), lty=1)\nlines(density(m3prior$b1), col = \"blue\", lty=2)\nlines(density(m3post$b1), col = \"blue\", lty=1)\n\n\n\n\nWe set up the models to have wider \\(a_0\\) and \\(b_0\\) priors for model 1, and the same priors for the \\(a_1\\) and \\(b_1\\) parameters. The dashed lines in the figures show that. Looking at the posteriors, we find that changing the priors has an impact, especially for \\(a_1\\) and \\(b_1\\). Not only does model 3 lead to more peaked posteriors, they are also not centered at the same values, especially for \\(b_1\\). I don’t think that’s a good sign. We want the data to dominate the results, the priors should just be there to ensure the models explore the right parameter space efficiently and don’t do anything crazy. The fact that the same model, started with different priors, leads to different posterior distributions is in my opinion concerning. It could be that with more sampling, the posteriors might get closer. Or it might suggest that we are overfitting and have non-identifiability problems here.\nOne way to check that further is to look at potential correlations between parameter posterior distributions, e.g., using a pairs() plot as shown above. Here are such plots for the parameters associated with \\(\\alpha\\) for model 1. I only plot a few for each dose, otherwise the plots won’t be legible inside this html document. But you can try for yourself, if you make the plot large enough you can fit them all. You can also make plots for model 3 and for the \\(b\\) parameters, those look very similar.\n\n# all \"a\" parameters - too big to show\n#pairs(fl[[1]]$fit, pars = c(\"a0\",\"a1\"))\n# a few parameters for each dose\n#low dose\npairs(fl[[1]]$fit, pars = c(\"a0[1]\",\"a0[2]\",\"a0[3]\",\"a0[4]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#medium dose\npairs(fl[[1]]$fit, pars = c(\"a0[8]\",\"a0[9]\",\"a0[10]\",\"a0[11]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#high dose\npairs(fl[[1]]$fit, pars = c(\"a0[16]\",\"a0[17]\",\"a0[18]\",\"a0[19]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n\nRecall that we set up the model such that dose is non-zero for low and high dose, while for the intermediate dose it drops out of the model. What seems to happen is that if the dose effect, i.e., \\(a_1\\), is present, there is a strong correlation among that parameter and the individual-level parameters for that dose. That part makes some sense to me. Both \\(a_{0,i}\\) or \\(a_1\\) can change \\(\\alpha\\) and thus the trajectory. If one is low, the other might be high, and the reverse, leading to similarly good fits.\nBecause every \\(a_{0,i}\\) is correlated with \\(a_1\\) in this way, this also leads to correlations among the \\(a_{0,i}\\) values. I am surprised that this is an essentially perfect correlation. Maybe, if I thought about it harder and/or did the math, it would be clear that it needs to be that way. But I haven’t yet, so for now I’m just taking it as given 😁. Overall, this is another sign of that we might be overfitting and have non-identifiability problems, i.e. combinations for different values of \\(a_{0,i}\\) and \\(a_1\\) can lead to more or less the same results (everything I write here of course also holds for the \\(b_{0,i}\\) and \\(b_1\\) parameters).\nLet’s move on and now look at the posterior distributions in numerical form. For that, I use the precis function from rethinking. Instead of printing all the \\(N\\) different values of \\(a_{0,i}\\) and \\(b_{0,i}\\), I compute their means. If you want to see them all, change to depth=2 in the precis function.\n\n# Model 1\na0mean = mean(precis(fl[[1]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(precis(fl[[1]]$fit,depth=2,\"b0\")$mean)\nprint(precis(fl[[1]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n       mean    sd  5.5% 94.5% n_eff Rhat4\na1     0.22 0.736 -0.96  1.40  2600     1\nb1    -0.21 0.745 -1.40  0.98  2384     1\nsigma  1.06 0.052  0.98  1.15 16450     1\n\nprint(c(a0mean,b0mean))\n\n[1] 2.961711 1.005527\n\n# Model 3\na0mean = mean(precis(fl[[3]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(precis(fl[[3]]$fit,depth=2,\"b0\")$mean)\nprint(precis(fl[[3]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n        mean    sd   5.5% 94.5% n_eff Rhat4\na1     0.141 0.110 -0.033 0.316  3005     1\nb1    -0.082 0.109 -0.256 0.093  2776     1\nsigma  1.062 0.052  0.985 1.148 15663     1\n\nprint(c(a0mean,b0mean))\n\n[1] 2.9757761 0.9810916\n\n\nThe models seem to have converged ok, based on Rhat values of 1. Some parameters sampled better than others, as can be seen by the varying n_eff values. I used 5 chains of 3000 post-warmup samples for each chain, so the actual samples are 15000. If n_eff is lower than that, it means the sampling was not efficient, more means it worked very well (see e.g. Statistical Rethinking why it’s possible to get more effective samples than actual samples.)\nWe find that estimates for \\(a_{0}\\), \\(b_0\\) and \\(\\sigma\\) are similar, \\(a_1\\) and \\(b_1\\) differ more.\nAgain, note that the only thing we changed between models 1 and 3 are to make the priors for the \\(a_{0,i}\\) and \\(b_{0,i}\\) parameters tighter. It didn’t seem to impact estimates for those parameters, but it did impact the estimates for the posterior distributions of parameters \\(a_1\\) and \\(b_1\\). The numbers are consistent with the posterior distribution figures above."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#comparing-model-estimates-with-the-truth",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#comparing-model-estimates-with-the-truth",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Comparing model estimates with the truth",
    "text": "Comparing model estimates with the truth\nWe know the “truth” here, i.e., the actual values of the parameters which we used to created the simulated data. To generate the data, we used these parameter values: \\(\\sigma =\\) 1, \\(\\mu_a =\\) 3, \\(\\mu_b =\\) 1, \\(a_1 =\\) 0.1, \\(b_1 =\\) -0.1. We also said that our main scientific question is if there is a dose effect, i.e. non-zero \\(a_1\\) and \\(b_1\\).\nThe models find estimates of \\(\\mu_a\\), \\(\\mu_b\\) and \\(\\sigma\\) that are close to what we used. The estimates for \\(a_1\\) and \\(b_1\\) are not that great. That’s especially true for model 1. With these models, we aren’t able to convincingly recover the parameters used to generate the data. I’m not sure if increasing the sampling (longer or more chains) would help. Both models, especially model 1, already took quite a while to run. Thus I’m not too keen to try it with even more samples. As we’ll see below, alternative models do better."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 2 and 2a",
    "text": "Models 2 and 2a\nNext, let’s look at models 2 and 2a. The estimates should be similar since the two models are conceptually pretty much the same.\n\n# Compare models 2 and 2a\n# first we compute the mean across individuals for model 2\na0mean = mean(precis(fl[[2]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(precis(fl[[2]]$fit,depth=2,\"b0\")$mean)\n\n#rest of model 2\nprint(precis(fl[[2]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n        mean     sd  5.5%  94.5% n_eff Rhat4\nmu_a   2.992 0.0271  2.94  3.026    16   1.4\nmu_b   0.999 0.0236  0.96  1.029    12   1.4\na1     0.095 0.0097  0.08  0.111   101   1.1\nb1    -0.097 0.0082 -0.11 -0.083   147   1.1\nsigma  6.867 0.2838  6.46  7.334   244   1.0\n\nprint(c(a0mean,b0mean))\n\n[1] 2.991509 0.998633\n\n#model 2a\nprint(precis(fl[[5]]$fit,depth=1),digits = 2)\n\n        mean     sd  5.5%  94.5% n_eff Rhat4\na0     2.920 0.0236  2.88  2.957  5343     1\nb0     0.938 0.0204  0.90  0.971  5651     1\na1     0.107 0.0109  0.09  0.125  6123     1\nb1    -0.098 0.0094 -0.11 -0.084  6787     1\nsigma  7.003 0.3207  6.51  7.536  7308     1\n\n\nThe first thing to note is that model 2 performs awfully, with Rhat values >1 and very low effective sample size n_eff. This indicates that this model doesn’t work well for the data. Whenever you see diagnostics like that, you should not take the estimated values seriously. However, let’s pretend for a moment that we can take them seriously. Here is what we find.\nFirst, both models produce similar estimates. Since model 2a is simpler and doesn’t have that strange feature of us enforcing a very tight distribution for the \\(a_{0,i}\\) and \\(b_{0,i}\\) parameters, it actually samples much better, see the higher n_eff numbers. It also runs much faster, 1.3 minutes compared to 30 minutes for model 2.\nBoth models do a very poor job estimating \\(\\sigma\\). That’s because we don’t allow the models to have the flexibility needed to fit the data, so it has to account for any variation between its estimated mean trajectory and the real data by making \\(\\sigma\\) large.\nSince the models are more constrained compared to models 1 and 3, they produce estimates for \\(a_1\\) and \\(b_1\\) that are tighter. However, these estimates are over-confident. overall these models underfitting and are not good. We can for instance look at this using the compare function:\n\ncompare(fl[[1]]$fit,fl[[3]]$fit,fl[[2]]$fit,fl[[5]]$fit)\n\n                 WAIC       SE        dWAIC        dSE     pWAIC        weight\nfl[[1]]$fit  832.9455 23.39942   0.00000000         NA 43.718111  5.067138e-01\nfl[[3]]$fit  832.9992 23.42829   0.05371325  0.4274202 43.737196  4.932862e-01\nfl[[2]]$fit 1778.3776 44.18971 945.43210978 45.5782040  9.698475 2.551459e-206\nfl[[5]]$fit 1788.1450 45.28523 955.19949836 47.0533571 10.466546 1.931199e-208\n\n\nI’m not going to discuss things in detail (see Statistical Rethinking), but a lower WAIC means a model that fits best in the sense that it strikes a good balance between fitting the data while not overfitting. As you can see, models 1 and 3 perform very similarly and models 2 and 2a are much worse.\nThe larger WAIC indicates either strong overfitting or underfitting. In this case, it’s underfitting. The models are not flexible enough to capture the individual-level variation. You’ll see that clearly in the plots shown further below. If we did indeed not want to account for individual-level variation, we should go with a model that simply doesn’t include it, i.e. model 2a. The contrived model 2 with very narrow priors is just a bad model, and I’m really only exploring it here for demonstration purposes."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 4 and 4a",
    "text": "Models 4 and 4a\nNow we get to the models we really care about. When I set up the models, I suggested that model 4 was similar to models 1-3, but with priors adaptively chosen. That didn’t apply during data generation/simulation since in that step, we always need to manually choose values. But during the fitting/estimation, we should expect that model 4 chooses priors in a smart way, such that it is better than the models where we fixed the priors. Let’s see what model 4 produces. We also look at model 4a, which is exactly the same model, just rewritten to potentially make the numerical fitting routine more efficient.\nLet’s start with prior and posterior plots.\n\n#get priors and posteriors for models 4 and 4a\nm4prior <- extract.prior(fl[[4]]$fit, n = 1e4)\nm4post <- extract.samples(fl[[4]]$fit, n = 1e4)\n\nm4aprior <- extract.prior(fl[[6]]$fit, n = 1e4)\nm4apost <- extract.samples(fl[[6]]$fit, n = 1e4)\n\nAs before, the dashed lines show the priors, the solid the posteriors. Black is model 4, blue is model 4a.\n\n#showing density plots for a0\nplot(density(m4prior$mu_a), xlim = c (-10,10), ylim = c(0,2), lty=2)\nlines(density(m4post$mu_a), lty=1)\nlines(density(m4aprior$mu_a), col = \"blue\", lty=2)\nlines(density(m4apost$mu_a), col = \"blue\", lty=1)\n\n\n\n#showing density plots for b0\nplot(density(m4prior$mu_b), xlim = c (-10,10), ylim = c(0,2), lty=2)\nlines(density(m4post$mu_b), lty=1)\nlines(density(m4aprior$mu_b), col = \"blue\", lty=2)\nlines(density(m4apost$mu_b), col = \"blue\", lty=1)\n\n\n\n#showing density plots for a1\nplot(density(m4prior$a1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m4post$a1), lty=1)\nlines(density(m4aprior$a1), col = \"blue\", lty=2)\nlines(density(m4apost$a1), col = \"blue\", lty=1)\n\n\n\n#showing density plots for b1\nplot(density(m4prior$b1), xlim = c (-3,3), ylim = c(0,2), lty=2)\nlines(density(m4post$b1), lty=1)\nlines(density(m4aprior$b1), col = \"blue\", lty=2)\nlines(density(m4apost$b1), col = \"blue\", lty=1)\n\n\n\n\nAs you can see, up to numerical sampling variability, the results for models 4 and 4a are pretty much the same. That should be expected, since they are the same model, just reformulated for potential efficiency. Also, the posterior distributions are much narrower than the priors. I think that’s a good sign as well, it indicates the data mostly informed the posterior distributions, the priors just helped to keep things efficient.\nWe can also explore pair plots again, showing them here for model 4.\n\n# a few parameters for each dose\n#low dose\npairs(fl[[4]]$fit, pars = c(\"a0[1]\",\"a0[2]\",\"a0[3]\",\"a0[4]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#medium dose\npairs(fl[[4]]$fit, pars = c(\"a0[8]\",\"a0[9]\",\"a0[10]\",\"a0[11]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#high dose\npairs(fl[[4]]$fit, pars = c(\"a0[16]\",\"a0[17]\",\"a0[18]\",\"a0[19]\",\"a1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n# mean of a0 prior\npairs(fl[[4]]$fit, pars = c(\"mu_a\",\"mu_b\",\"a1\",\"b1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#saving one plot so I can use as featured image\npng(filename = \"featured.png\", width = 6, height = 6, units = \"in\", res = 300)\npairs(fl[[4]]$fit, pars = c(\"mu_a\",\"mu_b\",\"a1\",\"b1\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\ndev.off()\n\npng \n  2 \n\n\nWe still see the same issue with correlations among the parameters for dose levels where \\(a_1\\) is acting, though the correlations are not as extreme. They are also minor between the overall estimates for the mean of the \\(a_0\\) and \\(b_0\\) parameters and \\(a_1\\) and \\(b_1\\). I interpret this to mean that the adaptive sampling helped somewhat with the identifiability and overfitting problem, though it seems to not fully resolve it. The fact that we gave each individual their own \\(a_{0,i}\\) and \\(b_{0,i}\\) values allows those parameters to still “absorb” some of the dose-dependent signal in \\(a_1\\) and \\(b_1\\).\nWe can also again look at the numerical outputs from the precis function.\n\n# model 4\nprint(precis(fl[[4]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n          mean    sd   5.5%  94.5% n_eff Rhat4\nmu_a     2.987 0.020  2.956  3.018 14437     1\nmu_b     0.986 0.025  0.946  1.026 14777     1\nsigma_a  0.093 0.016  0.071  0.121 10722     1\nsigma_b  0.119 0.020  0.092  0.153 12199     1\na1       0.086 0.010  0.069  0.102  2170     1\nb1      -0.107 0.013 -0.128 -0.085  2230     1\nsigma    1.062 0.052  0.983  1.148 13477     1\n\n# model 4a\nprint(precis(fl[[6]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n          mean    sd   5.5%  94.5% n_eff Rhat4\nmu_a     2.987 0.019  2.956  3.017  3803     1\nmu_b     0.986 0.025  0.946  1.026  3259     1\nsigma_a  0.093 0.016  0.072  0.121  4376     1\nsigma_b  0.119 0.020  0.092  0.153  4285     1\na1       0.086 0.010  0.069  0.103  4267     1\nb1      -0.106 0.013 -0.126 -0.084  3861     1\nsigma    1.062 0.051  0.985  1.148 10624     1\n\n\nThe numerics confirm that the two models lead to essentially the same results. The values for n_eff differ between models, though neither model is consistently larger. This suggests that each model formulation had advantages in sampling for some of the parameters.\nIn terms of run times, there wasn’t much difference, with 7 minutes for model 4 versus 10 minutes for model 4a (much better than models 1-3).\nIf we compare the parameter estimates with the true values and those found for models 1 and 3 above, we find that again the true \\(\\mu_a\\), \\(\\mu_b\\) and \\(\\sigma\\) are estimated fairly well. Estimates for \\(a_1\\) and \\(b_1\\) are now also pretty good, and the credible intervals are less wide.\nNow let’s briefly run the compare function too and include model 3 as well. In addition to using WAIC for comparison, I’m also including PSIS. Read about it in the Statistical Rethinking book. One advantage of PSIS is that it gives warnings if the estimates might not be reliable. You see that this happens here.\n\ncompare(fl[[3]]$fit,fl[[4]]$fit,fl[[6]]$fit, func = WAIC)\n\n                WAIC       SE     dWAIC       dSE    pWAIC    weight\nfl[[4]]$fit 831.0751 23.40197 0.0000000        NA 42.58290 0.4659494\nfl[[6]]$fit 831.6133 23.39119 0.5382072 0.3058545 42.76359 0.3560152\nfl[[3]]$fit 832.9992 23.42829 1.9241902 2.5641934 43.73720 0.1780353\n\ncompare(fl[[3]]$fit,fl[[4]]$fit,fl[[6]]$fit, func = PSIS)\n\nSome Pareto k values are high (>0.5). Set pointwise=TRUE to inspect individual points.\nSome Pareto k values are high (>0.5). Set pointwise=TRUE to inspect individual points.\nSome Pareto k values are high (>0.5). Set pointwise=TRUE to inspect individual points.\n\n\n                PSIS       SE     dPSIS       dSE    pPSIS    weight\nfl[[4]]$fit 839.5114 24.33096 0.0000000        NA 46.80107 0.4616132\nfl[[6]]$fit 839.8745 24.30905 0.3630563 0.5782962 46.89419 0.3849830\nfl[[3]]$fit 841.7147 24.33321 2.2033083 2.7044971 48.09493 0.1534037\n\n\nWe do find that model 4/4a performs a bit better, but not by much. Note that model 4/4a has more actual parameters, but the effective parameters (which is described by pWAIC) is a bit smaller.\nOverall, this suggests that the adaptive pooling approach helped to estimate results more precisely and efficiently and is the best of the models."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 5",
    "text": "Model 5\nAs stated above, due to a typo in my code, the above models, including 4/4a, initially produced estimates for \\(a_1\\) and \\(b_1\\) that were not close to those (I thought I) used to generate the data. Based on the pair plots, I suspected non-identifiability issues and wanted to explore what would happen if I removed the dose.\nIf we now look at the pairs plots, maybe not surprisingly, the correlations between individual \\(a_0\\) parameters are gone.\n\n# a few parameters for each dose\n#low dose\npairs(fl[[7]]$fit, pars = c(\"a0[1]\",\"a0[2]\",\"a0[3]\",\"a0[4]\",\"a0[5]\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#medium dose\npairs(fl[[7]]$fit, pars = c(\"a0[8]\",\"a0[9]\",\"a0[10]\",\"a0[11]\",\"a0[12]\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n#high dose\npairs(fl[[7]]$fit, pars = c(\"a0[16]\",\"a0[17]\",\"a0[18]\",\"a0[19]\",\"a0[20]\"))\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\nWarning in par(usr): argument 1 does not name a graphical parameter\n\n\n\n\n\nThe model estimates the parameters reasonably well\n\na0mean = mean(precis(fl[[7]]$fit,depth=2,\"a0\")$mean)\nb0mean = mean(precis(fl[[7]]$fit,depth=2,\"b0\")$mean)\nprint(precis(fl[[7]]$fit,depth=1),digits = 2)\n\n48 vector or matrix parameters hidden. Use depth=2 to show them.\n\n\n      mean    sd 5.5% 94.5% n_eff Rhat4\nsigma  1.1 0.052 0.98   1.1 16614     1\n\nprint(c(a0mean,b0mean))\n\n[1] 3.0031331 0.9655931\n\n\nIt doesn’t seem quite as good as the previous models.\n\ncompare(fl[[3]]$fit,fl[[4]]$fit,fl[[7]]$fit)\n\n                WAIC       SE    dWAIC      dSE    pWAIC    weight\nfl[[4]]$fit 831.0751 23.40197 0.000000       NA 42.58290 0.5705560\nfl[[3]]$fit 832.9992 23.42829 1.924190 2.564193 43.73720 0.2180046\nfl[[7]]$fit 833.0604 23.39781 1.985346 2.445539 43.82551 0.2114394\n\n\nAnd of course the main problem with this model: It can’t answer any question about the role of dose, since we removed that component from the model! So while ok to explore, scientifically not useful since it can’t help us address the question we want to answer regarding the potential impact of dose."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-1-and-3-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 1 and 3",
    "text": "Models 1 and 3\n\nplot(plotlist[[1]])\n\n\n\nplot(plotlist[[3]])\n\n\n\n\nDespite differences in estimates for the dose related parameters, the predicted outcomes of the models are very similar. In fact, it’s hard to tell any difference by just looking at the plots (but they are slightly different, I checked). Thus, despite the inability of these models to provide precises estimates of all the parameter values, the predictions/outcomes are fine, they fit the data well."
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-2-and-2a-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 2 and 2a",
    "text": "Models 2 and 2a\nFor models 2 and 2a, recall that the only variation is for dose, we didn’t allow variation among individuals. That’s reflected in the plots. The credible intervals based on parameters are tight, but because the variability, \\(\\sigma\\), had to account for all the differences, the prediction intervals are very wide.\n\nplot(plotlist[[2]])\n\n\n\nplot(plotlist[[5]])"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a-1",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#models-4-and-4a-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Models 4 and 4a",
    "text": "Models 4 and 4a\nThese models look good again, and very similar to models 1 and 3.\n\nplot(plotlist[[4]])\n\n\n\nplot(plotlist[[6]])"
  },
  {
    "objectID": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-2",
    "href": "posts/2022-02-23-longitudinal-multilevel-bayes-2/index.html#model-5-2",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 2",
    "section": "Model 5",
    "text": "Model 5\nThe model fits look fine, suggesting that one parameter for each individual is enough to capture the data. That’s not surprising. However, this of course does not allow us to ask and answer any scientific questions about the role of dose.\n\nplot(plotlist[[7]])\n\n\n\n\nSo overall, the figures make sense It seems that if we want to do prediction, all models that include individual variability are fine, models 2/2a are not great. If we wanted to estimate the model parameters, specifically \\(a_1\\) and \\(b_1\\), models 1 and 3 and of course 5 don’t work. In that case, model 2/2a works ok. I consider model 4/4a the best one overall.\nFor another example and more discussion of estimation versus prediction, see e.g. Section 6.1. in Statistical Rethinking, as well as 9.5.4 (all referring to the 2nd edition of the book)."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "",
    "text": "This is part 3 of a tutorial illustrating how one can use the brms and rethinking R packages to perform a Bayesian analysis of longitudinal data using a multilevel/hierarchical/mixed-effects setup.\nI assume you’ve read both part 1, and part 2 otherwise this post won’t make much sense.\n\n\n\nCitationBibTeX citation:@online{handel2022,\n  author = {Handel, Andreas},\n  title = {Bayesian Analysis of Longitudinal Multilevel Data Using Brms\n    and Rethinking - Part 3},\n  date = {2022-02-24},\n  url = {https://www.andreashandel.com/posts/2022-02-24-longitudinal-multilevel-bayes-3},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHandel, Andreas. 2022. “Bayesian Analysis of Longitudinal\nMultilevel Data Using Brms and Rethinking - Part 3.” February 24,\n2022. https://www.andreashandel.com/posts/2022-02-24-longitudinal-multilevel-bayes-3."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-1",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 1",
    "text": "Model 1\nThis is one of the models with individual-level and dose-level effects, all priors fixed. This model has \\(2N+2+1\\) parameters. \\(N\\) each for the individual-level intercepts for \\(\\alpha\\) and \\(\\beta\\) (the \\(a_{0,i}\\) and \\(b_{0,i}\\) parameters), the two dose-level parameters \\(a_1\\) and \\(b_1\\), and 1 overall deviation, \\(\\sigma\\) for the outcome distribution.\n\n#no-pooling model\n#separate intercept for each individual/id\n#2x(N+1)+1 parameters\nm1eqs <- bf(  #main equation for time-series trajectory\n          outcome ~  exp(alpha)*log(time) - exp(beta)*time,\n          #equations for alpha and beta\n          alpha ~ 0 + id + dose_adj,\n          beta  ~ 0 + id + dose_adj,\n          nl = TRUE)\n\nm1priors <- c(#assign priors to all coefficients related to both id and dose_adj for alpha and beta\n              prior(normal(2, 10),  class = \"b\",  nlpar = \"alpha\"),\n              prior(normal(0.5, 10),  class = \"b\",  nlpar = \"beta\"),\n              #change the dose_adj priors to something different than the id priors\n              prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n              prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n              prior(cauchy(0,1), class = \"sigma\") )\n\nNotice how this notation in brms looks quite a bit different from the mathematical equations or the ulam implementation. That’s a part I don’t particularly like about brms, the very condensed formula notation. It takes time getting used to and it always requires extra checking to ensure the model implemented in code corresponds to the mathematical model. One can check by looking at the priors and make sure they look as expected. We’ll do that below after we fit."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 2a",
    "text": "Model 2a\nThis is the easiest model, with only population level effects for intercept and dose, so only 2+2+1 parameters.\n\n#full-pooling model\n#2+2+1 parameters\nm2aeqs <- bf(  #main equation for time-series trajectory\n  outcome ~ exp(alpha)*log(time) - exp(beta)*time,\n  #equations for alpha and beta\n  alpha ~ 1 + dose_adj,\n  beta  ~  1 + dose_adj,\n  nl = TRUE)\n\nm2apriors <- c(prior(normal(2, 2),  class = \"b\",  nlpar = \"alpha\", coef = \"Intercept\"),\n              prior(normal(0.5, 2),  class = \"b\",  nlpar = \"beta\", coef = \"Intercept\"),\n              prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n              prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n              prior(cauchy(0,1), class = \"sigma\")  )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-3",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 3",
    "text": "Model 3\nThis is the same as model 1 but with different values for the priors.\n\n#same as model 1 but regularizing priors\nm3eqs <- m1eqs\n\nm3priors <- c(#assign priors to all coefficients related to id and dose_adj for alpha and beta\n  prior(normal(2, 1),  class = \"b\",  nlpar = \"alpha\"),\n  prior(normal(0.5, 1),  class = \"b\",  nlpar = \"beta\"),\n  #change the dose_adj priors to something different than the id priors\n  prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n  prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n  prior(cauchy(0,1), class = \"sigma\") )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 4",
    "text": "Model 4\nThis is the adaptive-pooling multi-level model where priors are estimated. Here we have for each main parameter (\\(\\alpha\\) and \\(\\beta\\)) an overall mean and standard deviation, and N individual intercepts, so 2 times 1+1+N. And of course we still have the 2 dose-related parameters and the overall standard deviation, so a total of 2*(1+1+N)+2+1 parameters.\n\n#adaptive prior, partial-pooling model\nm4eqs <- bf(  #main equation for time-series trajectory\n  outcome ~ exp(alpha)*log(time) - exp(beta)*time,\n  #equations for alpha and beta\n  alpha ~  (1|id) + dose_adj,\n  beta  ~  (1|id) + dose_adj,\n  nl = TRUE)\n\nm4priors <- c(prior(normal(2, 1),  class = \"b\",  nlpar = \"alpha\", coef = \"Intercept\"),\n              prior(normal(0.5, 1),  class = \"b\",  nlpar = \"beta\", coef = \"Intercept\"),\n              prior(normal(0.3, 1),   class = \"b\",  nlpar = \"alpha\", coef = \"dose_adj\"),\n              prior(normal(-0.3, 1),  class = \"b\",  nlpar = \"beta\", coef = \"dose_adj\"),\n              prior(cauchy(0,1), class = \"sd\", nlpar = \"alpha\"),\n              prior(cauchy(0,1), class = \"sd\", nlpar = \"beta\"),\n              prior(cauchy(0,1), class = \"sigma\")  )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#combine-models",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#combine-models",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Combine models",
    "text": "Combine models\nTo make our lives easier below, we combine all models and priors into lists.\n\n#stick all models into a list\nmodellist = list(m1=m1eqs,m2a=m2aeqs,m3=m3eqs,m4=m4eqs)\n#also make list for priors\npriorlist = list(m1priors=m1priors,m2apriors=m2apriors,m3priors=m3priors,m4priors=m4priors)\n# set up a list in which we'll store our results\nfl = vector(mode = \"list\", length = length(modellist))"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#fitting-setup",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#fitting-setup",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Fitting setup",
    "text": "Fitting setup\nWe define some general values for the fitting. Since the starting values depend on number of chains, we need to do this setup first.\n\n#general settings for fitting\n#you might want to adjust based on your computer\nwarmup = 6000\niter = warmup + floor(warmup/2)\nmax_td = 18 #tree depth\nadapt_delta = 0.9999\nchains = 5\ncores  = chains\nseed = 1234"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#setting-starting-values",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#setting-starting-values",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Setting starting values",
    "text": "Setting starting values\nWe’ll again set starting values, as we did for ulam/rethinking. Note that brms needs them in a somewhat different form, namely as list of lists for each model, one list for each chain.\nI set different values for each chain, so I can check that each chain ends up at the same posterior. This is inspired by this post by Solomon Kurz, though I keep it simpler and just use the jitter function.\nNote that this approach not only jitters (adds noise/variation) between chains, but also between the individual-level parameters for each chain. That’s fine for our purpose, it might even be beneficial.\n\n## Setting starting values\n#starting values for model 1\nstartm1 = list(a0 = rep(2,Ntot), b0 = rep(0.5,Ntot), a1 = 0.5 , b1 = -0.5, sigma = 1)\n#starting values for model 2a\nstartm2a = list(a0 = 2, b0 = 0.5, a1 = 0.5 , b1 = 0.5, sigma = 1)\n#starting values for model 3\nstartm3 = startm1\n#starting values for models 4\nstartm4 = list(mu_a = 2, sigma_a = 1, mu_b = 0, sigma_b = 1, a1 = 0.5 , b1 = -0.5, sigma = 1)\n#put different starting values in list\n#need to be in same order as models below\n#one list for each chain, thus a 3-leveled list structure\n#for each chain, we add jitter so they start at different values\nstartlist = list( rep(list(lapply(startm1,jitter,10)),chains),\n                  rep(list(lapply(startm2a,jitter,10)),chains),\n                  rep(list(lapply(startm3,jitter,10)),chains),\n                  rep(list(lapply(startm4,jitter,10)),chains)\n                  )"
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-fitting",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-fitting",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model fitting",
    "text": "Model fitting\nWe’ll use the same strategy to loop though all models and fit them. The fitting code looks very similar to the previous one for rethinking/ulam, only now the fitting is done calling the brm function.\n\n# fitting models\n#loop over all models and fit them using ulam\nfor (n in 1:length(modellist))\n{\n\n  cat('************** \\n')\n  cat('starting model', names(modellist[n]), '\\n')\n\n  tstart=proc.time(); #capture current time\n\n  fl[[n]]$fit <- brm(formula = modellist[[n]],\n                   data = fitdat,\n                   family = gaussian(),\n                   prior = priorlist[[n]],\n                   init = startlist[[n]],\n                   control=list(adapt_delta=adapt_delta, max_treedepth = max_td),\n                   sample_prior = TRUE,\n                   chains=chains, cores = cores,\n                   warmup = warmup, iter = iter,\n                   seed = seed,\n                   backend = \"cmdstanr\"\n  )# end brm statement\n\n  tend=proc.time(); #capture current time\n  tdiff=tend-tstart;\n  runtime_minutes=tdiff[[3]]/60;\n\n  cat('model fit took this many minutes:', runtime_minutes, '\\n')\n  cat('************** \\n')\n\n  #add some more things to the fit object\n  fl[[n]]$runtime = runtime_minutes\n  fl[[n]]$model = names(modellist)[n]\n}\n# saving the results so we can use them later\nfilepath = fs::path(\"D:\",\"Dropbox\",\"datafiles\",\"longitudinalbayes\",\"brmsfits\", ext=\"Rds\")\nsaveRDS(fl,filepath)\n\nYou’ll likely find that model 1 takes the longest, the other ones run faster. You can check the runtime for each model by looking at fl[[n]]$runtime. It’s useful to first run with few iterations (100s instead of 1000s), make sure everything works in principle, then do a “final” long run with longer chains."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#models-1-and-3",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#models-1-and-3",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Models 1 and 3",
    "text": "Models 1 and 3\nLet’s explore those two models first. Recall that they are the same, apart from the prior definitions. As previously, the wider priors for model 1 make it less efficient. With the settings I used, run times were 417 minutes for model 1 versus 61 minutes for model 3.\nLet’s see if the priors impact the results, i.e. the posterior distributions. We can actually do that by looking briefly at the summaries for both fits.\n\n#save some typing\nfit1 <- fl[[1]]$fit\nfit3 <- fl[[3]]$fit\nsummary(fit1)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome ~ exp(alpha) * log(time) - exp(beta) * time \n         alpha ~ 0 + id + dose_adj\n         beta ~ 0 + id + dose_adj\n   Data: fitdat (Number of observations: 264) \n  Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;\n         total post-warmup draws = 15000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_id1          3.50      1.67     0.26     6.80 1.00     2840     4734\nalpha_id2          3.46      1.67     0.21     6.75 1.00     2839     4794\nalpha_id3          3.26      1.67     0.02     6.56 1.00     2839     4760\nalpha_id4          3.19      1.67    -0.05     6.49 1.00     2841     4760\nalpha_id5          3.24      1.67    -0.01     6.53 1.00     2839     4760\nalpha_id6          3.33      1.67     0.08     6.62 1.00     2839     4760\nalpha_id7          3.28      1.67     0.03     6.58 1.00     2841     4796\nalpha_id8          2.98      0.02     2.95     3.01 1.00    17885    10342\nalpha_id9          2.91      0.02     2.88     2.94 1.00    17315    10942\nalpha_id10         2.98      0.02     2.95     3.01 1.00    17656    10966\nalpha_id11         2.94      0.02     2.91     2.97 1.00    18085    10133\nalpha_id12         2.84      0.02     2.81     2.88 1.00    17692    11631\nalpha_id13         2.97      0.02     2.94     3.00 1.00    18451    10345\nalpha_id14         3.09      0.01     3.06     3.12 1.00    18387    10003\nalpha_id15         2.95      0.02     2.91     2.98 1.00    17682    10963\nalpha_id16         2.77      1.67    -0.52     6.01 1.00     2839     4781\nalpha_id17         2.54      1.67    -0.76     5.79 1.00     2840     4750\nalpha_id18         2.73      1.67    -0.57     5.97 1.00     2839     4798\nalpha_id19         2.76      1.67    -0.53     6.01 1.00     2839     4820\nalpha_id20         2.73      1.67    -0.56     5.98 1.00     2840     4771\nalpha_id21         2.71      1.67    -0.59     5.96 1.00     2840     4751\nalpha_id22         2.66      1.67    -0.64     5.91 1.00     2839     4807\nalpha_id23         2.65      1.67    -0.64     5.90 1.00     2840     4764\nalpha_id24         2.59      1.67    -0.70     5.84 1.00     2838     4762\nalpha_dose_adj     0.22      0.73    -1.19     1.65 1.00     2839     4785\nbeta_id1           0.75      1.71    -2.66     4.10 1.00     2420     4179\nbeta_id2           0.65      1.71    -2.76     4.01 1.00     2420     4181\nbeta_id3           0.70      1.71    -2.72     4.04 1.00     2419     4158\nbeta_id4           0.71      1.71    -2.70     4.06 1.00     2419     4155\nbeta_id5           0.93      1.71    -2.48     4.28 1.00     2418     4167\nbeta_id6           0.68      1.71    -2.73     4.03 1.00     2419     4175\nbeta_id7           0.77      1.71    -2.64     4.13 1.00     2419     4155\nbeta_id8           1.01      0.01     0.99     1.04 1.00    16977    10323\nbeta_id9           0.91      0.02     0.88     0.94 1.00    17374    11382\nbeta_id10          0.98      0.01     0.96     1.01 1.00    18009    10155\nbeta_id11          1.15      0.01     1.13     1.18 1.00    18260    10293\nbeta_id12          1.05      0.01     1.02     1.07 1.00    17891    11580\nbeta_id13          1.01      0.01     0.98     1.04 1.00    18998    10824\nbeta_id14          0.95      0.01     0.92     0.98 1.00    18321    10396\nbeta_id15          0.79      0.02     0.75     0.82 1.00    17550    11046\nbeta_id16          1.36      1.71    -1.99     4.77 1.00     2418     4208\nbeta_id17          1.08      1.71    -2.27     4.49 1.00     2419     4159\nbeta_id18          1.36      1.71    -2.00     4.77 1.00     2421     4150\nbeta_id19          1.44      1.71    -1.92     4.85 1.00     2417     4173\nbeta_id20          1.09      1.71    -2.25     4.50 1.00     2420     4083\nbeta_id21          1.31      1.71    -2.04     4.73 1.00     2420     4118\nbeta_id22          1.24      1.71    -2.10     4.65 1.00     2421     4122\nbeta_id23          1.12      1.71    -2.23     4.53 1.00     2419     4157\nbeta_id24          1.09      1.71    -2.26     4.51 1.00     2419     4209\nbeta_dose_adj     -0.21      0.74    -1.69     1.24 1.00     2419     4163\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.06      0.05     0.97     1.17 1.00    16342    11307\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nsummary(fit3)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome ~ exp(alpha) * log(time) - exp(beta) * time \n         alpha ~ 0 + id + dose_adj\n         beta ~ 0 + id + dose_adj\n   Data: fitdat (Number of observations: 264) \n  Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;\n         total post-warmup draws = 15000\n\nPopulation-Level Effects: \n               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_id1          3.31      0.25     2.83     3.80 1.00     2443     4189\nalpha_id2          3.27      0.25     2.79     3.75 1.00     2418     4115\nalpha_id3          3.07      0.25     2.59     3.55 1.00     2447     4120\nalpha_id4          3.00      0.25     2.52     3.48 1.00     2433     4196\nalpha_id5          3.05      0.25     2.56     3.53 1.00     2437     4206\nalpha_id6          3.14      0.25     2.66     3.62 1.00     2465     4179\nalpha_id7          3.09      0.25     2.61     3.57 1.00     2438     4311\nalpha_id8          2.98      0.02     2.95     3.01 1.00    18493    10858\nalpha_id9          2.91      0.02     2.87     2.94 1.00    18973    11242\nalpha_id10         2.98      0.02     2.95     3.01 1.00    18479    10529\nalpha_id11         2.94      0.02     2.91     2.97 1.00    18804    10454\nalpha_id12         2.84      0.02     2.81     2.87 1.00    18873    11236\nalpha_id13         2.97      0.02     2.94     3.00 1.00    19043    11438\nalpha_id14         3.09      0.01     3.06     3.12 1.00    19247    10690\nalpha_id15         2.95      0.02     2.91     2.98 1.00    19114    12099\nalpha_id16         2.96      0.25     2.48     3.44 1.00     2432     4256\nalpha_id17         2.73      0.25     2.25     3.21 1.00     2418     4214\nalpha_id18         2.92      0.25     2.43     3.39 1.00     2433     4335\nalpha_id19         2.95      0.25     2.47     3.43 1.00     2438     4308\nalpha_id20         2.92      0.25     2.43     3.40 1.00     2427     4161\nalpha_id21         2.90      0.25     2.41     3.37 1.00     2418     4311\nalpha_id22         2.85      0.25     2.36     3.33 1.00     2439     4182\nalpha_id23         2.84      0.25     2.36     3.32 1.00     2431     4213\nalpha_id24         2.78      0.25     2.30     3.26 1.00     2438     4170\nalpha_dose_adj     0.14      0.11    -0.07     0.35 1.00     2426     4307\nbeta_id1           1.05      0.24     0.58     1.53 1.00     2953     5165\nbeta_id2           0.96      0.24     0.49     1.43 1.00     2937     5242\nbeta_id3           1.00      0.24     0.53     1.47 1.00     2944     5168\nbeta_id4           1.01      0.24     0.54     1.49 1.00     2937     5142\nbeta_id5           1.24      0.24     0.76     1.71 1.00     2939     5218\nbeta_id6           0.99      0.24     0.52     1.46 1.00     2946     5117\nbeta_id7           1.08      0.24     0.60     1.55 1.00     2941     5223\nbeta_id8           1.01      0.01     0.99     1.04 1.00    18029    10844\nbeta_id9           0.91      0.02     0.88     0.93 1.00    18953    11005\nbeta_id10          0.98      0.01     0.96     1.01 1.00    18500    10509\nbeta_id11          1.15      0.01     1.13     1.17 1.00    18599    10418\nbeta_id12          1.05      0.01     1.02     1.07 1.00    19002    10853\nbeta_id13          1.01      0.01     0.98     1.04 1.00    18714    11040\nbeta_id14          0.95      0.01     0.92     0.98 1.00    19168    10286\nbeta_id15          0.79      0.02     0.75     0.82 1.00    18771    11344\nbeta_id16          1.06      0.24     0.58     1.53 1.00     2943     5165\nbeta_id17          0.78      0.25     0.30     1.25 1.00     2941     5155\nbeta_id18          1.05      0.24     0.58     1.53 1.00     2939     5207\nbeta_id19          1.14      0.24     0.66     1.61 1.00     2951     5251\nbeta_id20          0.79      0.25     0.31     1.26 1.00     2962     5253\nbeta_id21          1.00      0.24     0.52     1.47 1.00     2944     5222\nbeta_id22          0.94      0.24     0.46     1.41 1.00     2951     5207\nbeta_id23          0.82      0.25     0.34     1.29 1.00     2943     5263\nbeta_id24          0.79      0.25     0.31     1.26 1.00     2957     5311\nbeta_dose_adj     -0.08      0.11    -0.29     0.13 1.00     2939     5258\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.06      0.05     0.97     1.17 1.00    15961    10880\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNote the different naming of the parameters in brms. It’s unfortunately not possible (as far as I know) to get the names match the mathematical model. The parameters that have dose in their names are the ones we called \\(a_1\\) and \\(b_1\\) in our models. The many _id parameters are our previous \\(a_0\\) and \\(b_0\\) parameters. Conceptually, the latter are on the individual level. But we don’t have a nested/multi-level structure here, which seems to lead brms to consider every parameter on the same level, and thus labeling them all population level.\nNow, let’s look at priors and posteriors somewhat more. First, we extract priors and posteriors.\n\n#get priors and posteriors for models 1 and 3\nm1prior <- prior_draws(fit1)\nm1post <- as_draws_df(fit1)\nm3prior <- prior_draws(fit3)\nm3post <- as_draws_df(fit3)\n\nNow we can plot the distributions. I’m focusing on the \\(a_1\\) and \\(b_1\\) parameters since those are of more interest, and because I couldn’t figure out quickly how to get out and process all the individual level \\(a_0\\) and \\(b_0\\) parameters from brms 😁.\n\n#showing density plots for a1\n\n#make a data frame and get it in shape for ggplot\na1df <- data.frame(m1_prior = m1prior$b_alpha_dose_adj,\n                   m1_post = m1post$b_alpha_dose_adj,\n                   m3_prior = m3prior$b_alpha_dose_adj,\n                   m3_post =  m3post$b_alpha_dose_adj) %>%\n        pivot_longer(cols = everything(), names_to = c(\"model\",\"type\"), names_pattern = \"(.*)_(.*)\", values_to = \"value\")\n# make plot\np1 <- a1df %>%\n  ggplot() +\n  geom_density(aes(x = value, color = model, linetype = type), size = 1) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nplot(p1)\n\n\n\n#save for display on post\nggsave(file = paste0(\"featured.png\"), p1, dpi = 300, units = \"in\", width = 6, height = 6)\n\n\n#showing density plots for b1\nb1df <- data.frame(m1_prior = m1prior$b_beta_dose_adj,\n                   m1_post = m1post$b_beta_dose_adj,\n                   m3_prior = m3prior$b_beta_dose_adj,\n                   m3_post =  m3post$b_beta_dose_adj) %>%\n  pivot_longer(cols = everything(), names_to = c(\"model\",\"type\"), names_pattern = \"(.*)_(.*)\", values_to = \"value\")\n\np2 <- b1df %>%\n  ggplot() +\n  geom_density(aes(x = value, color = model, linetype = type), size = 1) +\n  theme_minimal()\nplot(p2)\n\n\n\n\nAs before, the priors for the \\(a_1\\) and \\(b_1\\) parameters are the same. We only changed the \\(a_0\\) and \\(b_0\\) priors, but that change leads to different posteriors for \\(a_1\\) and \\(b_1\\). It’s basically the same result we found with ulam/rethinking.\nIt would be surprising if we did NOT find the same correlation structure again in the parameters, let’s check it.\n\n# a few parameters for each dose\n#low dose\npairs(fit1, variable = variables(fit1)[c(1:4,25)])\n\n\n\n#medium dose\npairs(fit1, variable = variables(fit1)[c(8:11,25)])\n\n\n\n#high dose\npairs(fit1, variable = variables(fit1)[c(16:19,25)])\n\n\n\n\nApart from the unfortunate naming of parameters in brms, these are the same plots as we made for the ulam fits and show the same patterns.\nLet’s look at the posteriors in numerical form.\n\n# model 1 first\nfit1pars = posterior::summarize_draws(m1post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\n\n#only entries for the a0 parameters\na0post <- m1post %>% dplyr::select(starts_with('b_alpha_id'))\nfit1a0mean <- mean(colMeans(a0post))\n#only entries for the b0 parameters\nb0post <- m1post %>% dplyr::select(starts_with('b_beta_id'))\nfit1b0mean <- mean(colMeans(b0post))\nfit1otherpars <- fit1pars %>% dplyr::filter(!grepl('_id',variable)) %>%\n  dplyr::filter(!grepl('prior',variable))\nprint(fit1otherpars)\n\n# A tibble: 4 × 8\n  variable             mean     sd       q5     q95  rhat ess_bulk ess_tail\n  <chr>               <dbl>  <dbl>    <dbl>   <dbl> <dbl>    <dbl>    <dbl>\n1 b_alpha_dose_adj    0.224 0.726    -0.972    1.44  1.00    2839.    4785.\n2 b_beta_dose_adj    -0.212 0.744    -1.44     1.01  1.00    2419.    4163.\n3 sigma               1.06  0.0514    0.981    1.15  1.00   16342.   11307.\n4 lp__             -549.    5.58   -559.    -540.    1.00    4974.    8286.\n\nprint(c(fit1a0mean,fit1b0mean))\n\n[1] 2.960140 1.006334\n\n# repeat for model 3\nfit3pars = posterior::summarize_draws(m3post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\n#only entries for the a0 parameters\na0post <- m3post %>% dplyr::select(starts_with('b_alpha_id'))\nfit3a0mean <- mean(colMeans(a0post))\n#only entries for the b0 parameters\nb0post <- m3post %>% dplyr::select(starts_with('b_beta_id'))\nfit3b0mean <- mean(colMeans(b0post))\nfit3otherpars <- fit3pars %>% dplyr::filter(!grepl('_id',variable)) %>%\n  dplyr::filter(!grepl('prior',variable))\nprint(fit3otherpars)\n\n# A tibble: 4 × 8\n  variable              mean     sd        q5       q95  rhat ess_bulk ess_tail\n  <chr>                <dbl>  <dbl>     <dbl>     <dbl> <dbl>    <dbl>    <dbl>\n1 b_alpha_dose_adj    0.142  0.107    -0.0334    0.316   1.00    2426.    4307.\n2 b_beta_dose_adj    -0.0811 0.106    -0.254     0.0921  1.00    2939.    5258.\n3 sigma               1.06   0.0515    0.982     1.15    1.00   15961.   10880.\n4 lp__             -453.     5.60   -463.     -444.      1.00    4605.    7829.\n\nprint(c(fit3a0mean,fit3b0mean))\n\n[1] 2.9756367 0.9808696\n\n\nAgain, model 1 seems worse, with higher uncertainty intervals for the \\(a_1\\) and \\(b_1\\) parameters and the mean further away from the true value.\nWe can also compare the models as we did for rethinking using these lines of code:\n\nfit13comp <- loo_compare(add_criterion(fit1,\"waic\"),\n            add_criterion(fit3,\"waic\"),\n            criterion = \"waic\")\n\nWarning: \n30 (11.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nWarning: \n29 (11.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\nprint(fit13comp, simplify = FALSE)\n\n                            elpd_diff se_diff elpd_waic se_elpd_waic p_waic\nadd_criterion(fit1, \"waic\")    0.0       0.0  -416.3      11.7         43.5\nadd_criterion(fit3, \"waic\")    0.0       0.2  -416.3      11.7         43.5\n                            se_p_waic waic   se_waic\nadd_criterion(fit1, \"waic\")    4.3     832.5   23.4 \nadd_criterion(fit3, \"waic\")    4.3     832.6   23.4 \n\n\nModel performance is similar between models. The WAIC values are also close to those reported by rethinking."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparison-with-the-truth-and-ulam",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparison-with-the-truth-and-ulam",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Comparison with the truth and ulam",
    "text": "Comparison with the truth and ulam\nThe values used to generate the data are: \\(\\sigma =\\) 1, \\(\\mu_a =\\) 3, \\(\\mu_b =\\) 1, \\(a_1 =\\) 0.1, \\(b_1 =\\) -0.1.\nSince the models are the same as those we previously fit with ulam, only a different R package is used to run them, we should expect very similar results. This is the case. We find that as for the ulam fits, the estimates for \\(a_0\\), \\(b_0\\) and \\(\\sigma\\) are similar to the values used the generate the data, but estimates for \\(a_1\\) and \\(b_1\\) are not that great. The agreement with ulam is good, because we should expect that if we fit the same models, results should - up to numerical/sampling differences - be the same, no matter what software implementation we use. It also suggests that we did things right - or made the same mistake in both implementations! 😁.\nWhy the WAIC estimates are different is currently not clear to me. It could be that the 2 packages use different definitions/ways to compute it. Or something more fundamental is still different. I’m not sure."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a-1",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-2a-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 2a",
    "text": "Model 2a\nThis is the model with only population-level estimates. We already explored it somewhat above when we looked at traceplots and trankplots and the like. Here is just another quick table for the posteriors.\n\nm2post <- as_draws_df(fit2)\nfit2pars = posterior::summarize_draws(m2post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\nfit2otherpars <- fit2pars %>% dplyr::filter(!grepl('prior',variable))\nprint(fit2otherpars)\n\n# A tibble: 6 × 8\n  variable               mean      sd        q5       q95  rhat ess_bulk ess_t…¹\n  <chr>                 <dbl>   <dbl>     <dbl>     <dbl> <dbl>    <dbl>   <dbl>\n1 b_alpha_Intercept    2.98   0.0211     2.95      3.02    1.00    6244.   6691.\n2 b_alpha_dose_adj     0.0960 0.00967    0.0802    0.112   1.00    6569.   7301.\n3 b_beta_Intercept     0.992  0.0188     0.961     1.02    1.00    6387.   6724.\n4 b_beta_dose_adj     -0.0971 0.00862   -0.111    -0.0829  1.00    6947.   7786.\n5 sigma                6.88   0.302      6.39      7.39    1.00    8391.   7850.\n6 lp__              -892.     1.59    -895.     -890.      1.00    4964.   7039.\n# … with abbreviated variable name ¹​ess_tail\n\n\nThe parameters that have _Intercept in their name are what we called \\(\\mu_a\\) and \\(\\mu_b\\), the ones containing _dose are our \\(a_1\\) and \\(b_1\\). We find pretty much the same results we found using ulam. Specifically, the main parameters are estimated well, but because the model is not very flexible, the estimate for \\(\\sigma\\) is much larger, since it needs to account for all the individual-level variation we ommitted from the model itself."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4-1",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#model-4-1",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Model 4",
    "text": "Model 4\nThis is what I consider the most interesting and conceptually best model. It performed best in the ulam fits. Let’s see how it looks here. It is worth pointing out that this model ran much faster compared to models 1 and 3, it only took 10.5518333 minutes.\nWe’ll start with the summary for the model.\n\nfit4 <- fl[[4]]$fit\nm4prior <- prior_draws(fit4)\nm4post <- as_draws_df(fit4)\nsummary(fit4)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: outcome ~ exp(alpha) * log(time) - exp(beta) * time \n         alpha ~ (1 | id) + dose_adj\n         beta ~ (1 | id) + dose_adj\n   Data: fitdat (Number of observations: 264) \n  Draws: 5 chains, each with iter = 9000; warmup = 6000; thin = 1;\n         total post-warmup draws = 15000\n\nGroup-Level Effects: \n~id (Number of levels: 24) \n                    Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(alpha_Intercept)     0.09      0.02     0.07     0.13 1.00     3685     6514\nsd(beta_Intercept)      0.12      0.02     0.09     0.16 1.00     4048     5853\n\nPopulation-Level Effects: \n                Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nalpha_Intercept     2.99      0.02     2.95     3.03 1.00     3771     5404\nalpha_dose_adj      0.09      0.01     0.07     0.11 1.00     3979     5040\nbeta_Intercept      0.99      0.02     0.94     1.03 1.00     3486     5134\nbeta_dose_adj      -0.11      0.01    -0.13    -0.08 1.00     3855     5732\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     1.06      0.05     0.97     1.17 1.00    10136    10314\n\nDraws were sampled using sample(hmc). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\n\nNext, the prior/posterior plots. To ensure one can see the priors, I’m cutting off the y-axis at 10, that’s why the posteriors look a bit weird. They do infected extend and peak like the distributions shown for models 1 and 3.\n\n#showing density plots for a1 and b1\n#make a data frame and get it in shape for ggplot\nm4df <- data.frame(a1_prior = m4prior$b_alpha_dose_adj,\n                   a1_post = m4post$b_alpha_dose_adj,\n                   b1_prior = m4prior$b_beta_dose_adj,\n                   b1_post = m4post$b_beta_dose_adj) %>%\n  pivot_longer(cols = everything(), names_to = c(\"parameter\",\"type\"), names_pattern = \"(.*)_(.*)\", values_to = \"value\")\n# make plot\np1 <- m4df %>%\n  ggplot() +\n  ylim(0, 10) + xlim(-2, 2) +\n  geom_density(aes(x = value, color = parameter, linetype = type), adjust = 10, size = 1) +\n  ggtitle('model 4, parameters a1 and b1') +\n  theme_minimal()\nplot(p1)\n\n\n\n\nNumerical output for the posterior:\n\nfit4pars = posterior::summarize_draws(m4post, \"mean\", \"sd\", \"quantile2\", default_convergence_measures())\nfit4otherpars <- fit4pars %>% dplyr::filter(!grepl('_id',variable)) %>%\n  dplyr::filter(!grepl('prior',variable)) %>%\n  dplyr::filter(!grepl('z_',variable))\n\nprint(fit4otherpars)\n\n# A tibble: 6 × 8\n  variable               mean     sd        q5       q95  rhat ess_bulk ess_tail\n  <chr>                 <dbl>  <dbl>     <dbl>     <dbl> <dbl>    <dbl>    <dbl>\n1 b_alpha_Intercept    2.99   0.0197    2.95      3.02    1.00    3771.    5404.\n2 b_alpha_dose_adj     0.0861 0.0106    0.0688    0.104   1.00    3979.    5040.\n3 b_beta_Intercept     0.987  0.0247    0.946     1.03    1.00    3486.    5134.\n4 b_beta_dose_adj     -0.106  0.0131   -0.127    -0.0844  1.00    3855.    5732.\n5 sigma                1.06   0.0517    0.981     1.15    1.00   10136.   10314.\n6 lp__              -468.     7.47   -481.     -457.      1.00    2720.    4987.\n\n\nThese estimates look good, close to the truth.\nFinishing with the pairs lots:\n\n# a few parameters for each dose\n#low dose\npairs(fit4, variable = variables(fit4)[c(1:4,25)])\n\n\n\n#medium dose\npairs(fit4, variable = variables(fit4)[c(8:11,25)])\n\n\n\n#high dose\npairs(fit4, variable = variables(fit4)[c(16:19,25)])\n\n\n\n\nThe strong correlations between parameters are reduced, the same we say with the ulam models.\nAs was the case for the ulam fits, model 4 seems to perform overall best."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparing-all-models",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#comparing-all-models",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Comparing all models",
    "text": "Comparing all models\nWe can repeat the model comparison we did above, now including all 4 models. I’m looking now at both WAIC and LOO (leave one out). Note the various warning messages. We got that as well when we computed PSIS (which is similar to LOO) with rethinking.\n\nfit1a <- add_criterion(fit1,c(\"waic\",\"loo\"))\n\nWarning: \n30 (11.4%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nWarning: Found 6 observations with a pareto_k > 0.7 in model 'fit1'. It is\nrecommended to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\nfit2a <- add_criterion(fit2,c(\"waic\",\"loo\"))\n\nWarning: \n5 (1.9%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\nfit3a <- add_criterion(fit3,c(\"waic\",\"loo\"))\n\nWarning: \n29 (11.0%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nWarning: Found 5 observations with a pareto_k > 0.7 in model 'fit3'. It is\nrecommended to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\nfit4a <- add_criterion(fit4,c(\"waic\",\"loo\"))\n\nWarning: \n27 (10.2%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\nWarning: Found 6 observations with a pareto_k > 0.7 in model 'fit4'. It is\nrecommended to set 'moment_match = TRUE' in order to perform moment matching for\nproblematic observations.\n\ncompall1 <- loo_compare(fit1a,fit2a,fit3a,fit4a, criterion = \"waic\")\ncompall2 <- loo_compare(fit1a,fit2a,fit3a,fit4a, criterion = \"loo\")\nprint(compall1, simplify = FALSE)\n\n      elpd_diff se_diff elpd_waic se_elpd_waic p_waic se_p_waic waic   se_waic\nfit4a    0.0       0.0  -415.7      11.7         42.7    4.3     831.4   23.4 \nfit1a   -0.6       1.2  -416.3      11.7         43.5    4.3     832.5   23.4 \nfit3a   -0.6       1.3  -416.3      11.7         43.5    4.3     832.6   23.4 \nfit2a -473.6      23.0  -889.4      22.4         10.3    3.0    1778.7   44.8 \n\nprint(compall2, simplify = FALSE)\n\n      elpd_diff se_diff elpd_loo se_elpd_loo p_loo  se_p_loo looic  se_looic\nfit4a    0.0       0.0  -419.8     12.1        46.8    5.0    839.6   24.2  \nfit3a   -0.6       1.4  -420.4     12.1        47.6    5.0    840.7   24.2  \nfit1a   -1.1       1.5  -421.0     12.2        48.2    5.1    841.9   24.4  \nfit2a -469.6      23.0  -889.4     22.4        10.3    3.1   1778.9   44.8  \n\n\nModel 4 is considered best, though not by much. The above results, namely faster runtime and better estimates, speak more convincingly to the fact that model 4 is the best of these. The LOO is close to the PSIS metric reported by rethinking, even though I don’t think it’s defined and computed exactly the same."
  },
  {
    "objectID": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#prior-exploration",
    "href": "posts/2022-02-24-longitudinal-multilevel-bayes-3/index.html#prior-exploration",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 3",
    "section": "Prior exploration",
    "text": "Prior exploration\nSince brms has a way of specifying the model and priors that makes direct mapping to the mathematical model a bit more opaque, it is useful to explore if the models we run are what we think we run. brms has two helpful functions for looking at priors. One can help set priors before fitting, the other shows priors after fitting. To make the output manageable, we look at the simplest model, model 2. This looks as follows\n\n#defining model again\nm2aeqs <- bf(outcome ~ exp(alpha)*log(time) - exp(beta)*time,\n  alpha ~ 1 + dose_adj,\n  beta  ~  1 + dose_adj,\n  nl = TRUE)\npreprior2 <- get_prior(m2aeqs,data=fitdat,family=gaussian())\npostprior2 <- prior_summary(fit2)\nprint(preprior2)\n\n               prior class      coef group resp dpar nlpar lb ub       source\n student_t(3, 0, 23) sigma                                  0         default\n              (flat)     b                           alpha            default\n              (flat)     b  dose_adj                 alpha       (vectorized)\n              (flat)     b Intercept                 alpha       (vectorized)\n              (flat)     b                            beta            default\n              (flat)     b  dose_adj                  beta       (vectorized)\n              (flat)     b Intercept                  beta       (vectorized)\n\nprint(postprior2)\n\n           prior class      coef group resp dpar nlpar lb ub  source\n          (flat)     b                           alpha       default\n  normal(0.3, 1)     b  dose_adj                 alpha          user\n    normal(2, 2)     b Intercept                 alpha          user\n          (flat)     b                            beta       default\n normal(-0.3, 1)     b  dose_adj                  beta          user\n  normal(0.5, 2)     b Intercept                  beta          user\n    cauchy(0, 1) sigma                                  0       user\n\n\nThe first output shows the priors as the model sees them, before we apply any settings. It uses defaults. The second output shows the actual priors used when fitting the model, which are the ones we set. I find these functions and the information useful, but overall it’s still a bit confusing to me. For instance why are there those flat entries in there? I don’t know what they mean.\nIt gets worse for bigger models, and here things get confusing to me. This is looking at the priors for models 1,3 and 4. Recall that we expect \\(2(N+1)+1\\) priors for models 1 and 3, and \\(2(N+1+1)+1\\) for model 4. Since our data has 24 samples, we should find 51 and 53 priors. Here is what we get:\n\npostprior1 <- prior_summary(fit1)\npostprior3 <- prior_summary(fit3)\npostprior4 <- prior_summary(fit4)\nprint(paste(nrow(postprior1),nrow(postprior3),nrow(postprior4)))\n\n[1] \"53 53 13\"\n\n\nCloser inspection shows that for models 1 and 3, the priors include those strange flat ones that only have a class but no coefficient. My guess is those are not “real”, and thus we actually have the right number of priors/parameters. This can be checked by looking at the names of all the parameters for say model 1. Here they are:\n\nnames(m1post)\n\n  [1] \"b_alpha_id1\"            \"b_alpha_id2\"            \"b_alpha_id3\"           \n  [4] \"b_alpha_id4\"            \"b_alpha_id5\"            \"b_alpha_id6\"           \n  [7] \"b_alpha_id7\"            \"b_alpha_id8\"            \"b_alpha_id9\"           \n [10] \"b_alpha_id10\"           \"b_alpha_id11\"           \"b_alpha_id12\"          \n [13] \"b_alpha_id13\"           \"b_alpha_id14\"           \"b_alpha_id15\"          \n [16] \"b_alpha_id16\"           \"b_alpha_id17\"           \"b_alpha_id18\"          \n [19] \"b_alpha_id19\"           \"b_alpha_id20\"           \"b_alpha_id21\"          \n [22] \"b_alpha_id22\"           \"b_alpha_id23\"           \"b_alpha_id24\"          \n [25] \"b_alpha_dose_adj\"       \"b_beta_id1\"             \"b_beta_id2\"            \n [28] \"b_beta_id3\"             \"b_beta_id4\"             \"b_beta_id5\"            \n [31] \"b_beta_id6\"             \"b_beta_id7\"             \"b_beta_id8\"            \n [34] \"b_beta_id9\"             \"b_beta_id10\"            \"b_beta_id11\"           \n [37] \"b_beta_id12\"            \"b_beta_id13\"            \"b_beta_id14\"           \n [40] \"b_beta_id15\"            \"b_beta_id16\"            \"b_beta_id17\"           \n [43] \"b_beta_id18\"            \"b_beta_id19\"            \"b_beta_id20\"           \n [46] \"b_beta_id21\"            \"b_beta_id22\"            \"b_beta_id23\"           \n [49] \"b_beta_id24\"            \"b_beta_dose_adj\"        \"sigma\"                 \n [52] \"prior_b_alpha_id1\"      \"prior_b_alpha_id2\"      \"prior_b_alpha_id3\"     \n [55] \"prior_b_alpha_id4\"      \"prior_b_alpha_id5\"      \"prior_b_alpha_id6\"     \n [58] \"prior_b_alpha_id7\"      \"prior_b_alpha_id8\"      \"prior_b_alpha_id9\"     \n [61] \"prior_b_alpha_id10\"     \"prior_b_alpha_id11\"     \"prior_b_alpha_id12\"    \n [64] \"prior_b_alpha_id13\"     \"prior_b_alpha_id14\"     \"prior_b_alpha_id15\"    \n [67] \"prior_b_alpha_id16\"     \"prior_b_alpha_id17\"     \"prior_b_alpha_id18\"    \n [70] \"prior_b_alpha_id19\"     \"prior_b_alpha_id20\"     \"prior_b_alpha_id21\"    \n [73] \"prior_b_alpha_id22\"     \"prior_b_alpha_id23\"     \"prior_b_alpha_id24\"    \n [76] \"prior_b_alpha_dose_adj\" \"prior_b_beta_id1\"       \"prior_b_beta_id2\"      \n [79] \"prior_b_beta_id3\"       \"prior_b_beta_id4\"       \"prior_b_beta_id5\"      \n [82] \"prior_b_beta_id6\"       \"prior_b_beta_id7\"       \"prior_b_beta_id8\"      \n [85] \"prior_b_beta_id9\"       \"prior_b_beta_id10\"      \"prior_b_beta_id11\"     \n [88] \"prior_b_beta_id12\"      \"prior_b_beta_id13\"      \"prior_b_beta_id14\"     \n [91] \"prior_b_beta_id15\"      \"prior_b_beta_id16\"      \"prior_b_beta_id17\"     \n [94] \"prior_b_beta_id18\"      \"prior_b_beta_id19\"      \"prior_b_beta_id20\"     \n [97] \"prior_b_beta_id21\"      \"prior_b_beta_id22\"      \"prior_b_beta_id23\"     \n[100] \"prior_b_beta_id24\"      \"prior_b_beta_dose_adj\"  \"prior_sigma\"           \n[103] \"lprior\"                 \"lp__\"                   \".chain\"                \n[106] \".iteration\"             \".draw\"                 \n\n\nWe can see that there are the right number of both priors and posterior parameters, namely 2 times 24 for the individual level parameters, plus 2 dose parameters and \\(\\sigma\\).\nI find model 4 more confusing. Here is the full list of priors:\n\nprint(postprior4)\n\n           prior class      coef group resp dpar nlpar lb ub       source\n          (flat)     b                           alpha            default\n  normal(0.3, 1)     b  dose_adj                 alpha               user\n    normal(2, 1)     b Intercept                 alpha               user\n          (flat)     b                            beta            default\n normal(-0.3, 1)     b  dose_adj                  beta               user\n  normal(0.5, 1)     b Intercept                  beta               user\n    cauchy(0, 1)    sd                           alpha  0            user\n    cauchy(0, 1)    sd                            beta  0            user\n    cauchy(0, 1)    sd              id           alpha  0    (vectorized)\n    cauchy(0, 1)    sd Intercept    id           alpha  0    (vectorized)\n    cauchy(0, 1)    sd              id            beta  0    (vectorized)\n    cauchy(0, 1)    sd Intercept    id            beta  0    (vectorized)\n    cauchy(0, 1) sigma                                  0            user\n\n\nAnd this shows the names of all parameters\n\nnames(m4post)\n\n  [1] \"b_alpha_Intercept\"         \"b_alpha_dose_adj\"         \n  [3] \"b_beta_Intercept\"          \"b_beta_dose_adj\"          \n  [5] \"sd_id__alpha_Intercept\"    \"sd_id__beta_Intercept\"    \n  [7] \"sigma\"                     \"r_id__alpha[1,Intercept]\" \n  [9] \"r_id__alpha[2,Intercept]\"  \"r_id__alpha[3,Intercept]\" \n [11] \"r_id__alpha[4,Intercept]\"  \"r_id__alpha[5,Intercept]\" \n [13] \"r_id__alpha[6,Intercept]\"  \"r_id__alpha[7,Intercept]\" \n [15] \"r_id__alpha[8,Intercept]\"  \"r_id__alpha[9,Intercept]\" \n [17] \"r_id__alpha[10,Intercept]\" \"r_id__alpha[11,Intercept]\"\n [19] \"r_id__alpha[12,Intercept]\" \"r_id__alpha[13,Intercept]\"\n [21] \"r_id__alpha[14,Intercept]\" \"r_id__alpha[15,Intercept]\"\n [23] \"r_id__alpha[16,Intercept]\" \"r_id__alpha[17,Intercept]\"\n [25] \"r_id__alpha[18,Intercept]\" \"r_id__alpha[19,Intercept]\"\n [27] \"r_id__alpha[20,Intercept]\" \"r_id__alpha[21,Intercept]\"\n [29] \"r_id__alpha[22,Intercept]\" \"r_id__alpha[23,Intercept]\"\n [31] \"r_id__alpha[24,Intercept]\" \"r_id__beta[1,Intercept]\"  \n [33] \"r_id__beta[2,Intercept]\"   \"r_id__beta[3,Intercept]\"  \n [35] \"r_id__beta[4,Intercept]\"   \"r_id__beta[5,Intercept]\"  \n [37] \"r_id__beta[6,Intercept]\"   \"r_id__beta[7,Intercept]\"  \n [39] \"r_id__beta[8,Intercept]\"   \"r_id__beta[9,Intercept]\"  \n [41] \"r_id__beta[10,Intercept]\"  \"r_id__beta[11,Intercept]\" \n [43] \"r_id__beta[12,Intercept]\"  \"r_id__beta[13,Intercept]\" \n [45] \"r_id__beta[14,Intercept]\"  \"r_id__beta[15,Intercept]\" \n [47] \"r_id__beta[16,Intercept]\"  \"r_id__beta[17,Intercept]\" \n [49] \"r_id__beta[18,Intercept]\"  \"r_id__beta[19,Intercept]\" \n [51] \"r_id__beta[20,Intercept]\"  \"r_id__beta[21,Intercept]\" \n [53] \"r_id__beta[22,Intercept]\"  \"r_id__beta[23,Intercept]\" \n [55] \"r_id__beta[24,Intercept]\"  \"prior_b_alpha_Intercept\"  \n [57] \"prior_b_alpha_dose_adj\"    \"prior_b_beta_Intercept\"   \n [59] \"prior_b_beta_dose_adj\"     \"prior_sigma\"              \n [61] \"prior_sd_id\"               \"prior_sd_id__1\"           \n [63] \"lprior\"                    \"lp__\"                     \n [65] \"z_1[1,1]\"                  \"z_1[1,2]\"                 \n [67] \"z_1[1,3]\"                  \"z_1[1,4]\"                 \n [69] \"z_1[1,5]\"                  \"z_1[1,6]\"                 \n [71] \"z_1[1,7]\"                  \"z_1[1,8]\"                 \n [73] \"z_1[1,9]\"                  \"z_1[1,10]\"                \n [75] \"z_1[1,11]\"                 \"z_1[1,12]\"                \n [77] \"z_1[1,13]\"                 \"z_1[1,14]\"                \n [79] \"z_1[1,15]\"                 \"z_1[1,16]\"                \n [81] \"z_1[1,17]\"                 \"z_1[1,18]\"                \n [83] \"z_1[1,19]\"                 \"z_1[1,20]\"                \n [85] \"z_1[1,21]\"                 \"z_1[1,22]\"                \n [87] \"z_1[1,23]\"                 \"z_1[1,24]\"                \n [89] \"z_2[1,1]\"                  \"z_2[1,2]\"                 \n [91] \"z_2[1,3]\"                  \"z_2[1,4]\"                 \n [93] \"z_2[1,5]\"                  \"z_2[1,6]\"                 \n [95] \"z_2[1,7]\"                  \"z_2[1,8]\"                 \n [97] \"z_2[1,9]\"                  \"z_2[1,10]\"                \n [99] \"z_2[1,11]\"                 \"z_2[1,12]\"                \n[101] \"z_2[1,13]\"                 \"z_2[1,14]\"                \n[103] \"z_2[1,15]\"                 \"z_2[1,16]\"                \n[105] \"z_2[1,17]\"                 \"z_2[1,18]\"                \n[107] \"z_2[1,19]\"                 \"z_2[1,20]\"                \n[109] \"z_2[1,21]\"                 \"z_2[1,22]\"                \n[111] \"z_2[1,23]\"                 \"z_2[1,24]\"                \n[113] \".chain\"                    \".iteration\"               \n[115] \".draw\"                    \n\n\nTo compare directly, this is the model we want:\n\\[\n\\begin{aligned}\nY_{i,t}  & \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\mu_{i,t} &  =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\alpha_{i} &  =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\n\\beta_{i} &  =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\na_{0,i} & \\sim \\mathrm{Normal}(\\mu_a, \\sigma_a) \\\\\nb_{0,i} & \\sim \\mathrm{Normal}(\\mu_b, \\sigma_a) \\\\\na_1 & \\sim \\mathrm{Normal}(0.3, 1) \\\\\nb_1 & \\sim \\mathrm{Normal}(-0.3, 1) \\\\\n\\mu_a & \\sim \\mathrm{Normal}(2, 1) \\\\\n\\mu_b & \\sim \\mathrm{Normal}(0.5, 1) \\\\\n\\sigma &  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_a & \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_b & \\sim \\mathrm{HalfCauchy}(0,1)  \n\\end{aligned}\n\\]\nIf understand brms correctly, those z_ parameters are internal adjustments to make things more efficient and can otherwise be ignored. That means we have 2 times 24 parameters for the individual levels that all start with r_id. Those correspond to the \\(a_{0,i}\\) and \\(b_{0,1}\\), and they don’t have pre-defined priors, since they are computed based on other parameters. Then we have 2 dose parameters, which map to \\(a_1\\) and \\(b_1\\), both come with priors. We have 2 _Intercept parameters, which correspond to \\(\\mu_a\\) and \\(\\mu_b\\), again with priors. We have \\(\\sigma\\) with prior, and the two sd_id parameters seem to be those we call \\(\\sigma_a\\) and \\(\\sigma_b\\) in our equations.\nSo it looks like there is a match between our mathematical model we want, and the way we implemented it in brms. Still, I find the brms notation confusing and not that easy to follow. In that respect I much prefer ulam/rethinking.\nIn any case, I somewhat convinced myself that I’m fitting the same models here with brms that I’m fitting with ulam."
  },
  {
    "objectID": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html",
    "href": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 4",
    "section": "",
    "text": "This is a continuation with some side analyses of this tutorial illustrating how one can use the brms and rethinking R packages to perform a Bayesian analysis of longitudinal data using a multilevel/hierarchical/mixed-effects setup.\n\nIntroduction\nI assume you read through the main posts of the tutorial, namely this one describing model setup and data generation, and this one describing fitting with rethinking. You probably also looked at the fitting with brms post, though that’s optional for following along with this post.\n\n\n\n\nCitationBibTeX citation:@online{handel2022,\n  author = {Handel, Andreas},\n  title = {Bayesian Analysis of Longitudinal Multilevel Data Using Brms\n    and Rethinking - Part 4},\n  date = {2022-02-25},\n  url = {https://www.andreashandel.com/posts/2022-02-25-longitudinal-multilevel-bayes-4},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nHandel, Andreas. 2022. “Bayesian Analysis of Longitudinal\nMultilevel Data Using Brms and Rethinking - Part 4.” February 25,\n2022. https://www.andreashandel.com/posts/2022-02-25-longitudinal-multilevel-bayes-4."
  },
  {
    "objectID": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html#a-model-that-doesnt-work",
    "href": "posts/2022-02-25-longitudinal-multilevel-bayes-4/index.html#a-model-that-doesnt-work",
    "title": "Bayesian analysis of longitudinal multilevel data using brms and rethinking - part 4",
    "section": "A model that “doesn’t work”",
    "text": "A model that “doesn’t work”\nWhen we originally scribbled down models that we might use to fit our data, we came up with this model.\n\\[\n\\begin{align}\n\\textrm{Outcome} \\\\\nY_{i,t}  \\sim \\mathrm{Normal}\\left(\\mu_{i,t}, \\sigma\\right) \\\\\n\\\\\n\\textrm{Deterministic time-series trajectory} \\\\\n\\mu_{i,t} =  \\exp(\\alpha_{i}) \\log (t_{i}) -\\exp(\\beta_{i}) t_{i} \\\\\n\\\\\n\\textrm{Distribution for main parameters} \\\\\n\\alpha_{i}  \\sim \\mathrm{Normal}\\left(am_{i}, \\sigma_a \\right)  \\\\\n\\beta_{i}  \\sim \\mathrm{Normal}\\left(bm_{i}, \\sigma_b \\right) \\\\\n\\\\\n\\textrm{Deterministic models for main parameters} \\\\\nam_{i}   =  a_{0,i} + a_1 \\left(\\log (D_i) - \\log (D_m)\\right)  \\\\\nbm_{i}  =  b_{0,i} + b_1 \\left(\\log (D_i) - \\log (D_m)\\right) \\\\\n\\\\\n\\textrm{Distribution for (hyper)parameters} \\\\\na_{0,i}  \\sim \\mathrm{Normal}(2, 0.1) \\\\\na_{1}  \\sim \\mathrm{Normal}(0.5, 0.1) \\\\\nb_{0,i}  \\sim \\mathrm{Normal}(0, 0.1) \\\\\nb_{1}  \\sim \\mathrm{Normal}(-0.5, 0.1) \\\\\n\\sigma_a  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma_b  \\sim \\mathrm{HalfCauchy}(0,1)  \\\\\n\\sigma  \\sim \\mathrm{HalfCauchy}(0,1)  \n\\end{align}\n\\]\nThe model is similar to models 1-3, but with another distribution for parameters \\(\\alpha_i\\) and \\(\\beta_i\\). Fitting this model didn’t work, the fitting routine kept choking. We concluded that with this model we are overfitting. However, I am also not sure if there is something more fundamentally wrong in the way we wrote down this model. I’m not sure if a mix of having parameters defined by equations, then as distributions and equations again is a generally wrong way. This is currently beyond my Bayesian understanding. Feedback appreciated 😄.\nIt is straightforward to translate the model to rethinking or brms code, but since it didn’t fit well, and I’m not even sure if it’s a “proper” model, there’s no point in showing the code. Implement it if you want, and let me know if you have some insights into what exactly might be wrong with the model."
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Cillian McHugh",
    "section": "",
    "text": "University of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Cillian McHugh",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  },
  {
    "objectID": "index.html#background",
    "href": "index.html#background",
    "title": "Cillian McHugh",
    "section": "",
    "text": "I am an Experimental Psychologist specialising in cognitive and social psychology. I have worked with a diverse range of research techniques, and I have applied experience in the development of new methods and materials, and in theory development. The main focus of my research to date has been on morality and moral judgement. I have applied understandings from the cognitive psychology of concepts and categories to the moral domain to propose a novel theory of Moral Judgment as Categorization (MJAC).\nMy empirical work has largely focused on the phenomenon of moral dumbfounding; when a person defends a moral judgement in the absence of reasons. The dumbfounding paradigm has been influential in moral psychology; however, there has been limited empirical work investigating it. This has meant that the main focus of my work to date has been testing, and attempting to understand the phenomenon.\nMy interests extend beyond the moral domain to include learning and knowledge acquisition, categorisation, skill/expertise, meaning, motivation, and memory. My theoretical interests include Ecological Psychology, Embodiment, and Dynamical Systems."
  },
  {
    "objectID": "index.html#featured-video",
    "href": "index.html#featured-video",
    "title": "Cillian McHugh",
    "section": "Featured Video",
    "text": "Featured Video\nMoral Judgment as Categorization (MJAC): presentation to IMP seminar series 31st May 2021"
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Publications\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nJust wrong? Or just WEIRD?\n\n\nInvestigating the prevalence of moral dumbfounding in non-Western samples\n\n\n\n\nmorality\n\n\nmoral dumbfounding\n\n\nWEIRD\n\n\n\n\nMemory & Cognition (2023)\n\n\n\n\n\n\nJan 17, 2023\n\n\nCillian McHugh, Run Zhang, Tanuja Karnatak, Nishtha Lamba, Olga Khokhlova\n\n\n\n\n\n\n  \n\n\n\n\nMoral Judgment as Categorization (MJAC)\n\n\n\n\n\n\n\nmorality\n\n\nmoral judgment\n\n\ncategorization\n\n\n\n\nPerspectives in Psychological Science (2022)\n\n\n\n\n\n\nJan 18, 2022\n\n\nCillian McHugh, Marek McGann, Eric R. Igou, Elaine L. Kinsella\n\n\n\n\n\n\n  \n\n\n\n\nSearching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding\n\n\n\n\n\n\n\nmorality\n\n\nmoral dumbfounding\n\n\nmethods\n\n\n\n\nCollabra: Psychology (2017)\n\n\n\n\n\n\nSep 20, 2017\n\n\nCillian McHugh, Marek McGann, Eric R. Igou, Elaine L. Kinsella\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html",
    "href": "publications/just-wrong-or-just-weird/index.html",
    "title": "Just wrong? Or just WEIRD?",
    "section": "",
    "text": "Moral dumbfounding occurs when people maintain a moral judgment even though they cannot provide a reason for this judgment. Dumbfounded responding may include admitting to not having reasons, or the use of unsupported declarations (“It’s just wrong”) as justification for a judgment. Published evidence for dumbfounding has drawn exclusively on samples of WEIRD backgrounds (Western, educated, industrialized, rich, and democratic), and it remains unclear to what extent the phenomenon is generalizable to other populations. Furthermore, the theoretical implications of moral dumbfounding have been disputed in recent years. In three studies we apply a standardized moral dumbfounding task, and show evidence for moral dumbfounding in a Chinese sample (Study 1, N = 165), an Indian sample (Study 2, N = 181), and a mixed sample primarily (but not exclusively) from North Africa and the Middle East (MENA region, Study 3, N = 264). These findings are consistent with a categorization theories of moral judgment.”\nThe phenomenon of moral dumbfounding occurs when people defend a moral judgement even though they cannot provide a reason in support of this judgement (Haidt, Björklund, and Murphy 2000; McHugh et al. 2017). It typically manifests as an explicit admission of not having reasons, or the use of unsupported declarations (e.g., “It’s just wrong”) as a justification for a judgement. For almost two decades, evidence for moral dumbfounding was limited to a single study, unpublished in peer reviewed form, and with a total sample of N = 30 (Haidt, Björklund, and Murphy 2000). This meant that, while the phenomenon was widely discussed in the literature, its existence was not well supported by empirical evidence. Recent work (McHugh et al. 2017, 2020), has provided additional evidence for the existence moral dumbfounding, demonstrating that it can be reliably elicited (though perhaps it not as widespread as previously assumed, see Royzman, Kim, and Leeman 2015; McHugh et al. 2020).\nDespite this recent work, it remains unclear how universal or generalisable the phenomenon is. Current evidence is limited to research involving exclusively WEIRD (Western, educated, industrialised, rich, and democratic, see Henrich, Heine, and Norenzayan 2010) samples. The purpose of the current research is to extend research on moral dumbfounding beyond these exclusively WEIRD contexts. Specifically we test for the presence of moral dumbfounding in a Chinese sample (Study 1), in an Indian sample (Study 2), and in a mixed sample from a diverse range of non-WEIRD countries (Study 3)."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#method",
    "href": "publications/just-wrong-or-just-weird/index.html#method",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Method",
    "text": "Method\n\nParticipants and design\nStudy 1 was a frequency based attempted replication of McHugh et al. (2017). The aim of Study 1 was to identify if dumbfounded responding could be evoked in a Chinese context. Results are primarily descriptive. We have included exploratory analyses investigating the possible influence of individualism/collectivism (Renzhi et al. 2013) on responding.\nAn initial sample of 42 (34 female, 8 male, 0 other; Mage = 21.43, min = 18, max = 27, SD = 1.74) participants took part. An additional 123 (75 female, 48 male, 0 other; Mage = 22.06, min = 18, max = 45, SD = 3.76) participants completed a follow-up study with the Cannibal scenario only, and in which we experimentally manipulated psychological distance (drawing on construal level theory e.g., Liberman, Trope, and Stephan 2007). The experimental manipulation had no influence on dumbfounded responding, \\(\\chi\\)2(2, N = 123) = 2.304, p = .316, V = 0.1023383, and as such we have included these participants in the current analysis (for clarity these studies are reported separately as Study 1a and Study 1b). Participants in this study were undergraduate students and postgraduate students, from Luoyang Normal University (China). All participants were recruited from China and had a high level of Chinese, the entire online questionnaire survey was presented in Chinese. Participation was voluntary and participants were not reimbursed for their participation.\n\n\nProcedure and materials\nData were collected through the Chinese language online survey software Wenjuanxing (“Wenjuanxing” 2006). Participants were provided with a link to the online survey. The first page of the survey was an information sheet. If participants chose to continue, they proceeded to the second page, the consent form. Participants could only proceed to the remainder of the survey if they provide consent on the consent form. Upon providing consent and proceeding, participants completed some questions relating to basic demographics.\nThe procedure and materials for the moral dumbfounding task were taken directly from McHugh et al. (2017). These were translated into Chinese by a member of the research team whose native language was Chinese. Four moral judgement scenarios were used, two “intuition” scenarios: Incest, Cannibal, and two “reasoning” scenarios Trolley, Heinz (taken from McHugh et al. 2017, see Appendix A).\nMoral dumbfounding task. The basic procedure for moral dumbfounding tasks is as follows. Participants are presented with a scenario to read. They are then asked to rate, on a 7-point Likert scale (1 = Morally wrong; 4 = Neutral; 7 = Morally right), how right or wrong they regarded the behaviour described in the scenario. Following this participants are asked to rate their confidence in their judgement (again on a 7-point Likert scale). Participants are then presented with a series of counterarguments, which refuted commonly used justifications for rating the behaviours as “wrong” (see Appendix B). After each counter-argument, participants are asked “Do you (still) think it is wrong?”, with a binary “yes/no” response option; and then they are asked “Do you have a reason for your judgement?”, with three possible response option “Yes, I have a reason”, “No I have no reason”, and “Unsure”. This sequence was repeated for each of the three counter-arguments.\nDumbfounding is measured using the “critical slide” which contains a statement defending the behaviour, and a question asking how the behaviour could be wrong (see Appendix C). There are three possible answer options: (a) “There is nothing wrong”; (b) an admission of not having reasons (“It’s wrong but I can’t think of a reason”); and finally a judgement with accompanying justification (c) “It’s wrong and I can provide a valid reason”. The selecting of option (b), the admission of not having reasons, is taken to be a dumbfounded response. Participants who selected (c) were promoted to type a reason on the next page. The order of these response options was randomised.\nFollowing the critical slide, participants rated the behaviour again, and completed the post-discussion questionnaire devised by Haidt, Björklund, and Murphy (2000). They were required to rate on a 7-point Likert scale how sure were they about their judgement; how much they changed their mind; how confused were they; how irritated were they; how much was their judgement based on reason; how much was their judgement based on “gut” feeling (see Appendix D). This process is repeated in full for each moral scenario. The order of presentation of the moral scenarios was randomised.\nCoding reasons. While there is a strong theoretical and empirical case for coding the reasons provided for unsupported declarations or tautological responses, as dumbfounded responses (see McHugh et al. 2017), this approach has been challenged by claims that these responses constitute the expression of a normative position (e.g., Royzman, Kim, and Leeman 2015). In response to this challenge, we adopt an “admission of not having reasons” as the only measure of moral dumbfounding in these studies. While this measure provides a more conservative estimate of the prevalence of moral dumbfounding, it provides a considerably less ambiguous estimate.\nIndividualism-collectivism scale. Following the dumbfounding task, participants completed the individualism-collectivism scale (Li and Aksoy 2007, see Appendix E). This sixteen-item scale includes four sub-scales: Vertical Collectivism (VC), Horizontal Collectivism (HC), Vertical Individualism (VI) and Horizontal Individualism (HI). The responses were recorded on a 9-point Likert scale ranging from 1 = strongly disagree, to 9 = strongly agree. The entire study lasted approximately twenty minutes."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#results-and-discussion",
    "href": "publications/just-wrong-or-just-weird/index.html#results-and-discussion",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nJudgements of the scenarios\nThe mean initial ratings for each scenario are as follows: MHeinz = 4.76, SDHeinz = 2.07; MCannibal = 1.52, SDCannibal = 1.13; MIncest = 2.88, SDIncest = 2.23; MTrolley = 3.29, SDTrolley = 1.95. The mean revised ratings for each scenario are as follows: MHeinz = 4.74, SDHeinz = 2.04; MCannibal = 1.6, SDCannibal = 1.08; MIncest = 2.9, SDIncest = 2.13; MTrolley = 3.36, SDTrolley = 2.03. The proportion of wrong, neutral, and ok, judgements for each scenario are displayed in Table @ref(tab:tab2judge).\n\n\n(#tab:tab2judge)\n\n\nValence of initial and revised judgements for each scenarion for each study\n\n\n\n\n\n\nN\npercent\nN\npercent\nN\npercent\nN\npercent\n\n\n\n\nStudy 1a\nInitial: Wrong\n9\n21.43%\n17\n40.48%\n25\n59.52%\n37\n88.1%\n\n\n\nInitial: Neutral\n13\n30.95%\n19\n45.24%\n8\n19.05%\n4\n9.52%\n\n\n\nInitial: ok\n20\n47.62%\n6\n14.29%\n9\n21.43%\n1\n2.38%\n\n\n\nRevised: Wrong\n10\n23.81%\n19\n45.24%\n23\n54.76%\n37\n88.1%\n\n\n\nRevised: Neutral\n12\n28.57%\n14\n33.33%\n10\n23.81%\n5\n11.9%\n\n\n\nRevised: ok\n20\n47.62%\n9\n21.43%\n9\n21.43%\n0\n0%\n\n\nStudy 1b\nInitial: Wrong\n-\n-\n-\n-\n-\n-\n84\n68.29%\n\n\n\nInitial: Neutral\n-\n-\n-\n-\n-\n-\n21\n17.07%\n\n\n\nInitial: OK\n-\n-\n-\n-\n-\n-\n18\n14.63%\n\n\n\nInitial: Wrong\n-\n-\n-\n-\n-\n-\n80\n65.04%\n\n\n\nInitial: Neutral\n-\n-\n-\n-\n-\n-\n28\n22.76%\n\n\n\nInitial: OK\n-\n-\n-\n-\n-\n-\n15\n12.2%\n\n\nStudy 2\nInitial: Wrong\n132\n70.21%\n127\n67.55%\n117\n62.23%\n146\n77.66%\n\n\n\nInitial: Neutral\n17\n9.04%\n18\n9.57%\n45\n23.94%\n27\n14.36%\n\n\n\nInitial: ok\n36\n19.15%\n39\n20.74%\n23\n12.23%\n12\n6.38%\n\n\n\nRevised: Wrong\n138\n73.4%\n124\n65.96%\n110\n58.51%\n147\n78.19%\n\n\n\nRevised: Neutral\n13\n6.91%\n23\n12.23%\n39\n20.74%\n24\n12.77%\n\n\n\nRevised: ok\n32\n17.02%\n37\n19.68%\n36\n19.15%\n14\n7.45%\n\n\nStudy 3\nInitial: Wrong\n155\n57.2%\n151\n55.72%\n82\n30.26%\n176\n64.94%\n\n\n\nInitial: Neutral\n37\n13.65%\n42\n15.5%\n99\n36.53%\n32\n11.81%\n\n\n\nInitial: ok\n36\n13.28%\n36\n13.28%\n62\n22.88%\n23\n8.49%\n\n\n\nRevised: Wrong\n148\n54.61%\n140\n51.66%\n51\n18.82%\n173\n63.84%\n\n\n\nRevised: Neutral\n37\n13.65%\n53\n19.56%\n96\n35.42%\n30\n11.07%\n\n\n\nRevised: ok\n37\n13.65%\n33\n12.18%\n91\n33.58%\n23\n8.49%\n\n\n\n\nA paired samples t-test revealed no differences in the ratings of behaviours from time one to time two, Heinz, t(81.979254) = 0.053, p = .958, d = 0.0115954; Cannibal, t(81.8464504) = -0.296, p = .768, d = 0.0644897; Incest, t(81.8102068) = -0.05, p = .960, d = 0.0109172; Trolley, t(81.868551) = -0.164, p = .870, d = 0.0358119.\nA one-way ANOVA revealed significant differences in initial judgements depending on scenario, F(3, ,, , 164) = 20.77, p &lt; .001, partial \\(\\eta\\)2 = .275. Tukey’s post-hoc pairwise comparison revealed that judgements in the Heinz dilemma were significantly more favourable than for each of the other scenarios: Cannibal, p &lt; .001, Incest, p &lt; .001, Trolley, p = .003; while judgements of Cannibal were significantly more harsh than all other scenarios: Heinz, p &lt; .001, Incest, p = .007. Trolley, p &lt; .001; there was no significant difference between judgements of Incest and of Trolley, p = .762.\nA one-way ANOVA revealed the same pattern of differences in revised judgements depending on scenario, F(3, ,, , 164) = 20.19, p &lt; .001, partial \\(\\eta\\)2 = .270. Again, Tukey’s post-hoc pairwise comparison revealed that judgements in the Heinz dilemma were significantly more favourable than for each of the other scenarios: Cannibal, p &lt; .001, Incest, p &lt; .001, Trolley, p = .005; while judgements of Cannibal were significantly more harsh than all other scenarios: Heinz, p &lt; .001, Incest, p = .009. Trolley, p &lt; .001; there was no significant difference between judgements of Incest and of Trolley, p = .685.\n\n\nTables of other Responses\n\n\n(#tab:tab2other)\n\n\nMeans and Standard Deviations for initial and revised confidence ratings, and for self-reported changed mind for each scenario for each study\n\n\n\n\n\n\nM\nSD\nM\nSD\nM\nSD\nM\nSD\n\n\n\n\nStudy 1a\nInitial Confidence\n5.5\n1.3\n5.5\n1.5\n5.4\n1.2\n5.8\n1.7\n\n\n\nRevised Confidence\n5.7\n1.4\n5.4\n1.7\n5.4\n1.5\n6.1\n1.5\n\n\n\nChanged Mind\n2.7\n2\n2.6\n1.9\n2.5\n2.1\n2.5\n2.1\n\n\nStudy 1b\nInitial Confidence\n-\n-\n-\n-\n-\n-\n4.9\n1.6\n\n\n\nRevised Confidence\n-\n-\n-\n-\n-\n-\n5.1\n1.7\n\n\n\nChanged Mind\n-\n-\n-\n-\n-\n-\n2.5\n1.7\n\n\nStudy 2\nInitial Confidence\n5.9\n1.2\n5.3\n1.6\n5.8\n1.4\n5.6\n1.4\n\n\n\nRevised Confidence\n5.8\n1.3\n5.5\n1.5\n5.7\n1.4\n5.7\n1.3\n\n\n\nChanged Mind\n1.7\n1.3\n2.1\n1.5\n2\n1.6\n1.8\n1.4\n\n\nStudy 3\nInitial Confidence\n5.8\n1.6\n5.4\n1.8\n5.6\n1.5\n5.8\n1.6\n\n\n\nRevised Confidence\n5.6\n1.6\n5.5\n1.6\n5.7\n1.4\n6\n1.5\n\n\n\nChanged Mind\n2.1\n1.6\n2.1\n1.5\n2.2\n1.7\n1.7\n1.3\n\n\n\n\n\n\n\n(#tab:tabPostQs)\n\n\nMeans and Standard Deviations for responses to the post-discussion questionnaire for each scenario for each study\n\n\n\n\n\n\nM\nSD\nM\nSD\nM\nSD\nM\nSD\n\n\n\n\nStudy 1a\nConfused\n2.3\n1.6\n3\n2.2\n2.3\n1.6\n2.4\n2.1\n\n\n\nIrritated\n2.3\n1.6\n2.9\n2\n2.5\n2\n3.6\n2.5\n\n\n\nReason-Based\n5.2\n1.8\n5.2\n1.8\n5.6\n1.6\n5.6\n1.8\n\n\n\nGut-Based\n3.5\n2\n3.7\n2\n3.4\n2\n3.7\n2.3\n\n\nStudy 1b\nConfused\n-\n-\n-\n-\n-\n-\n3\n2.1\n\n\n\nIrritated\n-\n-\n-\n-\n-\n-\n3.7\n2.1\n\n\n\nReason-Based\n-\n-\n-\n-\n-\n-\n5.2\n1.6\n\n\n\nGut-Based\n-\n-\n-\n-\n-\n-\n4.3\n1.9\n\n\nStudy 2\nConfused\n1.9\n1.4\n2.4\n1.7\n2\n1.6\n2\n1.4\n\n\n\nIrritated\n2.2\n1.7\n2.3\n1.7\n2.4\n1.9\n2.6\n1.9\n\n\n\nReason-Based\n5.5\n1.7\n5.1\n1.8\n5.1\n1.8\n5.2\n1.9\n\n\n\nGut-Based\n3.5\n2.3\n3.4\n2.1\n3.8\n2.2\n3.4\n2.2\n\n\nStudy 3\nConfused\n2.4\n1.9\n2.8\n2\n2.3\n1.8\n2.1\n1.7\n\n\n\nIrritated\n3.3\n2.3\n3.3\n2.2\n2.3\n1.8\n4\n2.4\n\n\n\nReason-Based\n5.5\n1.5\n5.4\n1.8\n5.5\n1.7\n5.5\n1.8\n\n\n\nGut-Based\n3.9\n2.1\n3.7\n2.2\n3.9\n2.2\n4\n2.4\n\n\n\n\n\n\n\nMeasuring dumbfounding\nParticipants who selected the admission of not having reasons were identified as dumbfounded. Across the four scenarios (Study 1a), 21 participants (50%) provided a dumbfounded response at least once. In Study 1b, 47 participants, (38.2113821%) provided a dumbfounded response for the Cannibal scenario. Table @ref(tab:tab2dumb) shows the number and percentage of participants who selected each response for each scenario across Studies 1a and 1b. Figure @ref(fig:chinesefig) shows this information for Study 1a, while Figure @ref(fig:chinesefig2) additionally includes the responses for Study1b. Crucially for the current study, rates of dumbfounded responding for each scenario in Study 1a were significantly greater than zero, Heinz: z = 2.97, p = .003; Trolley: z = 3.17, p = .001; Incest: z = 2.76, p = .006; Cannibal: z = 3.92, p &lt; .001. Similarly rates of dumbfounded responding in Study 1b were significantly greater than zero for the Cannibal scenario, z = 7.62, p &lt; .001.\n\n\n(#tab:tab2dumb)\n\n\nObserved frequency and percentage of each of the responses: dumbfounded, nothing wrong, and reasons provided for each scenario for each study\n\n\n\n\n\n\nN\npercent\nN\npercent\nN\npercent\nN\npercent\n\n\n\n\nStudy 1a\nNothing wrong\n14\n33.33%\n11\n26.19%\n19\n45.24%\n5\n11.9%\n\n\n\nDumbfounded\n8\n19.05%\n9\n21.43%\n7\n16.67%\n13\n30.95%\n\n\n\nReasons\n20\n47.62%\n22\n52.38%\n16\n38.1%\n24\n57.14%\n\n\nStudy 1b\nNothing wrong\n-\n-\n-\n-\n-\n-\n19\n15.45%\n\n\n\nDumbfounded\n-\n-\n-\n-\n-\n-\n47\n38.21%\n\n\n\nReasons\n-\n-\n-\n-\n-\n-\n57\n46.34%\n\n\nStudy 2\nNothing wrong\n50\n27.14%\n42\n22.8%\n75\n40.71%\n35\n19%\n\n\n\nDumbfounded\n21\n11.4%\n47\n25.51%\n33\n17.91%\n44\n23.88%\n\n\n\nReasons\n112\n60.79%\n95\n51.56%\n77\n41.79%\n106\n57.53%\n\n\nStudy 3\nNothing wrong\n60\n22.14%\n38\n14.02%\n170\n62.73%\n46\n16.97%\n\n\n\nDumbfounded\n33\n12.18%\n56\n20.66%\n22\n8.12%\n45\n16.61%\n\n\n\nReasons\n130\n47.97%\n132\n48.71%\n48\n17.71%\n140\n51.66%\n\n\n\n\n\n\n\n\n\nRates of each type of response for each scenario in the Chinese Sample ( = 42)\n\n\n\n\n\n\n\n\n\nRates of each type of response for each scenario in the Chinese Sample (including additional data on Cannibal scenario)\n\n\n\n\nThere was no significant difference in observed rates of dumbfounded responding depending on which scenario was being discussed, \\(\\chi\\)2(6, N = 271) = 12.34, p = .055. Similarly, there was no influence of type of scenario (reasoning vs intuition) on rates of dumbfounded responding \\(\\chi\\)2(2, N = 271) = 0.31, p = .855.\nWe found clear evidence for dumbfounded responding in our Chinese sample. Interestingly, while the Incest scenario is generally regarded as the most reliable for eliciting moral dumbfounding in Western samples (e.g., Royzman, Kim, and Leeman 2015), Cannibal appeared to be the scenario most likely to elicit dumbfounding in this sample. While this difference in responding to the critical slide is not statistically significant, we did observe significantly harsher judgements for Cannibal than for the other scenarios. The pattern of responding to the critical slide is therefore not surprising. Furthermore, it is possible that the small sample size meant that our study was not sufficiently powered to detect differences in responding to the critical slide. As such, we note that the converging evidence across three measures (initial judgement, revised judgement, and critical slide), point towards issues surrounding death and respect for the dead as being more relevant in this Chinese sample. This is consistent with existing research on the death taboo, and the importance of death in Chinese culture (e.g., Selin and Rakoff 2019; Wu and Lu 2011). This interpretation is further corroborated by analysis of the open-ended responses, with 20 participants (47.62%) providing statements such as “Jennifer eating human flesh is an immoral and uncivilized behaviour”. This suggests that while WEIRD samples appear to be more inclined to moralise, and present as dumbfounded for, the Incest scenario, it appears (from our small and limited sample) that it is the Cannibal scenario that is of greater concern to Chinese participants.\n\n\nIndividual differences (Study 1a)\nA hierarchical linear regression was conducted to test the possible relationship between ICS (Renzhi et al. 2013), and susceptibility to dumbfounding. Susceptibility to dumbfounding was operationalised by creating a new variable representing the number of times each participant provided a dumbfounded response. This measure was included as our outcome variable, and the four sub-scales of ICS were included as predictor variables. The overall model did not significantly predict susceptibility to dumbfounding \\(R^2 = .09\\), \\(F(4, 37) = 0.95\\), \\(p = .448\\).\nWe conducted a series of multinomial logistic regressions to investigate the possible relationship between ICS (Renzhi et al. 2013) and responding to each of the scenarios individually. Response to the critical slide scenario was the dependent variable for each scenario, and the four sub-scales of the ICS were included as predictor variables.\nThe overall model did not significantly predict responses for the Heinz dilemma, \\(\\chi\\)2(8, N = 42) = 10.48, p = .233, the observed power was 0.61; neither did the model significantly predict responses for the Trolley scenario, \\(\\chi\\)2(8, N = 42) = 11.02, p = .201, the observed power was 0.64; the Incest scenario, \\(\\chi\\)2(8, N = 42) = 13.92, p = .084, the observed power was 0.76; nor the Cannibal scenario, \\(\\chi\\)2(8, N = 42) = 6.84, p = .554, the observed power was 0.41.\n\n\nIndividual differences (Study 1b)\nGiven the larger sample size in Study 1b, we additionally tested if ICS (Renzhi et al. 2013) predicted responses to the Cannibal scenario in Study 1b. We conducted a multinomial logistic regression with response to the critical slide as the outcome variable and the four sub-scales of the ICS entered as predictor variables. Overall the model did not significantly predict responses to the critical slide for the Cannibal scenario, \\(\\chi\\)2(8, N = 123) = 10.96, p = .204, the observed power was 0.64."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#method-1",
    "href": "publications/just-wrong-or-just-weird/index.html#method-1",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Method",
    "text": "Method\n\nParticipants and design\nStudy 2 was a frequency based attempted replication of McHugh et al. (2017). The aim of Study 2 was to identify if dumbfounded responding could be evoked in an Indian context.\nA total sample of 188 (116 female, 69 male, 0 other, 3 declined to report their gender; Mage = 22.88, min = 16, max = 39, SD = 2.55) participants took part. The breakdown of participants’ religion is as follows, Hinduism: n = 138, Islam: n = 4, Christianity: n = 7, Sikhism: n = 4, Buddhism: n = 0, Jainism: n = 8, other: n = 10, and 17 participants declined to provide their religion. All participants were of Indian nationality, and 164 indicated that they resided in India at the time of completing the survey. Participants were recruited through snowball sampling.\n\n\nProcedure and materials\nThe procedure for Study 2 was the same as for Study 1, with some minor changes. Given the diversity of languages in India, and the high proficiency of English among Indian nationals, all written materials were presented in English. The survey was programmed and presented using Qualtrics. The demographic information recorded additionally included religion, given the prominence and diversity of religions in Indian society. We also included the meaning in life questionnaire (MLQ: Steger et al. 2008) in Study 2. The entire study lasted twenty to twenty-five minutes."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#results-and-discussion-1",
    "href": "publications/just-wrong-or-just-weird/index.html#results-and-discussion-1",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nJudgements of the scenarios\nThe mean initial ratings for each scenario were as follows: MHeinz = 2.61, SDHeinz = 1.88; MCannibal = 2.1, SDCannibal = 1.52; MIncest = 2.66, SDIncest = 1.84; MTrolley = 2.83, SDTrolley = 1.84. The mean revised ratings for each scenario are as follows: MHeinz = 2.66, SDHeinz = 1.84; MCannibal = 2.15, SDCannibal = 1.5; MIncest = 2.94, SDIncest = 1.94; MTrolley = 2.84, SDTrolley = 1.81. The proportion of wrong, neutral, and ok, judgements for each scenario are displayed in Table @ref(tab:tab2judge).\nA paired samples t-test revealed no differences in the ratings of behaviours from time one to time two, Heinz, t(365.9540429) = -0.26, p = .795, d = 0.0270679; Cannibal, t(367.9546849) = -0.31, p = .757, d = 0.0322612; Incest, t(367.0415483) = -1.403, p = .161, d = 0.1459137; Trolley, t(365.8881139) = -0.057, p = .954, d = 0.0059546.\nA one-way ANOVA revealed significant differences in initial judgements depending on scenario, F(3, ,, , 735) = 5.72, p &lt; .001, partial \\(\\eta\\)2 = .023. Tukey’s post-hoc pairwise comparison revealed that judgements of Cannibal were significantly more harsh than all other scenarios: Heinz, p = .033, Incest, p = .013. Trolley, p &lt; .001; there were no significant differences in the ratings of the other scenarios, Heinz/Incest, p = .988, Heinz/Trolley, p = .631, Incest/Trolley, p = .819.\nA one-way ANOVA revealed the same pattern of differences in revised judgements depending on scenario, F(3, ,, , 733) = 7.17, p &lt; .001, partial \\(\\eta\\)2 = .029. Again, Tukey’s post-hoc pairwise comparison revealed that judgements of Cannibal were significantly more harsh than all other scenarios: Heinz, p = .034, Incest, p &lt; .001. Trolley, p = .001; there were no significant differences in the ratings of the other scenarios, Heinz/Incest, p = .416, Heinz/Trolley, p = .763, Incest/Trolley, p = .944.\n\n\nMeasuring dumbfounding\nParticipants who selected the admission of not having reasons were identified as dumbfounded. Across the four scenarios 90 participants (49.1803279%) provided a dumbfounded response at least once. Table @ref(tab:tab2dumb) and Figure @ref(fig:indiafig) show the number and percentage of participants who selected each response for each scenario. Rates of dumbfounded responding for each scenario in Study 2 were significantly greater than zero, Heinz: z = 4.6, p &lt; .001; Trolley: z = 7.35, p &lt; .001; Incest: z = 6.03, p &lt; .001; Cannibal: z = 7.08, p &lt; .001, thus providing evidence for moral dumbfounding in our Indian sample. To test fot the possibility of order effects, we conducted a chi-square test for independence which revealed no significant differences in responses to the critical slide depending on when the scenario was presented, \\(\\chi\\)2(6, N = 183) = 8, p = .238.\n\n\n\n\n\nRates of each type of response for each scenario in the Indian Sample ( = 181)\n\n\n\n\nA chi-square test for independence revealed significant differences in responses to the critical slide depending on which scenario was being discussed, \\(\\chi\\)2(6, N = 183) = 36.86, p &lt; .001. Table @ref(tab:tabchisq1) shows the observed counts, expected counts and standardised residuals for each response for each scenario. For Heinz, people were significantly better at providing reasons, and significantly less likely to present as dumbfounded; while people were significantly more likely to be dumbfounded by Trolley than expected; for Incest, people were significantly less likely to provide reasons, and significantly more likely to select “There is nothing wrong” than expected; finally for Cannibal significantly fewer than expected selected “There is nothing wrong”.\nThe observed variability was not related to the type of scenario (intuition vs reasoning), with no relationship between type of scenario and response to the critical slide being observed, \\(\\chi\\)2(2, N = 183) = 3.63, p = .163.\n\n\n(#tab:tabchisq1)\n\n\nObserved counts, expected counts, and standardised residuals for each response to the critical slide depending on Scenario\n\n\n\n\nResponse\n\nHeinz\nTrolley\nIncest\nCannibal\n\n\n\n\nNothing Wrong\nObserved count\n50\n42\n75\n35\n\n\n\nExpected count\n50\n50\n51\n51\n\n\n\nStandardised residuals\n-0.03\n-1.61\n4.63**\n-2.99*\n\n\nDumbfounded\nObserved count\n21\n47\n33\n44\n\n\n\nExpected count\n36\n36\n36\n36\n\n\n\nStandardised residuals\n-3.22*\n2.31*\n-0.73\n1.62\n\n\nReason\nObserved count\n112\n95\n77\n106\n\n\n\nExpected count\n97\n97\n98\n98\n\n\n\nStandardised residuals\n2.59*\n-0.4\n-3.56**\n1.38\n\n\n\n\nNote. * = sig. at &lt; .05; ** = sig. at &lt; .001\n\n \n\nStudy 2 provided evidence that dumbfounded responding can be elicited in an Indian sample. Interestingly, the Cannibal appeared to be of more concern to the participants in this sample than the Incest scenario. Indeed, the proportion of participants selecting “there is nothing wrong” for the Incest scenario was significantly higher (75 participants; 40.7055631%) than for the other scenarios. This also appears to be higher than reported in previous studies involving WEIRD samples, however there is notable fluctuation in the selecting of this response for the Incest scenario, ranging from 16.7% (McHugh et al. 2017, Study 3a) to 32.4% (McHugh et al. 2020, Study 2). Regarding the Cannibal scenario, it appears the relative importance of death observed in Study 1 is similarly present in our Study 2 sample, pointing towards potentially important cultural differences that should be considered in future studies.\n\n\nIndividual differences\nA hierarchical linear regression was conducted to test the possible relationship between ICS (Renzhi et al. 2013), MLQ (Steger et al. 2008), and susceptibility to dumbfounding. As in Study 1a, we created a new variable by calculating the number of times each participant provided a dumbfounded response, and used this variable as a measure of participants’ susceptibility to dumbfounding. This measure was our outcome variable, and the four sub-scales of ICS, along with both sub-scales of the MLQ, were included as predictor variables. The overall model was a significant predictor of susceptibility to dumbfounding \\(R^2 = .08\\), \\(F(6, 176) = 2.42\\), \\(p = .029\\), with Vertical Individualism as the only variable making a significant contribution to the model, \\(b = -0.02\\), 95% CI \\([-0.05, 0.00]\\), \\(t(176) = -2.03\\), \\(p = .044\\), see Table @ref(tab:regressiontable).\n\n\n(#tab:regressiontable)\n\n\nStudy 2: Predictors of susceptibility to moral dumbfounding\n\n\n\n\nPredictor\n\\(b\\)\n95% CI\n\\(t\\)\n\\(\\mathit{df}\\)\n\\(p\\)\n\n\n\n\nIntercept\n1.62\n[0.51, 2.72]\n2.88\n176\n.004\n\n\nVC\n0.02\n[0.00, 0.05]\n1.96\n176\n.052\n\n\nHC\n0.00\n[-0.03, 0.03]\n-0.14\n176\n.886\n\n\nVI\n-0.02\n[-0.05, 0.00]\n-2.03\n176\n.044\n\n\nHI\n-0.03\n[-0.06, 0.00]\n-1.75\n176\n.082\n\n\nMLQ Presence\n-0.01\n[-0.04, 0.01]\n-1.21\n176\n.229\n\n\nMLQ Search\n0.01\n[-0.02, 0.04]\n0.92\n176\n.361\n\n\n\n\nWe conducted a series of logistic regressions to investigate the possible relationship between ICS (Renzhi et al. 2013) and responding to each of the scenarios individually. Response to the critical slide scenario was the dependent variable for each scenario, and the four sub-scales of the ICS were included as predictor variables.\nThe overall model did not significantly predict responses for the Heinz dilemma, \\(\\chi\\)2(12, N = 183) = 13.68, p = .322, the observed power was 0.67; neither did the model significantly predict responses for the Trolley scenario, \\(\\chi\\)2(12, N = 183) = 14.43, p = .274, the observed power was 0.7.\nInterestingly, the overall model significantly predicted responses for the Incest scenario, \\(\\chi\\)2(12, N = 183) = 26.33, p = .010, the observed power was 0.95. The overall model explained between 10.68% (Cox and Snell R square) and 14.43% (Nadelkerke R squared) of the variance in responses to the critical slide. The only significant predictors in the model were Horizontal Individualism, and Vertical Collectivism. As HI increased, participants were significantly more likely to select “there is nothing wrong” than to provide reasons for their judgement, Wald = 7.44, p = .006, odds ratio = 1.1097603, 95% CI [1.0297624, 1.1959729]. As VC increased, participants were significantly more likely to present as dumbfounded than to provide reasons, Wald = 5.01, p = .025, odds ratio = 0.9151576, 95% CI [0.8467967, 1.0036718].\nThe overall model also significantly predicted responses for the Cannibal scenario, \\(\\chi\\)2(12, N = 183) = 24.63, p = .017, the observed power was 0.94. The overall model explained between 7.16% (Cox and Snell R square) and 11.49% (Nadelkerke R squared) of the variance in responses to the critical slide. Meaning in Life: Presence (Steger et al. 2008) was the only significant predictor in the model, as Meaning in Life: Presence, increased, participants were significantly more likely provide reasons than to present as dumbfounded, Wald = 5.81, p = .016, odds ratio = 0.9258257, 95% CI [0.8695836, 0.9857055]."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#method-2",
    "href": "publications/just-wrong-or-just-weird/index.html#method-2",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Method",
    "text": "Method\n\nParticipants and design\nStudy 3 was a frequency based attempted replication of McHugh et al. (2017). The aim of Study 3 was to identify if dumbfounded responding could be evoked in a mixed sample of participants from a selection of non-WEIRD countries, primarily North Africa and the Middle East.\nAn initial sample of four-hundred-and-sixty-three participants were recruited for Study 3. Some participants did not provide full responses for all four scenarios (the total number of participants who completed the Critial Slide for all four scenarios was n = 203. In removing participants with missing data, we retained all participants who completed the Critical Slide for at least one scenario. Following this, we were left with a total sample of N = 282 (171 female, 103 male, 3 other, 5 declined to report their gender; Mage = 27.71, min = 18, max = 68, SD = 12.46).\n\n\n(#tab:tabcountries)\n\n\nParticipants by Country\n\n\n\n\nCountry\nFrequency\n\n\n\n\n1\n1\n\n\nAlgeria\n2\n\n\nBahrain\n5\n\n\nBangladesh\n2\n\n\nEgypt\n25\n\n\nindia\n1\n\n\nIndia\n25\n\n\nindiam\n1\n\n\nIndiSan\n1\n\n\nIran\n2\n\n\nIraq\n13\n\n\nIsrael\n1\n\n\nJordan\n10\n\n\nKuwait\n5\n\n\nLebanon\n34\n\n\nLibya\n14\n\n\nMorocco\n1\n\n\nOman\n1\n\n\nPakistan\n8\n\n\nPalestine\n14\n\n\nPhilippines\n13\n\n\nSaudi Arabia\n1\n\n\nSouth Africa\n1\n\n\nSri Lanka\n4\n\n\nSri Lankan\n1\n\n\nSudan\n33\n\n\nSyria\n30\n\n\nUAE\n21\n\n\nYemen\n1\n\n\n\n\nOur target sample was participants from non-WEIRD countries. As such we removed 11 participants who reported being from the UK (n = 3), USA (n = 1), Canada (n = 2), Germany (n = 1), Portugal (n = 1), Netherlands (n = 1), and participants who did not provide a country of origin (n = 2). This left a total sample of N = 271 (165 female, 98 male, 3 other, 5 declined to report their gender; Mage = 27.75, min = 18, max = 68, SD = 12.32). The breakdown of participants’ nationalities is displayed in Table @ref(tab:tabcountries). The breakdown of participants’ religions is as follows, Islam: n = 176, Christianity: n = 49, Hinduism: n = 10, other: n = 21, and 15 participants declined to provide their religion.\n\n\nProcedure and materials\nThe procedure for Study 3 was largely the same as Study 2, with some key changes. Data collection was conducted in collaboration with the Middlesex University Dubai, and participants were recruited through opportunity and snowball sampling by undergraduate students in the University. Given the potentially sensitive and offensive nature of some of the traditional dumbfounding scenarios (Incest and Cannibal), we replaced these scenarios with scenarios less likely to cause offence: Promise and Dog (see Appendix A).\nThe survey was programmed and presented using Qualtrics. The demographic information recorded additionally included participants’ nationality. We also included a filter question in an attempt to limit participation to participants from non-WEIRD countries. As in Study 2, we also included the meaning in life questionnaire (MLQ: Steger et al. 2008) and ICS (Renzhi et al. 2013). The entire study lasted twenty to twenty-five minutes."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#results-and-discussion-2",
    "href": "publications/just-wrong-or-just-weird/index.html#results-and-discussion-2",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nJudgements of the scenarios\nThe mean initial ratings for each scenario were as follows: MHeinz = 2.68, SDHeinz = 1.78; MDog = 2.15, SDDog = 1.76; MPromise = 3.88, SDPromise = 1.65; MTrolley = 2.61, SDTrolley = 1.88. The mean revised ratings for each scenario are as follows: MHeinz = 2.84, SDHeinz = 1.74; MDog = 2.21, SDDog = 1.79; MPromise = 4.37, SDPromise = 1.65; MTrolley = 2.86, SDTrolley = 1.87. The proportion of wrong, neutral, and ok, judgements for each scenario are displayed in Table @ref(tab:tab2judge).\nA paired samples t-test revealed no differences in the ratings of behaviours from time one to time two for Heinz, t(447.9965543) = -0.979, p = .328, d = 0.0922583; Dog, t(454.4625702) = -0.367, p = .714, d = 0.0342957; or Trolley, t(452.9942137) = -1.456, p = .146, d = 0.1365021. In contrast, participants revised ratings of Promise (M = 4.37, SD = 1.65) were significantly more favourable than their initial ratings M = 3.88, SD = 1.65, t(478.8640634) = -3.276, p = .001, d = 0.2987195\nA one-way ANOVA revealed significant differences in initial judgements depending on scenario, F(3, ,, , 927) = 41.37, p &lt; .001, partial \\(\\eta\\)2 = .118. Tukey’s post-hoc pairwise comparison revealed that judgements of Promise were significantly more favourable than all other scenarios: Heinz, p &lt; .001, Dog, p &lt; .001. Trolley, p &lt; .001; Heinz was rated significantly more favourably than Dog, = .009 there were no significant differences in the ratings of the other scenarios, Heinz/Trolley, p = .976, Dog/Trolley, p = .030.\nA one-way ANOVA revealed a similar pattern of differences in revised judgements depending on scenario, F(3, ,, , 908) = 63.46, p &lt; .001, partial \\(\\eta\\)2 = .173. Again, Tukey’s post-hoc pairwise comparison revealed that judgements of Promise were significantly more favourable than all other scenarios: Heinz, p &lt; .001, Dog, p &lt; .001. Trolley, p &lt; .001; and judgements of Dog were significantly more harsh than both Heinz, p = .001 and Trolley, p &lt; .001; all there were no significant differences in ratings of Heinz and Trolley, p = .999.\n\n\nMeasuring dumbfounding\nParticipants who selected the admission of not having reasons were identified as dumbfounded. Across the four scenarios 109 participants (40.2214022%) provided a dumbfounded response at least once. Table @ref(tab:tab2dumb) and Figure @ref(fig:study3fig) show the number and percentage of participants who selected each response for each scenario. Rates of dumbfounded responding for each scenario in Study 2 were significantly greater than zero, Heinz: z = 5.68, p &lt; .001; Trolley: z = 7.36, p &lt; .001; Promise: z = 4.81, p &lt; .001; Dog: z = 6.68, p &lt; .001, thus providing evidence for moral dumbfounding in our MENA sample. To test for the possibility of order effects, we conducted a chi-square test for independence which revealed no significant differences in responses to the critical slide depending on when the scenario was presented, \\(\\chi\\)2(6, N = 271) = 3.92, p = .687.\n\n\n\n\n\nRates of each type of response for each scenario in the Mixed Sample\n\n\n\n\nA chi-square test for independence revealed significant differences in responses to the critical slide depending on which scenario was being discussed, \\(\\chi\\)2(6, N = 271) = 205.55, p &lt; .001. Table @ref(tab:tabchisq1b) shows the observed counts, expected counts and standardised residuals for each response for each scenario. For Heinz, Dog, and Trolley, people were significantly more likely to provide reasons than to select “there is nothing wrong”. In contrast, for Promise participants were more likely to select “there is nothing wrong” than to present as dumbfounded, or to present as dumbfounded or provide reasons. We note that this finding may have been confounded by the responses to Promise, however the result holds when Promise is excluded from the analysis, \\(\\chi\\)2(4, N = 271) = 11.48, p &lt; .001. Study 3 provided further evidence that dumbfounded responding can be elicited in a non-WEIRD sample. Furthermore, the use of alternative scenarios provide evidence that moral dumbfounding can be elicited by a broader range of scenarios than normally demonstrated in the existing literature.\n\n\n(#tab:tabchisq1b)\n\n\nObserved counts, expected counts, and standardised residuals for each response to the critical slide depending on Scenario\n\n\n\n\nResponse\n\nDog\nHeinz\nPromise\nTrolley\n\n\n\n\nNothing Wrong\nObserved count\n46\n60\n170\n38\n\n\n\nExpected count\n79\n76\n82\n77\n\n\n\nStandardised residuals\n-5.27**\n-2.61*\n13.95**\n-6.32**\n\n\nDumbfounded\nObserved count\n45\n33\n22\n56\n\n\n\nExpected count\n39\n38\n41\n38\n\n\n\nStandardised residuals\n1.18\n-0.99\n-3.74**\n3.61**\n\n\nReason\nObserved count\n140\n130\n48\n132\n\n\n\nExpected count\n113\n109\n117\n111\n\n\n\nStandardised residuals\n4.11**\n3.22*\n-10.42**\n3.29*\n\n\n\n\nNote. * = sig. at &lt; .05; ** = sig. at &lt; .001\n\n \n\n\n\nIndividual differences\nA hierarchical linear regression was conducted to test the possible relationship between ICS (Renzhi et al. 2013), MLQ (Steger et al. 2008), and susceptibility to dumbfounding. As in Studies 1 and 2, susceptibility to dumbfounding was operationalized by calculating the number of times a participant provided a dumbfounded response. With this measure as the outcome variable, we included the four sub-scales of ICS, along with both sub-scales of the MLQ, as predictor variables in a multinomial logistic regression model. The overall model was not a significant predictor of susceptibility to dumbfounding \\(R^2 = .06\\), \\(F(6, 179) = 2.03\\), \\(p = .064\\) in Study 3.\nWe conducted a series of logistic regressions to investigate the possible relationship between ICS (Renzhi et al. 2013) and MLQ (Steger et al. 2008), and responding to each of the scenarios individually. Response to the critical slide scenario was the dependent variable for each scenario, and the four sub-scales of the ICS, along with the two sub-scales of the MLQ were included as predictor variables.\nThe overall model predicted responses for the Heinz dilemma, \\(\\chi\\)2(12, N = 205) = 22.7, p = .030, the observed power was 0.91. neither did the model significantly predict responses for the Trolley scenario, \\(\\chi\\)2(12, N = 205) = 13.45, p = .337, the observed power was 0.66. The overall model explained between 6.94% (Cox and Snell R square) and 10.25% (Nadelkerke R squared) of the variance in responses to the critical slide. The only significant predictors in the model were Vertical Individualism, and Meaning in Life: Search. As VI increased, participants were significantly more likely to select “there is nothing wrong” than to provide reasons for their judgement, Wald = 6.3, p = .377, odds ratio = 0.9733865, 95% CI [1.0151915, 1.1302198]. As Meaning in Life: Search increased, participants were significantly more likely to present as dumbfounded than to provide reasons, Wald = 5.23, p = .638, odds ratio = 1.0216129, 95% CI [1.0116615, 1.1621541].\nThe overall models did not significantly predict responses for any of the other scenarios: Trolley, \\(\\chi\\)2(12, N = 195) = 13.45, p = .337, the observed power was 0.66; Promise, \\(\\chi\\)2(12, N = 195) = 18.27, p = .108, the observed power was 0.83; Dog, \\(\\chi\\)2(12, N = 195) = 9.43, p = .666, the observed power was 0.48."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#individual-differences-and-dumbfounded-responding",
    "href": "publications/just-wrong-or-just-weird/index.html#individual-differences-and-dumbfounded-responding",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Individual Differences and Dumbfounded Responding",
    "text": "Individual Differences and Dumbfounded Responding\nIn addition to testing for the existence of moral dumbfounding in non-WEIRD samples, we investigated the possible relationship between dumbfounded responding and ICS (Renzhi et al. 2013: Studies 1 and 2) and MLQ (Steger et al. 2008: Study 2). Study 1 revealed no significant relationship between ICS and (a) overall susceptibility to dumbfounding, or (b) responses for each scenario individually.\nIn Study 2 we found that Vertical Individualism (Renzhi et al. 2013) was related to susceptibility to dumbfounded responding, with those scoring high in Vertical Individualism, being less likely to present as dumbfounded. It is possible that this observed relationship emerged as a result of participants’ relative motivations to do well in the task of providing reasons. Previous research (McHugh et al. 2017) provides suggestive evidence that a dumbfounded response is aversive, that people are motivated to appear consistent. This consistency can be successfully achieved by providing a reason for a judgement, or by revising a judgement and selecting “there is nothing wrong”. In contrast, providing a dumbfounded response may be seen as failing to present as consistent. The items in the Vertical Individualism sub-scale appear to provide a measure of people’s motivations to do well in relation to others (e.g., “It is important that I do my job better than others”; “When another person does better than I do, I get tense and aroused”). However, it is possible that this sub-scale additionally provides an indication of people’s motivations for success (e.g., “Winning is everything”). As such people who are more motivated to “succeed” in general, may be more motivated to avoid the perceived failure associated with a dumbfounded response. This interpretation is merely speculative, and follow up studies should investigate this in more detail.\nIn Study 2 we also found evidence that responses to specific scenarios were related to to the individual difference variables measured. Responses to the Incest scenario were predicted by Horizontal Individualism, and Vertical Collectivism (Renzhi et al. 2013). It is possible that this relationship emerges as a result of the content of the scenario rather than providing an insight into the cognitive processes involved in moral dumbfounding. Participants scoring higher in Horizontal Individualism were more likely to select “There is nothing wrong” than to provide a reason for their judgement. It appears that HI is linked with the valence of participants’ judgements of Incest rather than whether or not it leads to dumbfounding. A key consideration in the Incest scenario is the importance of individual autonomy. Similarly, the items in the HI sub-scale appear to relate to individual autonomy (e.g., “I rely on myself most of the time; I rarely rely on others,” “I often do ‘my own thing’,” Renzhi et al. 2013). As such it is not surprising that participants who score highly on HI, place higher value on individual autonomy when considering the Incest scenario.\nFurthermore, three of the four Vertical Collectivism (Renzhi et al. 2013) items specifically relate to the importance of the family. thus providing an indirect measure of the degree to which people value the family unit. Participants who scored higher in VC were more likely to present as dumbfounded than to provide reasons for their judgement of the Incest scenario. It is likely that participants who score higher in VC regard the family as particularly important, and the Incest scenario presents an affront to the family. As such, these participants may perceive the actions of Julile and Mark as a threat to something that they value, but may not be able to articulate this as a reason for their judgement (e.g., it is too abstract, or they do not think it is an “acceptable” reason).\nFinally, Meaning in Life: Presence (Steger et al. 2008), was related to responses to the Cannibal scenario, such that participants who scored higher in Meaning in Life: Presence were more likely to provide reasons for their judgements then to present as dumbfounded by Cannibal. Again this is likely due to the specific content, rather than informing the cognitive processes involved. It could be argued that the Cannibal scenario involves considerations about the value of life. Participants who score higher in Meaning in Life: Presence, have given consideration to the meaning (and by extension, value) of life (e.g., “I understand my life’s meaning”). It seems that this reflection life’s meaning may provide people with the necessary justifications/arguments/resources to articulate why they disapprove of Cannibal.\nStudy 3 did not find any relationship between susceptibility to dumbfounded responding and either ICS (Renzhi et al. 2013) or MLQ (Steger et al. 2008). We did find that Vertical Individualilsm predicted selecting “There is nothing wrong” for the Heinz dilemma. The content of the scenario may provide an explanation for this observed relationship. Participants were judging the behaviour of the Druggist, who charged an extremely high price for the drug he developed. The druggest’s behaviour is consistent with individualistic values, and it is not surprising that participants who score higher on this individualism measure endorse the behaviour of the druggist. Study 3 also found that participants who scored higher in Meaning in Life: Search were more likely to be dumbfounded than to provide reasons for their judgement."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#limitations-and-future-directions",
    "href": "publications/just-wrong-or-just-weird/index.html#limitations-and-future-directions",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Limitations and Future directions",
    "text": "Limitations and Future directions\nA key limitation in the current studies is the sample make up. Participants in Study 1 were recruited through a Chinese university, participants in Study 2 were university graduates, who also were proficient in English, and participants in Study 3 were also proficient in English, and recruited through snowball sampling in a University setting. This means that none of our samples are representative of their respective population. Furthermore, the samples involved either university graduates, or those currently studying in a university setting, and as such this high level of education is a significant challenge to our stated aim of recruiting from non-WEIRD samples.\nThe moral foreign language effect (Cipolletti, McFarlane, and Weissglass 2016) means that the use of an English language survey in Studies 2 and 3 is another potential limitation. Previous research has shown that people appear to make more utilitarian judgements when moral scenarios are presented in another language (Costa et al. 2014). In the case of the Intuition scenarios, this could potentially lead to a higher number of participants selecting “There is nothing wrong”, rather than presenting as dumbfounded. Despite this potential confound, dumbfounded responding was observed for all scenarios in Studies 2 and 3.\nOur studies were not intended as a systematic investigation of cultural differences in evaluation of specific moral content (there are other research programmes dedicated to this, e.g., Haidt and Joseph 2008; Shweder et al. 1997; Narvaez 2016). Here, we applied existing methods in three novel contexts, to assess whether or not moral dumbfounding can be elicited in these under-studied contexts. We found evidence for moral dumbfounding in a Chinese sample, an Indian sample and a mixed sample. Our results provided some evidence for cultural variation (e.g., the relative importance of the death taboo), that may inform the development of future research programmes."
  },
  {
    "objectID": "publications/just-wrong-or-just-weird/index.html#footnotes",
    "href": "publications/just-wrong-or-just-weird/index.html#footnotes",
    "title": "Just wrong? Or just WEIRD?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOne scenario (Incest) depicted an act of consensual sibling incest, using contraception; another (Cannibal) described an act of cannibalism involving a laboratory cadaver due to cremated.↩︎\nHeinz could not afford drugs priced at 10 times the cost price, and steals drugs to save his wife’s life.↩︎\nParticipants do not articulate or consistently apply harm-based reasons/principles, nor do they articulate norm-based reasons/principles with sufficient consistency to provide evidence that these reasons are guiding their judgements in the dumbfounding paradigm (see McHugh et al. 2020).↩︎\nWe note that, although not statistically significant, the Cannibal scenario appeared to elicit more dumbfounded responding in the Chinese sample↩︎"
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html",
    "href": "publications/moral-judgment-as-categorization/index.html",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "",
    "text": "Observed variability and complexity of judgments of ‘right’ and ‘wrong’ cannot be readily accounted for within extant approaches to understanding moral judgment. In response to this challenge, we present a novel perspective on categorization in moral judgment. Moral judgment as categorization (MJAC) incorporates principles of category formation research while addressing key challenges of existing approaches to moral judgment. People develop skills in making context-relevant categorizations. They learn that various objects (events, behaviors, people, etc.) can be categorized as morally ‘right’ or ‘wrong’. Repetition and rehearsal results in reliable, habitualized categorizations. According to this skill formation account of moral categorization, the learning and the habitualization of the forming of moral categories, occurs within goal-directed activity that is sensitive to various contextual influences. By allowing for the complexity of moral judgments, MJAC offers greater explanatory power than existing approaches while also providing opportunities for a diverse range of new research questions."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#type-token-interpretation",
    "href": "publications/moral-judgment-as-categorization/index.html#type-token-interpretation",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Type-Token Interpretation",
    "text": "Type-Token Interpretation\nBarsalou (2003; 1999) proposes that the learning and maintaining of categorizations occurs through the process of type-token interpretation, defined as the binding of specific tokens (category members) to general types (category). For the category THINGS TO PACK INTO A SUITCASE (Barsalou 2003; Barsalou, Solomon, and Wu 1999), this entails identifying a given item (token) as something that you pack or do not pack into a suitcase (type). Crucially, this process can be implicit, simply involving treating an item as a member or not a member of a particular category within an appropriate context for action, in this case, packing it or not packing it. Skill in forming the categories emerges from repetition and rehearsal of the type-token interpretation; people become skilled at deploying categories that they encounter frequently."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#context-sensitivity",
    "href": "publications/moral-judgment-as-categorization/index.html#context-sensitivity",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Context Sensitivity",
    "text": "Context Sensitivity\nType-token interpretation occurs every time a given token is encountered such that every categorization of a given token (object/item/event) is subject to contextual influences of the current situation. This results in dynamic and complex categories, without necessary and sufficient conditions, and without even stable best exemplars or prototypes. The properties of an object relevant to that particular context become salient, and the categorization process is accented by the details of the particular circumstances in which the actions are being taken. Stable or recurring properties (both object and contextual) can be learned, and their identification or recognition become a part of the subsequent engagement in the relevant goal-directed activity and the enactment of different relevant type-token interpretations of objects. This is dependent on the experience and learning history of the individual and not inherent in the categories themselves, however, which is what gives rise to the complex, dynamic aspects of concepts central to Barsalou’s approach.\nConsider a study by Barsalou (1982). Participants were presented with a series of sentences involving particular items. For example: “The basketball was used when the boat sank”; or “The basketball was well worn from much use” (Barsalou 1982; see also Barsalou 2003). Following each sentence, participants were asked to verify whether particular properties were true for the item; for example whether or not “floats” is true for “basketball” following reading either of the above sentences. The fact that basketballs float is relevant to the first sentence, and thus this property is inferred from reading this sentence. In the second sentence, this property (while still true for basketball) is irrelevant and does not become salient by reading the sentence. Thus, while what is true for basketball does not change depending in the situation, the properties that are inferred in a given instance do. This is evident in that participants were faster at verifying “floats” as true for basketball following reading the first sentence than the second (Barsalou 1982, 2003). Other studies have yielded similar results, demonstrating that different sentences cause different properties to become salient depending on these properties’ relevance to the given sentence (Greenspan 1986; Tabossi 1988; Yeh and Barsalou 2006). The contextually relevant inferences made when we encounter category members are not limited to object properties but can also include situational and introspective inferences (e.g., Barsalou and Wiemer-Hastings 2005)."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#habitualization",
    "href": "publications/moral-judgment-as-categorization/index.html#habitualization",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Habitualization",
    "text": "Habitualization\nHighly skilled categorizations become habitualized (automatic/intuitive) to the point where these categorizations provide an illusion of “stable categories”. Typically, these “stable categories” mirror real-world categories or classes and social norms that are frequently and reliably encountered in day-to-day life. This reflects the use of these categories in (a) interacting effectively with the world and (b) communicating with others. Natural kinds and social norms would constitute prototypical classes of such frequently encountered and reliably implemented categories (e.g., Keil, Rozenblit, and Mills 2004). In some cases, categories that can be referenced to natural kinds may take on the causal rules that distinguish natural kinds. For example, fruit is distinct from vegetables in that the agreed scientific classification of fruit (in our culture) is as containing the seeds. This causal rule is not necessarily operationalized in everyday interactions with fruit and vegetables; however, in certain situations, it may be referenced to aid in the classification of ambiguous items.\nMore abstract categories are more difficult to define because there may not be a set of causal rules governing membership to draw on. There is a large body of literature documenting the search for causal rules or identifying characteristics of particular emotion categories, for instance, but no approach has fully answered this question (Griffiths 1997; see also Barrett, Wilson-Mendenhall, and Barsalou 2014; Mesquita, Barrett, and Smith 2010).\nBarsalou and Wiemer-Hastings (2005) directly address this question of abstract concepts, demonstrating that the content of increasingly abstract concepts contains increasingly situational and introspective focus. Consider the possible inferences associated with the categorization of SOFA versus FREEDOM. Various properties of SOFA will remain relatively stable across contexts. However, to make sense, any conceptualization of FREEDOM needs to be embedded in a specific situational (e.g., freedom from oppression) or introspective (e.g., feeling free) context. Inferences regarding FREEDOM are necessarily more context-dependent. This results in greater situational or introspective inferences being made for abstract categories, while concrete categories allow for more object-level inferences.\nThe abstract nature of moral categories means they are similarly rich in situational and introspective inferences. That is, whether a particular behavior is viewed as right or wrong varies depending on the situation and may be categorized as right or wrong in different ways, specific to the context and the goal-directed activity in which the person is engaged. The link of introspection and the abstract nature of moral categories is supported by recent approaches that stress the tight coupling of moral judgments and emotions (e.g., Cameron, Payne, and Doris 2013; Huebner, Dwyer, and Hauser 2009; Royzman et al. 2014; Rozin et al. 1999; Valdesolo and DeSteno 2006).\nAs with the mapping of habitualized categorizations on to real-world natural kinds, moral categories may appear to follow principles or rules, reflecting social norms of society or a specific social group. A behavior that is encountered frequently and consistently identified as MORALLY RIGHT, may emerge as a “good example”, or a Token2 for MORALLY RIGHT. Over time, people develop a range of Tokens for the categories MORALLY RIGHT (and for MORALLY WRONG). Furthermore, similar behaviors may become categorized together, for example, continued identification of “hitting people” as WRONG, and “kicking people” as WRONG may lead a person to form a superordinate category CAUSING HARM TO PEOPLE, which is consistently identified as WRONG. This may then be taken a step further, and “don’t harm people” and “don’t harm animals” may merge to form INFLICTING HARM, which is consistently identified as WRONG.\nThe emergence of habitualized, highly generalized, morally grounded Tokens may form the basis of what we call values. Furthermore, as more and more Tokens are developed and become increasingly generalized, these generalized Tokens become arranged hierarchically in terms of severity. This essentially becomes our “moral code”. There is not necessarily an underlying set of rules (or moral principles) governing this moral code, it is based on a large collection of Tokens, and a process of categorization that is sensitive to context and on-going actions. Some of the generalized Tokens (values) may appear to exhibit sufficient powers of “governance” to constitute rules. However, these are not true rules; as with the mapping of stable categorizations onto natural kinds, it may be possible to construct plausible (and often true) causes for the associations that define many categories, however, the process of categorization remains grounded in type-token interpretation (rather than the rules that can be inferred from referencing observable categories, Barsalou 2003; Barsalou and Wiemer-Hastings 2005). MJAC provides a framework for the emergence of what appears to be relative stability in categorization while simultaneously accounting for the observed variability and context-dependency that pose a challenge to existing theories of moral judgment."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#moral-dumbfounding",
    "href": "publications/moral-judgment-as-categorization/index.html#moral-dumbfounding",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Moral Dumbfounding",
    "text": "Moral Dumbfounding\nThe processes underlying moral judgment, according to MJAC, predict the phenomenon of moral dumbfounding. Moral dumbfounding occurs when people defend a moral judgment even though they cannot provide a reason to support it (Haidt 2001; Haidt, Björklund, and Murphy 2000; McHugh et al. 2017). Typically, moral dumbfounding occurs for harmless taboo behaviors (consensual incest, cannibalism involving a body that is already dead). Consider the learning of taboo behaviors as wrong through type-token interpretation and typical interaction with such behavior. The taboo nature of these topics means that they are consistently identified as morally wrong, without much discussion [the Scottish public petitions committee notably dismissed a call to legalize incest with no discussion at all; see Sim (2016)]. This leads to a high degree of stability in categorizing them as WRONG. However, while other behaviors may be discussed or disputed, generating a deeper knowledge surrounding the rationale for identifying as right or wrong, the taboo nature of these behaviors prevents them from being discussed. This means that a typical encounter with such behavior involves little more than identifying it as wrong, possibly with an expression of disgust, and changing the subject (Sim 2016). Identifying causal rules that govern the behavior’s membership of the category MORALLY WRONG is likely problematic, in that a person would have limited experience at attempting to do so. In this view, type-token interpretation of taboo behaviors logically leads to moral dumbfounding.\nPhenomena similar to moral dumbfounding have been observed in the non-moral domain. While these have not been explicitly identified as “dumbfounding” we suggest that dumbfounding also occurs for categories other than MORALLY WRONG. For example, Boyd and Keil (Boyd 1989, 1991; Keil 1989; see also Griffiths 1997) found that participants struggled to explain their reasons for categorizing an imagined creature as A CAT or NOT A CAT. Descriptions of participants’ responding in such situations bear a striking similarity, whether the target categorization is in the moral domain or not. In discussing their work on the illusion of explanatory depth, Keil, Rozenblit, and Mills (2004) describe the sensation of being “surprised by our inability to explain something” (2004, 277). Similarly, in discussing moral dumbfounding, Haidt describes how people “express surprise at their inability to find supporting reasons” (Haidt 2001, 817). The illusion of explanatory depth and moral dumbfounding are likely phenomena with common underpinnings."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#categorizing-people-versus-categorizing-actions",
    "href": "publications/moral-judgment-as-categorization/index.html#categorizing-people-versus-categorizing-actions",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Categorizing people versus categorizing actions",
    "text": "Categorizing people versus categorizing actions\nIn line with Barsalou and Wiemer-Hastings (2005), we have been describing the cognitive processes in relation to the development of the abstract categories MORALLY WRONG and MORALLY RIGHT. In reality, people do not deal with these abstractions, rather moral categorization is situated in specific contexts, occurring as part of goal-directed behavior. In some situations, we may identify specific actions as morally questionable or morally praiseworthy, while in others, we may identify specific actors as morally questionable or morally praiseworthy. While, the action or actor may belong to the super-ordinate category MORALLY WRONG, or MORALLY RIGHT (or NOT MORALLY RELEVANT), it is likely that in everyday interactions people are more concerned with the subordinate categories in question, for example, BAD/GOOD PERSON or BAD/GOOD ACTION.\nPrevious authors have argued that when people make moral judgments, the primary evaluation is of the character of the person committing the act (e.g., Uhlmann, Pizarro, and Diermeier 2015; Landy and Uhlmann 2018; see also Siegel, Crockett, and Dolan 2017; Siegel et al. 2018). MJAC does not adopt this position, rather we recognize that there are many potential contextual factors that influence whether the target of any given moral categorization is the actor or on the action (or both). The variability relating to the target of moral categorization can influence which super-ordinate category is eventually implicated, that is, whether the final judgment is MORALLY WRONG, or MORALLY RIGHT (or NOT MORALLY RELEVANT); for example, if a corrupt politician helps a neighbor with shopping, even though this action may be categorized as good, the actor is likely to still be categorized as a bad."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#moral-categorization-involving-known-others",
    "href": "publications/moral-judgment-as-categorization/index.html#moral-categorization-involving-known-others",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Moral Categorization Involving Known Others",
    "text": "Moral Categorization Involving Known Others\nMJAC assumes that moral categorization is dynamic and context-dependent. We propose that consideration of the goal-directed nature of moral categorizations provides a key insight into some of the contexts that may affect the target of a given categorization. Consider the following two scenarios:\n\nyou find out that a colleague has been fired for stealing from your employer - they have been bringing home office equipment for their own personal use, and they have been exaggerating their expense claims;\na close friend of yours reveals to you that they have been stealing from their employer - they have been bringing home office equipment for their own personal use, and they have been exaggerating their expense claims.\n\nIt seems intuitive that people should judge (b) differently from (a), and we predict that people will be more lenient in their judgments of (b) than of (a). Despite the historical paucity of research investigating the influence of the relationship between the person making a judgment and the apparent perpetrator Feltz and May (2017), recent findings support this prediction (Forbes 2018; Heiphetz and Craig 2020; Hofmann et al. 2014; Lee and Holyoak 2020; McManus, Kleiman-Weiner, and Young 2020; Weidman et al. 2020). Several studies have demonstrated that people appear to be more lenient in their judgments of people they are close to versus strangers (Forbes 2018; Hofmann et al. 2014; Lee and Holyoak 2020; Weidman et al. 2020). Further evidence that close others are judged differently to strangers has been found by Heiphetz and Craig (2020). They showed that a tendency to dehumanize racists (and sexists) is associated with a greater tendency to view strangers’ ambiguous actions as racially biased (or sexist), but not the ambiguous actions of friends (Heiphetz and Craig 2020). The importance of accounting for possible relationships in moral judgment research is not limited to the relationship between the observer and the relevant actors. Recent work has shown that people are judged more favorably for helping strangers than helping kin, while a failure to help kin is judged more harshly, suggesting a stronger obligation towards kin than towards strangers (McManus, Kleiman-Weiner, and Young 2020).\nA further prediction is that for (b), the target of categorization will be the action rather than the actor. People are motivated to see close others positively (Forbes 2018; Murray, Holmes, and Griffin 1996a, 1996b). If faced with a situation where a close other committed a moral transgression, people would be motivated to avoid making a negative judgment of the person (Ditto, Pizarro, and Tannenbaum 2009; Murray, Holmes, and Griffin 1996a, 1996b; Proulx and Inzlicht 2012). One way to avoid this is to make the target of the categorization the action rather than the actor.3\nIn making the action the target of the categorization rather than the actor, people can reduce the degree to which they view their close others negatively. However, this strategy is implemented in addition to making judgments that are more lenient. Making more lenient judgments about specific transgressions based on the actor introduces context-specific inconsistency in regard to the categorization of that transgression. MJAC predicts that this inconsistency may threaten the long term stability of the categorization. Specifically, we predict that leniency towards close others for a specific behavior should eventually lead to more general leniency towards that behavior. This development of more general leniency should occur independently of deliberate attempts to present as consistent (although it could be accelerated by attempts to be consistent). For instance, an increased tolerance of “locker room talk” by people who would otherwise disapprove of sexism."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#moral-categorization-involving-unknown-others",
    "href": "publications/moral-judgment-as-categorization/index.html#moral-categorization-involving-unknown-others",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Moral Categorization Involving Unknown Others",
    "text": "Moral Categorization Involving Unknown Others\nDrawing on the goal-directed nature of moral categorization, we further predict any prospective relationships between the observer and the actor. Success in social interactions involves successfully predicting the actions of others (Waytz and Young 2018). As such, a key goal of moral categorization is to distinguish “good” from “bad” people (Uhlmann, Pizarro, and Diermeier 2015), by attempting to identify a person’s moral “essence” (e.g., Dunlea and Heiphetz 2020; Heiphetz and Dunlea 2019), or “character” (N. Klein and O’Brien 2016; Siegel, Crockett, and Dolan 2017; Siegel et al. 2018). This enables people to establish relationships or pursue continued interactions with “good” people, and to limit their interactions with “bad” people (or at least treat interactions with “bad” people with caution).\nThus, evaluations of strangers’ actions should show a bias for categorizing the actor rather than the action. Furthermore, this bias should be more pronounced in situations when people anticipate that there may be follow-up interactions with the stranger. Research on reciprocity and repeated interactions with strangers or partners (e.g., Fehr and Gächter 2000, 2003) provides an ideal framework that could be adapted to test this prediction. In conditions where participants are partnered, their initial evaluations should be more focused on their partner’s character than in conditions where participants interact with a new “stranger” for each trial.\nDrawing on the well-established tendency for negative information to be weighted more heavily than positive information (e.g., Kahneman and Tversky 1979; Rozin and Royzman 2001; A. Smith 1759), we predict that people will be more sensitive to negative actions than positive actions. Indeed, this has been shown to be the case. N. Klein and O’Brien (2016) presented participants with vignettes describing changes in patterns of behavior. Participants were asked to indicate how many consecutive instances of the new behavior would need to occur to convince them that the actor’s “moral character had transformed” (N. Klein and O’Brien 2016, 152). Participants perceived negative transformations much quicker than positive transformations, which was true for commencing negative behaviors and ceasing positive behaviors (N. Klein and O’Brien 2016). A general heightened sensitivity to negative information means that people appear to be quicker to categorize an actor as “bad” (vs. “good”).\nThis identification of “bad” actors appears to be present from an early age, such that even pre-verbal infants show a preference for good actors over bad actors (Hamlin, Wynn, and Bloom 2007, 2010; Hamlin and Wynn 2011; cf. Margoni and Surian 2018; Schlingloff, Csibra, and Tatone 2020; Steckler, Woo, and Hamlin 2017). We do not claim that infants in these studies have acquired fully developed categories of MORALLY WRONG and MORALLY RIGHT, and that they assign different actors to these categories. Rather, type-token interpretation predicts that category members should be treated as similar, independently of whether or not a person can describe the category, or even the relationship between the category members.4 Previous research has demonstrated that we implicitly treat similar items as similar even though we may not be able to articulate what makes them similar (e.g., recognising ‘good decks’ from ‘bad decks’ in the Iowa Gambling Task: Bechara and Damasio 2005; Damasio 1994; or implicit identification of abstract patterns, Proulx and Heine 2009; Whitson and Galinsky 2008).\nThese findings should not be interpreted as categorizations of “bad” actors are more stable than categorizations of “good” actors. Indeed, the opposite is the case (Siegel et al. 2018), where beliefs about “bad” agents are more volatile than beliefs about “good” agents. MJAC explains this volatility in the categorization of “bad” agents relative to “good” as emerging due to the relative consistency with which categorizations are made. As noted by Siegel et al., “bad people often behave morally, but good people rarely behave immorally” (2018, 750). The contexts in which actors are categorized as “good” are more consistent than the contexts in which they are categorized as “bad”. This consistency makes the categorization “good” actor a more stable categorization than “bad” actor. This apparent stability categorizing “good” actors relative to “bad” actors can also be seen in research on moral essentialism, people show a greater tendency to attribute essence based on moral goodness than moral badness (Heiphetz 2020; Newman, Bloom, and Knobe 2014).\nThe findings discussed above reflect the goal-directed nature of moral categorization. Specifically, people are motivated to understand and predict others’ actions to guide future interactions (Uhlmann, Pizarro, and Diermeier 2015; Waytz and Young 2018). If we understand that some behaviours are associated with positive experiences and some with negative outcomes, then it is not surprising that we show a preference for people who behave in a more positive way, even from a very young age (Hamlin and Wynn 2011).\nInterestingly, distinguishing between categorizing an action or categorizing an actor has implications for behavior, specifically when the actor in question is the self. In a series of studies by Bryan, Adams, and Monin (2013), participants took part in tasks in which cheating for financial gain (at the expense of the experimenter) was possible. When task instructions discouraging cheating used the term “cheater”, participants’ rates of cheating was significantly lower than when the term used was “cheating”. Committing an action that might fall into the category MORALLY WRONG is less aversive than being categorized as a BAD PERSON."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#beyond-unidimensional-conceptions-of-morality",
    "href": "publications/moral-judgment-as-categorization/index.html#beyond-unidimensional-conceptions-of-morality",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Beyond Unidimensional Conceptions of Morality",
    "text": "Beyond Unidimensional Conceptions of Morality\n\nDual-Process Theories of Moral Judgment\nThe three dual-processing theories of moral judgment that we address here each use some form of one-to-one mapping between a key dimension of moral judgment and the underlying differences in information processing expressed in that dual-processing account. Identification of the moral dimension is usually made through categorization of responses to challenges such as the trolley problem (the moral judgment literature is, unfortunately, replete with vehicular homicides).\nFor instance, Greene’s theory describes the distinction between deontological and consequentialist outcomes to moral judgments as a qualitative difference in processing, where deontological judgments are grounded in implicit, emotional, automatic processing, and consequentialist judgments involve deliberate, controlled processing (Greene 2016). Byrd and Conway’s (2019) softer approach is less dichotomous, such that deontological judgments are viewed as involving relatively more affective processing. For both Crockett’s (2013) and Cushman’s (2013) model-free vs. model-based accounts the logic is similar, though the emphasis is reversed. While for Greene (2016), and Byrd and Conway (2019), the form of processing drives the form of moral judgments, for both Cushman and Crockett, the framing of the moral task drives the kind of processing that is likely to result. Crockett and Cushman both avoid the simple deontological/consequentialist divide but focus instead on evaluating either moral actions or moral outcomes, which give rise to model-free or model-based judgments respectively. As with Greene and Byrd and Conway, however, they hold a stable one-to-one mapping between this dimension of the content of the moral judgment and the underlying processing.\nThe clarity of these mappings is appealing, but we argue here that the complexity and inconsistency of the findings in the existing literature on these relationships are disconfirming for these accounts (e.g., De Neys and Białek 2017; Gamez-Djokic and Molden 2016; Gubbins and Byrne 2014; Körner and Volk 2014; McPhetres et al. 2018; Pizarro and Bloom 2003; Reynolds and Conway 2018). We note that research on categorization also predicts reliably distinguishable patterns of response along the lines of many dual-processes accounts, distinguished by individual learning histories and experience in performing given categorizations in different circumstances. For clarity and consistency, we will refer to this distinction as one between habitual versus deliberative responses, positioned at either end of a continuum (Kruglanski and Gigerenzer 2011).\nWe follow the categorization research in identifying as a key dimension the extent to which specific categorizations (instances of type-token interpretations) are well-rehearsed and thus, become fluent, stable, and habitual within frequently enacted goal-directed activities (Barsalou 1999, 2003, 2017). Less experience with a particular type-token interpretation will result in less consistent deployment of the category and demand more deliberative consideration of the situation and appropriate action.\nTherefore, this key dimension in underlying processing is not predicted by MJAC to map straightforwardly onto any aspect of task content or framing in moral judgment, such as habitual judgments being deontological while deliberative ones are consequentialist. While well-worn deontic exhortations (“It’s wrong to hurt people”, “Thou shalt not kill”, “You shouldn’t hit your sister”) will no doubt develop a strong habitual foundation, within the MJAC framework, consequentialist judgments that are well-practiced will also be supported by habitual responses [associated with quick intuitive or affective reactions to moral judgments as studied by deneys_dual_2017; Gubbins and Byrne (2014); Reynolds and Conway (2018)]. Consequentialist reasoning, likely requiring explicit moral argument to arise, may be somewhat less commonly practiced, but also some deontological situations have novel characteristics that therefore also require deliberation [as illustrated by the likes of Gamez-Djokic and Molden (2016); Körner and Volk (2014); McPhetres et al. (2018); Pizarro and Bloom (2003)).\nThis variation in the relationship between deontological and consequentialist judgments and the ways (habitual vs. deliberative) they get made undermines both Greene’s and Byrd and Conway’s accounts. Neither Cushman (2013) nor Crockett (2013) connect the moral perspective with a specific form of processing. Still, they do map the distinction between action- and outcome-focused judgments onto the distinction between model-free and model-based processing. While this can accommodate such variability in deontological or utilitarian perspectives depending on circumstances, it runs afoul of what is termed the “doctrine of double effect” (Doris 2010; Mikhail 2000). The doctrine of double effect concerns the difference between causing harm as a means to an end being seen as different to causing harm as a side-effect of achieving the same ends, even when the actions taken are the same (e.g., Mikhail 2000; see also R. A. Klein et al. 2017). It is unclear what about such cases could trigger a difference in processing that would explain differential judgments for model theories. These theories are also challenged by versions of the trolley problem presented in virtual reality environments (Francis et al. 2016), where a usual pattern of responding (preference for inaction over pushing someone onto the track to stop the tram) was reversed. This runs directly counter to the predictions of the action-outcome mapping to form of processing made by these model theories. However, the shift to a more deliberative, calculating mode of thinking is perhaps less surprising for MJAC, given the novelty of the mode of presentation.\nAccording to MJAC, the making of moral judgments is dynamical, and context-dependent, and occurs as part of goal-directed activity; thus we should expect to see this observed variability that poses a challenge to any stable mapping between content and form of processing or judgment outcome. MJAC also assumes that relative stability in moral categorizations emerges as a result of continued and consistent type-token interpretation, such that particular categorizations become habitualized (and hence intuitive). Thus, we should expect a variety of contextual factors affecting people’s moral judgments, not limited to any single key dimension. Constraints on space mitigate against exploring each of these in detail. Still, the sheer range of such factors that have been reported offers compelling evidence that whatever underlies variation in moral judgment is a complex of issues and is not unidimensional in any given situation [the reader is referred to the wealth of literature examining such factors as emotional influences, Cameron, Payne, and Doris (2013); intentionality, evitability, benefit recipient, Christensen et al. (2014); Christensen and Gomila (2012); action-outcome distinction Crockett (2013); cushman_action_2013; trustworthiness and social evaluation Everett, Pizarro, and Crockett (2016); Everett et al. (2018); personal-impersonal distinction, Greene et al. (2001); doctrine of double effect, Mikhail (2000); level of physical contact, Valdesolo and DeSteno (2006); order effects, Wiegmann, Okan, and Nagel (2012)).\n\n\nTheory of Dyadic Morality\nThe theory of dyadic morality (TDM, Gray, Young, and Waytz 2012) that has recently been presented by Gray and colleagues would also seem to be grounded in generic categorization processes [Gray, Waytz, and Young (2012), p. 206; gray_mind_2012, p. 102; Schein and Gray (2018), p. 42]. As such, the approach is not heavily focused on a single processing dimension explaining moral judgment (or the variation therein). While TDM has not been identified with a specific theory of categorization, Gray et al. ((2012, 206) make reference to “prototypes or exemplar sets”, and it is here that the divergence with MJAC becomes clear. Barsalou (2003) summarized a range of findings indicating that neither prototype nor exemplar approaches can adequately explain the dynamic and variable nature of performance in categorization tasks.\nMore problematically, though TDM has been linked to exemplar and prototype theories, its proponents highlight moral situations as those involving a set of necessary and sufficient conditions - those which involve “an intentional agent causing damage to a vulnerable patient” (Schein and Gray 2018, 33), or “an intentional moral agent and a suffering moral patient (Gray, Young, and Waytz 2012, 101). Such appeals to essentialism are at odds with decades of research demonstrating dynamism and context-dependency in categorization (Barsalou 1982, 1987, 2003, 2017; Harman, Mason, and Sinnott-Armstrong 2010; McCloskey and Glucksberg 1978; Mervis and Rosch 1981; Oden 1977; Rosch 1975; Rosch and Mervis 1975; Stich 1993), and returns us to a unidimensional approach to moral judgment, this time identifying the moral character of a situation as the extent to which it involves harm. While intuitively appealing, this does not bear empirical scrutiny.\nProponents of TDM argue that even in ostensibly harmless moral transgressions, people perceive harm (Gray, Schein, and Ward 2014). This perception of harm guides participants’ judgments in moral dumbfounding scenarios (Schein and Gray 2018; Schein 2020). Dumbfounding is displayed when people maintain a moral judgment in the absence of a means of justifying their judgment, usually evoked by vignettes of supposedly “harmless wrongs” such as consensual incest or cannibalism of an already-dead body (Haidt, Björklund, and Murphy 2000; McHugh et al. 2017). Schein and Gray (2018) point to a series of studies by Royzman, Kim, and Leeman (2015), to support their appeal to perceived harm in the “moral dumbfounding” paradigm. royzman_curious_2015 investigating the case of consensual incest, included additional questions that appear to demonstrate that people’s judgments were (at least in part) grounded in perceptions of harm.\nHowever, more recent dumbfounding work fails to support the TDM perspective on this matter (McHugh et al. 2020). In addressing specific methodological limitations of the Royzman, Kim, and Leeman (2015) study, McHugh et al. (2020) found that people do not cite harm as a reason for their judgment. Participants were asked to judge a vignette describing consensual incest, asked to provide reasons for their judgment, and then provided with the questions examining perceptions of harm developed by Royzman, Kim, and Leeman (2015). The responses to the harm-based questions provided one measure of participants’ perceptions of harm, that is, did participants endorse a harm-based reason for their judgment when it was presented to them? Another measure of perceptions of harm was taken by coding the reasons provided for whether or not participants mentioned harm as justifying their judgment. Figure 1 presents a matrix plotting rows of participants’ judgments (wrong vs. not wrong) against columns of their endorsing of harm (left matrix), or whether or not they mentioned harm (right matrix) across three studies (N = 723).5 According to TDM, all participants should be located in either the top left (harm/wrong) or the bottom right (no harm/not wrong) quadrants, the responding of participants in either of the other two quadrants cannot be explained by TDM.\n\n\n\n\n\n\n\n\n\n\nEven in taking the most generous measure of perceptions of harm (left), the responding of 17% of participants (9% plus 8%) cannot be explained by TDM. Taking the stricter (and arguably more accurate, see McHugh et al. 2020) measure of perceptions of harm further reduces the explanatory power of TDM – only 45% of participants responded in line with the predictions of TDM. In addition to evidence for harmless wrongs, the same set of studies had questions explicitly related to the wrongness of behaviors linked with harm and potential harm. While participants were not explicitly asked about their perceptions of harm for boxing or contact team sports, they were presented with a question “How would you rate the behavior of two people who engage in an activity that could potentially result in harmful consequences for either of them?”. Only 50% of participants across two studies (N = 613) rated this as wrong, providing clear evidence for the idea of “wrongless harms” that is rejected by TDM (Schein and Gray 2018).\nSo far, there is nothing uniquely “moral” in moral judgment. The people researchers have studied do not appear to apply any given mode of processing or content in a sufficiently consistent manner to provide stable account of moral judgment. We argue, therefore, that a more successful approach is to explore what the capacity to identify morally right and morally wrong actors, actions and outcomes has in common with people’s capacity to identify categories more generally."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#moral-phenomena-with-domain-general-categorization-explanations",
    "href": "publications/moral-judgment-as-categorization/index.html#moral-phenomena-with-domain-general-categorization-explanations",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Moral Phenomena with Domain General (Categorization) Explanations",
    "text": "Moral Phenomena with Domain General (Categorization) Explanations\nMJAC assumes that moral categorization is a dynamical, context-dependent process, and as such, we predict the same phenomena as have been found within the categorization literature at large. In this section, we briefly outline some evidence for this predicted similarity, though we note that at present, these patterns are more suggestive than conclusive. However, we argue that these patterns should be seen not as noise obscuring an underlying stable moral category but a signal of the complexity of the processes that give rise to that category. We believe that the phenomenon of moral judgment is no more undermined or challenged by this complexity than the cognitive psychology of concepts, and category formation is more generally. These include such phenomena as order effects, language effects, the impact of emotions, and typicality of instance.\n\nOrder effects\nIn morality research, responses to different moral dilemmas have been found to vary depending on the order of presentation (Petrinovich and O’Neill 1996; Wiegmann, Okan, and Nagel 2012). MJAC can explain these in the same way as order effects in non-moral categorization are explained. That is, they occur as a result of priming. The scenario that is presented first causes some features of the second scenario to become more salient. The salience of these features leads to a different judgment than if the initial scenario was not presented. In the case of categorization, the effect of this type of priming is primarily studied concerning reaction times. For example, a study by Barsalou (1982, 2003) showed that reading sentences that made particular features of a given object salient influenced the speed at which participants verified related properties of the given object (see also Tabossi 1988). We predict similar reaction time variability should be observed when participants are primed with relevant properties for making moral categorizations.\nThere is also evidence that priming people with particular concepts can influence their subsequent categorizations. In a study by Higgins, Bargh, and Lombardi (1985), participants completed a task in which they were required to create sentences from a selection of words. Some of the words presented were selected to prime a particular concept, e.g., “bold”, “courageous”, and “brave” primed “Adventurous”; “careless”, “foolhardy”, and “rash” primed “Reckless” (Higgins, Bargh, and Lombardi 1985, 63). Participants were later presented with a description of ambiguous behavior. It was found that the categorizations of these behaviors were influenced by the concept that was primed. A similar study demonstrated the same effect (Srull and Wyer 1979). We predict that this same effect should occur for moral categorizations, for example, participants responses to descriptions of behavior that could be viewed as either “moral” or “self-righteous”, or a behavior that could be viewed as either “immoral” or “crafty” should be subject to the same effect as described by Higgins, Bargh, and Lombardi (1985).\n\n\nLanguage effects\nThough the influence of language on the categories available to a given person has a long and controversial history in psychology, recent research has made it increasingly clear that a given language forms a significant constraint on categorization tasks due to the resources of vocabulary and grammatical structure that it provides (Cubelli et al. 2011; Davidoff 2001). Second language acquisition also impacts how categorizations are formed, as a person learns to deploy new linguistic resources in the service of their goal-directed activities (Athanasopoulos 2007)\nPeople’s moral judgments have been shown to vary depending on whether they read a moral scenario in their first language or in a second language (the ‘foreign language effect,’ e.g., Cipolletti, McFarlane, and Weissglass 2016; Costa et al. 2014; Driver 2020; Geipel, Hadjichristidis, and Surian 2015; Hayakawa et al. 2017). Specifically, people appear to be more willing to endorse action in the Footbridge/Push version of the trolley dilemma when this dilemma is presented a language other than their native language. According to MJAC, deontological judgments become intuitive as a result of consistency across contexts. The changing of the language presents a novel context, which means the inferences associated with the regular context (e.g., emotional inferences) of encountering or this scenario are not as salient. Evidence for this interpretation comes from research investigating people’s reactions to non-moral taboo words in their first language vs. a second language. Harris, Ayçiçeĝi, and Gleason (2003) measured skin conductance of English speakers and Turkish speakers when rating different types of words in their first language and in their second language. It was found that (non-moral) taboo words led to greater arousal when presented in participants’ first language than when presented in a second language (see also, Colbeck and Bowers 2012), suggesting that the emotional inferences associated with the footbridge dilemma are less salient when it is presented in a foreign language.\n\n\nEmotion effects\nEmotion is perhaps the most widely discussed contextual influence on moral judgments (e.g., Cameron, Payne, and Doris 2013; Giner-Sorolla 2018; Huebner, Dwyer, and Hauser 2009; Landy and Goodwin 2015; May 2014; Prinz 2005; Royzman et al. 2014; Rozin et al. 1999; Russell and Giner-Sorolla 2011; Valdesolo and DeSteno 2006). Above, we have outlined how specific emotions may become associated with particular types of judgment; that is, the emergence of relative stability in making specific categorizations is linked with consistency in relevant contextual features, where the relevant contextual features include emotions. In other words, the emotions that may be experienced when a moral categorization is learned (or reinforced/consolidated) are likely to also be present during later categorizations. A corollary of this is that the experience of the specific emotion may provide a contextual cue, reminding people of previous experiences, making a particular categorization more salient (e.g., Barsalou 2003; Barsalou and Wiemer-Hastings 2005; Damasio 1994; Damasio and Damasio 1994; Rosenfield 1988).\nIn line with the prediction that manipulations designed to suppress the salience of these contextual factors (S. M. Smith and Vela 2001), we predict the same type of manipulations should have similar effects on the influences of emotions on moral categorizations. The foreign language (Colbeck and Bowers 2012; Costa et al. 2014; Driver 2020; Geipel, Hadjichristidis, and Surian 2015; Harris, Ayçiçeĝi, and Gleason 2003; Hayakawa et al. 2017) effect described above provides some evidence for this, whereby the salience of the emotional content is reduced by being presented in the second language. Similar effects should be observed using mindset manipulations (Igou 2011; Igou and Bless 2007).\nThe specific contextual influences discussed above provide just a sample of the broader contextual factors known to influence the making of moral judgment. MJAC assumes that moral judgments are dynamical and context-dependent, and as such, it is the approach that is best positioned to understand the diverse contextual influences on moral judgment. It is beyond the scope of the current paper to describe and account for all the known contextual influences on moral judgment (e.g., an incomplete list would include: Bostyn, Sevenhant, and Roets 2018; Christensen et al. 2014; Christensen and Gomila 2012; Costa et al. 2014; Cushman et al. 2012; Everett, Pizarro, and Crockett 2016; Everett et al. 2018; Forbes 2018; Francis et al. 2016; Francis et al. 2017; Lee and Holyoak 2020; Petrinovich and O’Neill 1996; Rozin et al. 1999; Schein 2020; Timmons and Byrne 2019; Uhlmann, Pizarro, and Diermeier 2015; Valdesolo and DeSteno 2006; Vasquez et al. 2001; Vasudev and Hummel 1987). However, MJAC predicts understanding these diverse context effects depends on (a) accounting the learning history (e.g., in the cases of emotional influences and the foreign language effect) and, (b) viewing moral categorization as occurring as part of goal-directed activity (e.g., categorization of actor versus action discussed above). Incorporating both of these considerations into a program of research inevitably leads to attempts to make the study of moral judgment reflective of real-world moral decision making (Bauman et al. 2014; Bostyn, Sevenhant, and Roets 2018; Gilligan 1977, 1993; Hester and Gray 2020; Hofmann et al. 2014; Schein 2020; Watkins 2020).\n\n\nTypicality\nFinally, one of the most salient phenomena within the field of categorization concerns the fact that there are “better” and “worse” examples of any given category (McCloskey and Glucksberg 1978; Oden 1977), for example, a chair is viewed as a more typical member of the category FURNITURE than bookends (McCloskey and Glucksberg 1978). Such judgments are made even for those categories with supposedly logical or sharp boundaries such as geometric figures (Bourne 1982; Feldman 2000).\nMJAC predicts that this same phenomena of typicality should be observed for moral categorizations, e.g., cold-blooded murder versus violence in pursuit of a cause. We further predict that relative typicality should be related to the relative consistency with which category members are identified as members of the given category (and should be independent of perceived severity). This facet of moral judgment has already seen some discussion in the existing moral judgment theoretical literature. Cushman (2013, 282) makes a passing reference – that pushing someone “with your hands” is more typically harmful than pushing someone “with your buttocks”. However, typicality sees more substantial discussion in the context of TDM (Gray and Keeney 2015; Schein and Gray 2018).\nTypicality ratings in moral judgments, as described by TDM, are related to the degree to which a given scenario matches the defined prototype of morality, as an “intentional agent causing damage to a vulnerable patient” (Schein and Gray 2018, 32). An act that more clearly involves harm is rated as more typically wrong than an action where the perceived harm is less. Similarly, if there are evident intentional agent and vulnerable patient, an action is rated as more typically wrong than if the actors are more similar in their intentionality and vulnerability (Gray and Keeney 2015; Schein and Gray 2018).\nThis account of typicality is based on assumptions related to content (agent-patient, harm) and does not inform our understanding of the cognitive processes underlying moral judgments. As such, it cannot clearly distinguish between typicality and severity. Indeed the strong overlap between severity of an act and it’s typicality as an example of moral wrongness is acknowledged: “By definition, more severe acts are more immoral; that is, they are better examples of the category”immorality” (Gray and Keeney 2015, 860).\nWith MJAC, we propose that typicality is related to both frequency and consistency of exposure; that is, behaviors that are frequently encountered and consistently identified as members of a given moral category should emerge as typical category members. Given the consistency with which harm related transgressions are identified as wrong, the emergence of the prototypical template described by Gray and colleagues is not surprising (Gray and Keeney 2015; Schein and Gray 2018). However, we attribute these typicality ratings to the learning history rather than to perceptions of harm and of agents and patients.\nGiven the possible confounding influence of severity on typicality ratings, unpacking this difference in interpretation will prove challenging; however, we believe it will be a worthwhile endeavor. We hypothesize typicality ratings are related to the learning history and not linked to specific content. This predicts differences in typicality ratings when controlling for severity (either by focusing on harmless dilemmas or by keeping the severity of harm constant). This also predicts differences in typicality ratings within populations, through individual differences in moral values (e.g., Graham et al. 2012; Haidt and Joseph 2008), and between populations through cultural variation (e.g., Haidt, Koller, and Dias 1993). Furthermore, this view of typicality of moral categorizations predicts that perceptions of typicality will be context sensitive, that is intra-personal variability should be observed depending on current context, and crucially depending on current goal-directed activity. A professor grading papers would rate straight plagiarism as more typically wrong than plagiarism by omitting references. Whereas when not grading papers, the same professor may be more concerned with the ethics of her colleagues’ precarious contracts and entirely indifferent to the shortcuts students may take in their assignments. Or a sports fan may claim to view cheating as wrong, where different cheating behaviors vary in their typicality (e.g., overt fouling, cynical fouling, feigning injury so that the referee penalizes the other team) however, the same fan may turn a blind eye to these behaviors when committed by members of the team she supports.\nIt is worth noting that this sensitivity to the context of moral judgment implies that the importance of understanding moral judgments in more real-life contexts rather than through the study of abstract decontextualized dilemmas has been well documented (e.g., Bauman et al. 2014; Bostyn, Sevenhant, and Roets 2018; Gilligan 1977, 1993; Hester and Gray 2020; Hofmann et al. 2014; Schein 2020; Watkins 2020). By focusing specifically on context-sensitive categorizations occurring as part of goal-directed activity, MJAC offers a framework for attempting to make the study of moral judgments more reflective of the making of moral judgments in everyday life. Furthermore, in recognizing the broader array of contextual influences on moral categorizations, rather than focusing on specific contextual influences on specific types of judgments, MJAC is uniquely positioned to incorporate known context effects into a coherent parsimonious framework. This would provide opportunities for the combined influences of these contextual factors to be studied relative to each other, with the potential to identify clear boundary conditions to understand how and when specific contextual factors influence moral categorizations more than others."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#summarizing-the-differences-between-mjac-and-existing-approaches",
    "href": "publications/moral-judgment-as-categorization/index.html#summarizing-the-differences-between-mjac-and-existing-approaches",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Summarizing the Differences Between MJAC and Existing Approaches",
    "text": "Summarizing the Differences Between MJAC and Existing Approaches\nAbove, we have outlined how MJAC differs from existing theories in terms of assumptions and explanation. These theories make assumptions based on content, and this results in essentialist theorizing, either implicit or explicit attempts to define an “essence” of morality. In contrast, MJAC rejects essentialism, instead assuming moral categorizations are dynamical, context-dependent, and occurring as part of goal-directed activity. Each of the theories discussed is explicitly or implicitly (e.g., Schein and Gray 2018, 41) based on dual-process assumptions, with related dichotomous assumptions regarding the cognitive mechanisms (where these mechanisms are specified). MJAC does not assume distinct, separable processes, adopting type-token interpretation, occurring as part of goal-directed activity (Barsalou 2003, 2017), as the mechanism that underlies moral categorization. These differences in assumptions underlie the differences in the explanation discussed above. These differences are summarized in Table 1.\nTable 1: Specific points of divergence between MJAC and existing theories\n\n\n\n\n\n\n\n\n\n\n\n\nGreene’s Dual-process theory\n“Soft” dual-process theory\nModel-based accounts\nTDM\nMJAC\n\n\n\n\nAssumptions:\n\n\n\n\n\n\n\nContent\nDeontology-utilitarianism / personal-impersonal\nDeontology-utilitarianism\nAction-outcome\nHarm-based, dyadic\nDynamical Context-dependent Goal-directed\n\n\nMoral “Essence”\n(Implicit)\n(Not discussed)\n(Implicit)\nExplicit\nRejected\n\n\nProcesses\nDual-processes\nDual-processes\nDual-processes\n(implicitly dual-process)\nContinuum\n\n\nMechanisms\nIntuition (emotion) / cognition\nEmotion / cognition\nModel-based / model-free\nCategorization (unspecified)\nType-token interpretation\n\n\nPhenomena Explained:\n\n\n\n\n\n\n\nDumbfounding (harmless wrongs)\n(Not discussed)\n(Not discussed)\nExplained\nDenied\nExplained: learning history\n\n\nWrongless harms\n(Not discussed)\n(Not discussed)\n(Not discussed)\nDenied\nExplained: learning history\n\n\nTypicality\n(Not discussed)\n(Not discussed)\n(Not discussed)\nMatching of “prototype”\nContext-dependent\n\n\nContextual influences\nSpecific: Personal-impersonal\nSpecific: Emotion / cognition\n\nSpecific: Action-outcome\nSpecific: Harm-based"
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#effects-not-directly-predicted-by-mjac",
    "href": "publications/moral-judgment-as-categorization/index.html#effects-not-directly-predicted-by-mjac",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Effects not Directly Predicted by MJAC",
    "text": "Effects not Directly Predicted by MJAC\nDespite predicting a broad range of contextual variability, there remain some influences on moral judgment that are not directly predicted by MJAC. Three such phenomena are the doctrine of double effect, moral luck, and moral conviction. While not directly predicted, these phenomena further illustrate the variability and complexity that theories of moral judgment must account for.\nFirstly, the doctrine of double effect is the name given to the finding that people view causing harm as a means to achieving a goal as worse than causing harm as a side-effect of achieving a goal (Doris 2010; Mikhail 2000). Above, we have presented the doctrine of double effect as a limitation of model-based approaches (Crockett 2013; Cushman 2013); the action-outcome distinction does not adequately explain why people should make a distinction between harm as a means and harm as a side effect (henceforth, means-side effect). Similarly, this means-side effect distinction is not directly predicted by MJAC. Interestingly, it has been found that people apply this distinction even though they cannot reliably articulate it (Cushman, Young, and Hauser 2006; Hauser et al. 2007). This suggests a similarity with moral dumbfounding, and the possibility of a common explanation. In the case of moral dumbfounding, MJAC posits that people implicitly learn (through continued and consistent type-token interpretation) that something is wrong and that learning the categorization occurs independently of learning the reasons for the categorization. Distinguishing side effects from means is much more subtle than distinguishing different types of actions, however, there is no reason for such a distinction not to emerge through the same process of type-token interpretation if others are making the same distinction in their moral judgments (Cushman, Young, and Hauser 2006; Hauser et al. 2007; Mikhail 2000). In this way, while it is not an obvious a priori prediction of MJAC, the doctrine of double effect is not inconsistent with its assumptions.\nThe second known effect that is not directly predicted by MJAC is the phenomenon of moral luck. Moral luck demonstrates that different outcomes can lead to different evaluations of the same behavior (Nagel 1979, 2013; Williams 1982; Wolf 2001; Young, Nichols, and Saxe 2010). Consider the following two scenarios (adapted from Wolf 2001; see also Royzman and Kumar 2004; Williams 1982):\nJo\n\nA truck driver (Jo), needs to make an emergency stop. Jo has neglected to check the brakes of the truck recently. When attempting to stop the truck, Jo loses control and the truck crashes into the ditch.\n\nPat\n\nA truck driver (Pat), needs to make an emergency stop. Pat has neglected to check the brakes of the truck recently. When attempting to stop the truck, Pat loses control and the truck runs over a child.\n\nThe actions of Jo and Pat are the same, however, previous research has shown that in situations like this, people are likely to view Pat as more morally blameworthy than Jo (Walster 1966; Wells and Gavanski 1989; Young, Nichols, and Saxe 2010). People are more harsh in their moral judgments of the same actions when the actions result in negative outcomes. Williams (1982; see Wolf 2001) is attributed with coining the phrase “moral luck” to describe this asymmetry of judgments of actions based on outcomes.\nAs with the trolley problem, and the emergence of typicality, MJAC explains the phenomenon of moral luck with reference to the consistency of previous categorizations. Causing harm to another person is relatively consistently categorized as MORALLY WRONG (Cushman et al. 2012; Schein and Gray 2018; though not with perfect consistency, e.g., Alicke 2012; McHugh et al. 2020). This relative consistency means that encountering an event in which the actions of an agent cause harm is highly likely to be categorized as MORALLY WRONG. The actions described in classic moral luck scenarios are typically ambiguous or minimally problematic. That is, they are not categorized as wrong with the same consistency. This mismatch in the consistency with which the actions vs the outcomes are categorized as wrong that leads to what we observe as moral luck. In effect, the harmful outcome may be viewed as a contextual influence that leads to harsher judgments of actions.\nA third phenomenon that is not directly addressed by MJAC is moral conviction (e.g., Skitka 2010), or zeal in moral positions (e.g., McGregor 2006). While MJAC does not make specific claims about moral conviction, previous research has linked this to identity and identification with particular groups (e.g., Greene 2013; see also Proulx and Inzlicht 2012; Heine, Proulx, and Vohs 2006), and more recently attitude strength has been linked with connectivity (e.g., Dalege et al. 2019) We suggest that the meaning maintenance model provides an ideal framework for understanding zeal in moral categorization. According to the meaning maintenance model (Heine, Proulx, and Vohs 2006), there are four primary domains of meaning: certainty, self-esteem, social relations, and mortality. Where non-moral category knowledge constitutes meaning in the domain of certainty (Heine, Proulx, and Vohs 2006), moral knowledge additionally holds meaning in the social domain (Greene 2013; see also Proulx and Inzlicht 2012; Heine, Proulx, and Vohs 2006). We hypothesize that it is this spanning of both the certainty and the social domains of meaning that leads to moral zeal.\nWhen we apply this insight to the broader framework of MJAC, it appears that some contexts (namely social/group contexts) matter more in the development of robust moral categories. We hypothesize that robustness in moral categorization is related to the consistency of categorization across multiple (social) contexts. Consider the categorization of sexist jokes as MORALLY WRONG. Some groups would endorse this categorization, and there are groups who would disagree. The degree to which a person will be motivated to defend this categorization will be related to the social groups they are members of, and the consistency across these groups. Someone who agrees with this categorization but spends a lot of time tolerating “locker room talk” will be less zealous than someone who socializes with people who openly identify as feminists."
  },
  {
    "objectID": "publications/moral-judgment-as-categorization/index.html#footnotes",
    "href": "publications/moral-judgment-as-categorization/index.html#footnotes",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe use the term “skill” to refer to the type of automaticity that is developed through practice/rehearsal. This does not imply any objectivist moral truth that can be accessed by moral “experts”, we refer only to the cognitive processes involved.↩︎\nWhere Token denotes a token that may be viewed as ‘prototypical’ for a given type.↩︎\nRelatedly, for favourable judgements, we predict the opposite effect. That is, if for morally praiseworthy actions are being performed by a close other, the target of the categorisation is more likely to be the actor than the action, helping to maintain a positive view of the close other (Forbes 2018; Murray, Holmes, and Griffin 1996a, 1996b).↩︎\nRecall the discussion regarding the ad-hoc category THINGS TO PACK INTO A SUITCASE (Barsalou 1991, 2003).↩︎\nThese figures are not reported in McHugh et al. (2020), however see McHugh et al. (2018) for full data sets.↩︎"
  },
  {
    "objectID": "shiny.html",
    "href": "shiny.html",
    "title": "Shiny",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "music/index.html",
    "href": "music/index.html",
    "title": "Moral Judgment as Categorization (MJAC)",
    "section": "",
    "text": "I have been playing music from a young age and started writing songs as a teenager. Influences range from heavy, multi-instrumental Prog (Yes, Rush, Pink Floyd and Genesis) to more traditional acoustic folk and blues (Neil Young, Eric Bibb and Tom Waits) and everything in between.\nI have released 2 albums to date: Back to the Drawing Board was released in 2012, and It Will Pass… was released in 2019. Both albums are available online (check out Cillian Mc Hugh on Spotify (also on youtube music, and itunes). Have a listen to the new album below. I also have the unreleased Spectrum which included some songs that made it onto Back to the Drawing Board along with some longer songs that reflect my Prog influences. Some material from Spectrum is available on Soundcloud (in particular see this playlist).\n\n\n\n&lt;iframe src=\"https://www.youtube.com/embed/videoseries?list=OLAK5uy_mYaYyP7L_Zq72SMK54b7lnV3nFXMDiHmc\" frameborder=\"0\" allowfullscreen\n  style=\"position:absolute;top:0;left:0;width:100%;height:100%;\"&gt;&lt;/iframe&gt;\n\n\n\n(follow the links below for more information)"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html",
    "href": "music/back-to-the-drawing-board/index.html",
    "title": "Back to the Drawing Board",
    "section": "",
    "text": "Track information for the album Back to the Drawing Board. Listen to the full album below (or on youtube music, Spotify, or itunes)"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#rain",
    "href": "music/back-to-the-drawing-board/index.html#rain",
    "title": "Back to the Drawing Board",
    "section": "Rain",
    "text": "Rain\nVocals: Ciara McHugh Backing Vocals: Cillian McHugh Guitar: Cillian McHugh\n\n\n  \n    \n  \n\n\n\n\nThe sky is grey, it’s raining today  Pearls on the grass  It’s always the same, drops of rain  But the tears keep falling down\nTears in your eyes, misty skies  Vision is blurred Where is the pain, you can’t explain Tears in the rain\nWhy do we cry? Love or loss?\nDrops on the glass trace a path Of patterns and maps Trickling down, from your frown It tickles your cheek\nIt’s always the same, drops of rain On the window pane You wipe your eyes but continue to cry And the rain falls"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#bigness",
    "href": "music/back-to-the-drawing-board/index.html#bigness",
    "title": "Back to the Drawing Board",
    "section": "Bigness",
    "text": "Bigness\nVocals: Cillian McHugh Backing Vocals: Ciara McHugh Guitars: Cillian McHugh Organ: Ciara McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nLook up to see, to see how high can we ever reach the sky what’ll i be before i die and should i wonder why\nstart with one, another makes two are the numbers really true it can’t be done, if we only knew to count them all: impossible to do\ntime goes on and on, every day we’re here adds up to pass the year when it’s done and gone you can’t go back to change the past it doesn’t work like that\nthe universe expands that’s what they say getting bigger every day the big bang blew it all away far and wide past the milky way\n  Verse:\n        C       G(1)    C       G(1)    Am      G(2)    Am      G(2)\n  E||---0---|---0---|---0---|---0---|---0---|---0---|---0---|---0---|\n  B||---3---|---1---|---3---|---1---|---1---|---3--1|---1---|---3---|\n  G||---0---|---0---|---0---|---0---|---2---|---0---|---2---|---0---|\n  D||---2---|---0---|---2---|---0---|---2---|---0---|---2---|---0---|\n  A||---3---|---2---|---3---|---2---|---0---|---2---|---0---|---2---|\n  E||-------|-------|-------|-------|-------|-------|-------|-------|\n\n  Chorus 1\n        F    C G1 Am    F     Am G(2)   F     C  Em     Dm    Am G(2)\n  E||---1---|0-0--0-|---1---|-0--0--|---1---|-0--0--|---1---|-0--0--|\n  B||---1---|1-1--1-|---1---|-1--3--|---1---|-1--0--|---3---|-1--3--|\n  G||---2---|0-0--2-|---2---|-2--0--|---2---|-0--0--|---2---|-2--0--|\n  D||---3---|2-0--2-|---3---|-2--0--|---3---|-2--2--|---0---|-2--0--|\n  A||---3---|3-2--0-|---3---|-0--2--|---3---|-3--2--|-------|-0--2--|\n  E||---1---|-------|---1---|-------|---1---|----0--|-------|-------|\n\n  Bridge:\n        Dm    C  G(1)   Dm    Am G(2)   Dm    C  G(1)   Dm    Am G(2)\n  E||---1---|-0--0--|---1---|-0--0--|---1---|-0--0--|---1---|-0--0--|\n  B||---3---|-1--1--|---3---|-1--3--|---3---|-1--1--|---3---|-1--3--|\n  G||---2---|-0--0--|---2---|-2--0--|---2---|-0--0--|---2---|-2--0--|\n  D||---0---|-2--0--|---0---|-2--0--|---0---|-2--0--|---0---|-2--0--|\n  A||-------|-3--2--|-------|-0--2--|-------|-3--2--|-------|-0--2--|\n  E||-------|-------|-------|-------|-------|-------|-------|-------|\n\n  Chorus 2\n        F     C  Em     F     Am G(2)   F    C G1 Am    Dm    Am G(2)\n  E||---1---|-0--0--|---1---|-0--0--|---1---|0-0--0-|---1---|-0--0--|\n  B||---1---|-1--0--|---1---|-1--3--|---1---|1-1--1-|---3---|-1--3--|\n  G||---2---|-0--0--|---2---|-2--0--|---2---|0-0--2-|---2---|-2--0--|\n  D||---3---|-2--2--|---3---|-2--0--|---3---|2-0--2-|---0---|-2--0--|\n  A||---3---|-3--2--|---3---|-0--2--|---3---|3-2--0-|-------|-0--2--|\n  E||---1---|----0--|---1---|-------|---1---|-------|-------|-------|"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#chasing-rainbows",
    "href": "music/back-to-the-drawing-board/index.html#chasing-rainbows",
    "title": "Back to the Drawing Board",
    "section": "Chasing Rainbows",
    "text": "Chasing Rainbows\nVocals: Cillian McHugh Guitar: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nto love is to catch a rainbow the sun and the rain, the joy and the pain\nchase a rainbow ’till you catch the wind and it carries you off over the sky washed up left on the moon broken hearts chase rainbows\nswirl for a while with a surface smile but bubbles burst or float away on a breeze\nchase a rainbow ’till you catch the wind and it carries you off over the sky washed up left on the moon broken hearts chase rainbows"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#passing-days",
    "href": "music/back-to-the-drawing-board/index.html#passing-days",
    "title": "Back to the Drawing Board",
    "section": "Passing Days",
    "text": "Passing Days\nVocals: Ciara McHugh Backing Vocals: Cillian McHugh Guitar: Cillian McHugh\n\n  \n    \n  \n\n\n\n\n\nThe days pass. But your heart still cries (But inside, inside it still rains)"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#back-to-the-drawing-board-1",
    "href": "music/back-to-the-drawing-board/index.html#back-to-the-drawing-board-1",
    "title": "Back to the Drawing Board",
    "section": "Back to the Drawing Board",
    "text": "Back to the Drawing Board\nVocals: Ciara McHugh Backing Vocals: Cillian McHugh Guitars: Cillian McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\n\n  Verse:                                                Bridge:\n       G     C     G     C     G     C     G     C      Em D~ G  Am    F\n  E||--3--|--3--|--3--|--3--|--3--|--3--|--3--|--3--  |-0-----3--0-|--(0)--|\n  B||--3--|--3--|--3--|--3--|--3--|--3--|--3--|--3--  |-0--3--3--1-|---1---|\n  G||--0--|--0--|--0--|--0--|--0--|--0--|--0--|--0--  |-0--2--0--2-|---2---|\n  D||--0--|--2--|--0--|--2--|--0--|--2--|--0--|--2--  |-2--0--0--2-|---3---|\n  A||--2--|--3--|--2--|--3--|--2--|--3--|--2--|--3--  |-2-(0)-2--0-|---3---|\n  E||--3--|-----|--3--|-----|--3--|-----|--3--|-----  |-0--2--3----|-------|\n\n  Chorus:\n     Cmaj79  D~  Cmaj79  D~  Cmaj79  Bm~   Em D~  \n  E||--0--|-----|--0--|-----|--0--|--0--|--0-----|\n  B||--3--|--3--|--3--|--3--|--3--|--3--|--0--3--|\n  G||--4--|--2--|--4--|--2--|--4--|--4--|--0--2--|\n  D||--2--|--0--|--2--|--0--|--2--|--0--|--2--0--|\n  A||--3--|-(0)-|--3--|-(0)-|--3--|--2--|--2-(0)-|\n  E||-----|--2--|-----|--2--|-----|-----|--0--2--|\n\nVerse 1 paint a picture, write a song… to illustrate this life the page is yours, your work your life… you do your best to get it right\nbut the colours start to run … words become undone\nChorus 1 cross it out tear it up erase it till there leaves no trace and start again, start again\nVerse 2 ealainteoir ar leathnach bán … dathanna an saol amhran beo le mothúchán … focail don saol\nach theip an ghrá bhris an peann ní leor a bhí ann\nChorus 2 scrois é chuir sa tine é bris é go dtí píosaí beag is tosaigh arís, tosaigh arís\nVerse 3 start again on a brand new page … we all learn with age peann nua, dathanna nua … ach an amhrán chéanna\nnuair a theip an ghrá bhris an peann ní leor a bhí ann\nChorus 3 scrois é chuir sa tine é bris é go dtí píosaí beag is tosaigh arís, tosaigh arís"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#the-price-of-love",
    "href": "music/back-to-the-drawing-board/index.html#the-price-of-love",
    "title": "Back to the Drawing Board",
    "section": "The Price Of Love",
    "text": "The Price Of Love\nVocals: Cillian McHugh Acoustic Guitar: Cillian McHugh Electric Guitar: Maurice McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh Harmonica: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nWaiting… Holding out Hoping time won’t run out Torn between a rock and a hard place Torn between love and happiness\nNeither is enough, neither will suffice Carry on the same - that’s the best advice Hide from the shame, hide from your heart Cause no pain, break no-one’s heart"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#broken-words",
    "href": "music/back-to-the-drawing-board/index.html#broken-words",
    "title": "Back to the Drawing Board",
    "section": "Broken Words",
    "text": "Broken Words\nVocals: Ciara McHugh, Cillian McHugh Guitars: Cillian McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nYou mean it. You say it’s true. I believe it. I feel it too. You said to me, I said to you.\nWords pick me up and carry me. The thrill, the rush, the waves under me. (Surfing the waves)\nBut waves break words are lost. Crashing truths of changing tides, Washed ashore and left behind.\nWords are washed out ripples on the shore. (Washed out waves) Broken words can’t carry me anymore. (Broken, broken words)\nYou said it then. Means nothing now. Meaning lost. (From your) Broken words."
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#the-happy-song",
    "href": "music/back-to-the-drawing-board/index.html#the-happy-song",
    "title": "Back to the Drawing Board",
    "section": "The Happy Song",
    "text": "The Happy Song\nVocals: Cillian McHugh Guitars: Cillian McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh Harmonica: Cillian McHugh\n\n  \n    \n  \n\n\n\n\n\n  Verse:\n       G   Bm7 E7    G   Bm7 E7    G   Bm7 E7  A7    C   D7  G   B7   \n  E||-(3)-----(0)-|-(3)-----(0)-|-(3)-----(0)-(0)-|--0---2---3---2--|\n  B||--3---3---3--|--3---3---3--|--3---3---3---2--|--1---1---3---0--|\n  G||--0---2---1--|--0---2---1--|--0---2---1---0--|--0---2---0---2--|\n  D||--0---0---0--|--0---0---0--|--0---0---0---2--|--2---0---0---1--|\n  A||-(2)--2---2--|-(2)--2---2--|-(2)--2---2---0--|--3-------2---2--|\n  E||--3-------0--|--3-------0--|--3-------0------|----------3------|\n\n  Bridge:\n       G   Bm7 E7    A7 Am7 D D9D D7   G   Bm  Em  C9    C9 E~ E B  D  D7 D6 D   \n  E||-(3)-----(0)-|- 0--0---2-0-----|-(3)------3---3--|-------(0)-|-------------|\n  B||--3---3---3--|--2--1---3-3-3-1-|--3---3---3---2--|--3--1--0--|-3--1--0-----|\n  G||--0---2---1--|--0--0---2-2-2-2-|--0---2---0---0--|--0--0--1--|--2--2----2--|\n  D||--0---0---0--|--2--2---0-0-0-0-|--0---0---2---2--|--2--0--2--|-0--0---0--0-|\n  A||-(2)--2---2--|--0--0-----------|-(2)--2---2---3--|--3--2--2--|-------------|\n  E||--3-------0--|-----------------|--3-------0------|--------0--|-------------|\n\nIf i’ve got somethin’ to say i say it If i’ve got something to do i do it And i feel so good\nDoesn’t even matter if the rains fall And i Don’t care if there’s no sun at all Coz i feel good\nSome things that happen you cannot change What reason why? But if i’m ever feelin down I know\nNothing can be thrown at me i cannot catch all obstacles can be overcome and even if i feel alone i know\ni’ve got my friends there’ll always be someone to call on someone to cheer me up\nwhen i’m happy i’m with friends and when i’m with friends i’m happyv and i believe"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#too-close",
    "href": "music/back-to-the-drawing-board/index.html#too-close",
    "title": "Back to the Drawing Board",
    "section": "Too Close",
    "text": "Too Close\nVocals: Cillian McHugh Guitars: Cillian McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh Harmonica: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nListening to the silence of chords that were never played Humming through the lines of mistakes we made Listening while we wait is it too late\nThe final words he spoke still echoed in your mind Oh as a song comes to a close for a second time True for you now, wondering how"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#sun",
    "href": "music/back-to-the-drawing-board/index.html#sun",
    "title": "Back to the Drawing Board",
    "section": "Sun",
    "text": "Sun\nVocals: Ciara McHugh Backing Vocals: Cillian McHugh Guitar: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nKids are playin’ on the grass, The sun is out, the rain has passed. But the sun can’t warm the void inside When you’re all alone.\nOutside the window, bright and blue, The empty sky is reflected through. Through your eyes and into you, Just the same.\nThe sun shines. Burning inside.\nA gentle breeze shakes the leaves As it blows through the trees. From deep inside you breathe a sigh, Wondering why.\nThe sun is bright the sky is clear. You’re all alone, there’s no one here. Hurting inside you fight the tears, And the sun shines."
  },
  {
    "objectID": "music/videos/index.html",
    "href": "music/videos/index.html",
    "title": "Videos",
    "section": "",
    "text": "In addition to the studio albums I also have some home video recordings available here.\nThese include recordings of my own songs as well as some covers."
  },
  {
    "objectID": "music/videos/index.html#there-are-no-words",
    "href": "music/videos/index.html#there-are-no-words",
    "title": "Videos",
    "section": "There are no Words",
    "text": "There are no Words"
  },
  {
    "objectID": "music/videos/index.html#it-will-pass",
    "href": "music/videos/index.html#it-will-pass",
    "title": "Videos",
    "section": "It will pass…",
    "text": "It will pass…"
  },
  {
    "objectID": "music/videos/index.html#the-race",
    "href": "music/videos/index.html#the-race",
    "title": "Videos",
    "section": "The Race",
    "text": "The Race"
  },
  {
    "objectID": "music/videos/index.html#bon-iver-cover-29-strafford-apts",
    "href": "music/videos/index.html#bon-iver-cover-29-strafford-apts",
    "title": "Videos",
    "section": "Bon Iver Cover: 29 #Strafford APTS",
    "text": "Bon Iver Cover: 29 #Strafford APTS"
  },
  {
    "objectID": "music.html",
    "href": "music.html",
    "title": "Music",
    "section": "",
    "text": "Music\nI have been playing music from a young age and started writing songs as a teenager. Influences range from heavy, multi-instrumental Prog (Yes, Rush, Pink Floyd and Genesis) to more traditional acoustic folk and blues (Neil Young, Eric Bibb and Tom Waits) and everything in between.\nI have released 2 albums to date: Back to the Drawing Board was released in 2012, and It Will Pass… was released in 2019. Both albums are available online (check out Cillian Mc Hugh on Spotify (also on youtube music, and itunes). Have a listen to the new album below. I also have the unreleased Spectrum which included some songs that made it onto Back to the Drawing Board along with some longer songs that reflect my Prog influences. Some material from Spectrum is available on Soundcloud (in particular see this playlist).\n\n\n\n\n\n  \n    \n  \n\n\n\n\n\n\n\n\n\nMore\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nVideos\n\n\n\n\n\n\n\n\n\n\n\n\nDec 12, 2021\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nLaunch Gig (photos and videos)\n\n\n\n\n\n\n\n\n\n\n\n\nApr 12, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nIt will pass…\n\n\n\n\n\nAlbum released in 2019\n\n\n\n\n\n\nMar 13, 2019\n\n\nCillian Mc Hugh\n\n\n\n\n\n\n  \n\n\n\n\nBack to the Drawing Board\n\n\n\n\n\nAlbum released in 2012\n\n\n\n\n\n\nSep 12, 2012\n\n\nCillian Mc Hugh\n\n\n\n\n\n\n  \n\n\n\n\nSpectrum\n\n\n\n\n\nAlbum released in 2009\n\n\n\n\n\n\nApr 20, 2009\n\n\nCillian Mc Hugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "music/it-will-pass/index.html",
    "href": "music/it-will-pass/index.html",
    "title": "It will pass…",
    "section": "",
    "text": "Track information for the album It Will Pass…. Listen to the full album below (or on youtube music, Spotify, or itunes)"
  },
  {
    "objectID": "music/it-will-pass/index.html#the-race",
    "href": "music/it-will-pass/index.html#the-race",
    "title": "It will pass…",
    "section": "The Race",
    "text": "The Race\nVocals: Cillian McHugh Guitars: Cillian McHugh\n\n  \n    \n  \n\n\n\nWhat if you, if you were in my shoes? Would they fit? And would they keep you warm? What if you, if you were in my shoes? What have I missed?\nTime… Time… Time… Goes on, moves fast\nStealing words of green tinted blue Shadows of what we thought we knew No more…\n… time … time … time is caught up, is made up"
  },
  {
    "objectID": "music/it-will-pass/index.html#another-goodbye",
    "href": "music/it-will-pass/index.html#another-goodbye",
    "title": "It will pass…",
    "section": "Another Goodbye",
    "text": "Another Goodbye\nVocals: Maggie McHugh Backing Vocals: Donal McHugh, Cillian McHugh Guitars: Cillian McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Simon O’Donnell\n\n  \n    \n  \n\n\n\n\n\n  Verse:\n          D/F#    G~  \n  E||------------------\n  B||-----(3)----(0)---\n  G||------2------2----\n  D||------0------0----\n  A||------------------\n  E||------2------3----\n\n  Chorus:\n           Bm     A9      G  \n  E||-----(0)----------------\n  B||------3------0------0---\n  G||------2------2------2---\n  D||------0------2------0---\n  A||------2------0----------\n  E||--------------------3---\n\nAll good things come to an end All good things come to an end\nYou don’t want them to You don’t want them to\nAnd all you’ve lived and all you own, and all you’ve learned an all you’ve known And all you’ve worked and all you’ve done And everything you’ve become\nAnd you don’t even know And you don’t even know And you don’t even know And you don’t even know\nAnd those you’ve loved and those you’ve known And those you’ve seen and those you’ve shown The ones who knew and understood The ones you thought would stay for good\nAnd you have to say goodbye And you have to say goodbye And you have to say goodbye And you have to say goodbye"
  },
  {
    "objectID": "music/it-will-pass/index.html#there-are-no-words",
    "href": "music/it-will-pass/index.html#there-are-no-words",
    "title": "It will pass…",
    "section": "There Are No Words",
    "text": "There Are No Words\nVocals: Cillian McHugh Guitar: Cillian McHugh\n\n\n  \n    \n  \n\n\n\nThere are no words, there are no words, there’s just hanging on  And holding out, and holding on, until it’s gone Your broken heart, your broken heart, you thought had healed\nYou check the screen, you check the screen for any sign For her reply, for her reply, but you know inside The empty screen and what that means, it’s all you feel\nYou came too close, you got too close, like you did before And every time, and every time, no matter what you try Will come again, will come again…"
  },
  {
    "objectID": "music/it-will-pass/index.html#unknown-arms",
    "href": "music/it-will-pass/index.html#unknown-arms",
    "title": "It will pass…",
    "section": "Unknown Arms",
    "text": "Unknown Arms\nVocals: Cillian McHugh Guitars: Cillian McHugh Organ: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh Harmonica: Cillian McHugh\n\n  \n    \n  \n\n\n\nChances that I blew And candles that I knew And all now, as I see Is all\nPeople said, and tears were shed We both knew it could only ever be as unknown arms wrapped around me unknown arms wrapped around me unknown arms wrapped around me\nChances that I blew and candles that I knew, And all Is all\nAll night tears were shed We both knew it could only ever be as unknown arms wrapped around me unknown arms wrapped around me unknown arms wrapped around me"
  },
  {
    "objectID": "music/it-will-pass/index.html#summer-rain",
    "href": "music/it-will-pass/index.html#summer-rain",
    "title": "It will pass…",
    "section": "Summer Rain",
    "text": "Summer Rain\nVocals: Cillian McHugh Guitars: Cillian McHugh\n\n  \n    \n  \n\n\n\n\n  Verse:\n           Em     D/F#   C/G           X2\n  E||-------------------(0)----\n  B||-------------------(0)----\n  G||------0------2------0-----\n  D||------2------0------2-----\n  A||------2------0------3-----\n  E||------0------2------3-----\n\n           Em     Bm     D/F#         C9\n  E||--------------------------|------0----------\n  B||-------------3------3-----|------3------3---\n  G||------0------2------2-----|------0------2---\n  D||------2------0------0-----|------2------0---\n  A||------2------2------------|------3----------\n  E||------0-------------2-----|-------------2---\n\n  Chorus:\n  1st time\n           Em     D/F#   C/G       Bm      C/G     D/A\n  E||-------------------(0)----|---0---|---0---|---0---\n  B||-------------------(0)----|---3---|---0---|---0---\n  G||------0------2------0-----|---2---|---0---|---0---\n  D||------2------0------2-----|---0---|---2---|---4---\n  A||------2------0------3-----|---2---|---3---|---5---\n  E||------0------2------3-----|-------|---3---|---5---\n\n  2nd time “And I knowww. . . “\n        Em      Bm      C/G     D/A\n  E||---0---|---0---|---0---|---0---\n  B||---0---|---0---|---0---|---0---\n  G||---7---|---7---|---0---|---0---\n  D||---5---|---4---|---2---|---4---\n  A||---7---|---5---|---3---|---5---\n  E||---0---|---7---|---3---|---5---\n\nSmells like summer rain Smells like summer rain The path is wet But the air is warm\nFeels like summer rain Feels like summer rain My clothes are damp but my skin is warm\n\n\n\nMemories of the past Memories of the past What I’ve learned And what didn’t last\nChorus 1 And I know I’m too young to know But I’ve asked those questions long ago And April rain Still falls\nAnd I know She’s right but she’s wrong And passing showers Only last so long\n\n\n\nMemories of the past Of games on the path Memories of the past What’s changed and what can last\nSmells like summer rain Smells like summer rain What I’ve learned And what’s the same\nChorus 2 And I know what I want to know And I’ve seen it happen, I’ve seen it grow While April rain Still falls\nAnd I know It’s right but it’s wrong And passing showers Only last so long"
  },
  {
    "objectID": "music/it-will-pass/index.html#counting-the-days",
    "href": "music/it-will-pass/index.html#counting-the-days",
    "title": "It will pass…",
    "section": "Counting the Days",
    "text": "Counting the Days\nVocals: Cillian McHugh Guitar: Cillian McHugh\n\n\n  \n    \n  \n\n\n\nWalking home across the city from the train to the plane You’re on your own, there’s no pity there’s no pain in vain When you said good-bye on the platform this time Like you said good-bye the first time\nYou know this place by day but it’s night this time Cold breeze on your face, night air and the city lights shine Across the water reflect the sights Across the water you’ll trek tonight\nGrowing apart, time and distance will take their toll But in your heart you know it’s wort it to make you whole The signs are there but hope remains ’Cause love is to share, it’s worth the strain\nBack on land back to normal doubt left behind You’re almost home, just the drive and in your mind You count the days till the next time Like you count the days each time"
  },
  {
    "objectID": "music/it-will-pass/index.html#if-i-care",
    "href": "music/it-will-pass/index.html#if-i-care",
    "title": "It will pass…",
    "section": "If I Care",
    "text": "If I Care\nVocals: Cillian McHugh Guitars: Cillian McHugh\n\n  \n    \n  \n\n\n\n\n\n  Bridge:\n           G~     D     Cmaj7   C9\n  E||------3------2------3------0---\n  B||------3------3------5------3---\n  G||------0------2------4------0---\n  D||------2------0------5------2---\n  A||--------------------3------3---\n  E||-------------------------------\n\n  Verse (picked):\n           Bm~    D~    Cmaj79  \n  E||-----(0)------------0---\n  B||------3------3------5---\n  G||------2------2------4---\n  D||------0------0------2---\n  A||------2-------------3---\n  E||-------------2----------\n\n  (Final chord is that D~)\n\nWhat if the best advice was wrong? Advice you followed for too long. And you turn to the bottle or the rolling good times, Did you find it there? Do you even care?\nI remember the secret joy, I remember your voice…… ……..your eyes…..\nI don’t know if i care for you, And I don’t know if you want me to. I don’t know…………. ….You…..\nI don’t care if you miss me too Coz I don’t care (how I felt for you) Coz if I care……. I’d care"
  },
  {
    "objectID": "music/it-will-pass/index.html#it-will-pass-1",
    "href": "music/it-will-pass/index.html#it-will-pass-1",
    "title": "It will pass…",
    "section": "It Will Pass",
    "text": "It Will Pass\nVocals: Cillian McHugh Guitar: Cillian McHugh\n\n  \n    \n  \n\n\n\nToo many times, But it will pass and come again, As history plays out, the cycle comes round again. And it’s back to the drawing board but you know its the same song over again same song over again…..same song over again\nYou can only be a happy fool for so long, Till you have to face the truth and accept that you’ve got it wrong. And poets of the past speak truths that will always last And the band sings your heart, drawing distractions\nToo many times, But it will pass and come again."
  },
  {
    "objectID": "music/it-will-pass/index.html#the-last-flight-home",
    "href": "music/it-will-pass/index.html#the-last-flight-home",
    "title": "It will pass…",
    "section": "The Last Flight Home",
    "text": "The Last Flight Home\nVocals: Cillian McHugh Backing Vocals: Maggie McHugh Acoustic Guitar: Cillian McHugh Electric Guitar (solo and slide): David Park Electric Guitar (rhythm and harmonies): Cillian McHugh Piano: Donal McHugh Drums: Donal McHugh Bass: Simon O’Donnell\n\n  \n    \n  \n\n\n\n  \n  (Capo 4th fret)\n  Intro/first 2 lines: \n       C#m     B       A       E  \n  G#|--0----|--0----|-(0)(0)|--0--0-|\n  D#|--1----|--1----|--1--1-|--1--1-|\n  B-|--2----|--0----|--2-p0-|--0--0-|\n  F#|--2-2-0|--0-2-0|--3--3-|--2--0-|\n  C#|--0----|--0----|--3--3-|--3--3-|\n  G#|--0----|--3----|-------|-------|\n  \n  Second 2 lines: \n       A       B       A       E  B \n  G#|-(0)---|--0--0-|-(0)---|--0--3-|\n  D#|--1----|--1--1-|--1----|--1--0-|\n  B-|--2----|--0--0-|--2----|--0--0-|\n  F#|--3----|--2--0-|--3----|--2--0-|\n  C#|--3----|--3--3-|--3----|--3--2-|\n  G#|-------|-------|-------|-----3-|\n\nAnd you know it’s done, and you know it’s too late to talk ’cause nothing you can say, and nothing you can do will change what’s come And you always knew this day could come, But your heart still believed\nBelieved you could overcome, believed she was the one, You tried to make it work, you thought you could make it work, And even when the cracks began to show, You never gave up hope\nThe four leaf charm, to hang on her arm, The phone call in the sun, remembering as one, one last time Back to how it used to be and what we had.\nTwo months on, the time you bought is gone, Today’s flight home, the last flight home"
  },
  {
    "objectID": "music/it-will-pass/index.html#three-cars-back",
    "href": "music/it-will-pass/index.html#three-cars-back",
    "title": "It will pass…",
    "section": "Three Cars Back",
    "text": "Three Cars Back\nVocals: Cillian McHugh Guitar: Cillian McHugh Harmonica: Cillian McHugh\n\n  \n    \n  \n\n\n\nSitting three cars back from the red light The fan’s on full, and the windows are down The light’s change as you wipe the sweat from your face\nBut the oncoming cars Mean nothing is going right And you’re stuck three cars back And you’re going nowhere\nTake out your phone and glance at the screen Still no reply, or answer to be see And you know that it’s worth waiting for But you know that you’ve been here before\nAnd is that all this is? Just another drunken kiss? And you’re three cars back And you’re going nowhere"
  },
  {
    "objectID": "music/it-will-pass/index.html#writers-block",
    "href": "music/it-will-pass/index.html#writers-block",
    "title": "It will pass…",
    "section": "Writer’s Block",
    "text": "Writer’s Block\nVocals: Cillian McHugh Everything Else: Lobster Johnson Electric Guitars (rhythm and clean lead): Andrew (Duke) Park Electric Guitars (lead): David Park Bass: Simon O’Donnell Drums: Donal McHugh Organ: Donal McHugh\n\n  \n    \n  \n\n\n\nHe tries to speak his mind and he finds the line but the words that he chose don’t fit, He tries to hide behind the disguise but he’s blind in the maze that he built himself, And he knows he shouldn’t worry And he knows that its only a song and that There really is no hurry But at the same time\nIt’s been a long long time and he’s falling behind and he feels he’s losing touch It’s been a long long time and its on his mind and he knows that that’s enough It’s been a long time he knows It’s been a long time he knows\nHe tries to sing his song but its been so long, the chords sound wrong and the melody’s gone He’s forgotten the lines that were in his mind, he can’t shake the feeling that he’s stealing from someone And he knows there’s something better And he knows that a reason will come And the next time will sound better But at the same time\nIt’s been a long long time and he’s falling behind and he feels he’s losing touch It’s been a long long time and its on his mind and he knows that that’s enough It’s been a long time he knows It’s been a long time he knows"
  },
  {
    "objectID": "music/launch-gig-photos/index.html",
    "href": "music/launch-gig-photos/index.html",
    "title": "Launch Gig (photos and videos)",
    "section": "",
    "text": "A collection of photos from the official launch of It will pass… in The Commercial on the 12th of April. Many thanks to Mags (and others) for her photography skills.\n\n\n\nalbum cover\n\n\nThe Artwork by Celina Buckley \n\n  \n    \n  \n\n\nFull set: Writer’s Block (With CLEF Choir); Counting the Days; There are no Words; Summer Rain; Unknown Arms; The Race; 24; The Happy Song; Back to the Drawing Board; It will pass…; The Last Flight Home; Bigness; Another Goodbye (With CLEF Choir) \n\n\n&lt;iframe src=\"https://www.youtube.com/embed/eqv_HIa3mfE\" frameborder=\"0\" allowfullscreen\n  style=\"position:absolute;top:0;left:0;width:100%;height:100%;\"&gt;&lt;/iframe&gt;\n\n\nAnother Goodbye with Clef Choir \n\n\n\nchoir\n\n\nClef Choir \n\n\n\nmaggie and cillian\n\n\nMaggie McHugh & Cillian McHugh \n\n\n\nmaggie and cillian\n\n\nCillian McHugh & Maggie McHugh \n\n\n\nmaggie and cillian\n\n\nMaggie McHugh & Cillian McHugh \n\n\n\ncillian\n\n\nCillian McHugh \n\n\n\nmaggie and cillian\n\n\nCillian McHugh & Maggie McHugh \n\n\n\nmaggie and cillian\n\n\nCillian McHugh & Maggie McHugh \n\n\n\nmaggie and cillian\n\n\nCillian McHugh & Maggie McHugh \n\n\n\ncillian\n\n\nCillian McHugh \n\n\n\nchoir\n\n\nClef Choir \n\n\n\ncillian\n\n\nCillian McHugh \n\n\n\npaul’s photo\n\n\nNiall Carmody, Cillian McHugh & Maggie McHugh"
  },
  {
    "objectID": "music/spectrum/index.html",
    "href": "music/spectrum/index.html",
    "title": "Spectrum",
    "section": "",
    "text": "Spectrum is a collection of the earliest songs I wrote. The shorter songs were later included on Back to the Drawing Board. The three longer songs (Thoughtcrime, Only in Dreams, and Riding Out) have not been properly released and are only available on Soundcloud (see links below)."
  },
  {
    "objectID": "music/spectrum/index.html#i-dust",
    "href": "music/spectrum/index.html#i-dust",
    "title": "Spectrum",
    "section": "(i) Dust",
    "text": "(i) Dust\n\n“war is peace, freedom is slavery, ignorance is strength”\n\nBlue overalls and dusty streets Doublethink and newspeak His poster’s on the wall The party will never fall.\nAlways watching you Watching every move Thought police in the ministry of love Like the powers of above\nTelescreen’s always on Even when you’re gone Never out of sight Watches through the night\nControl you’re entire life Even choose your wife Kids grow up to think like them And they’ll get you in the end\nAlways watching you…\nTwo minutes hate he saw her there Perfect figure and dark hair Chastity ribbon around her waist And the party in her face\nHe hated her from the start Saw a coldness in her heart She was one of them They’ll get you in the end\nAlways watching you…\nEvening in the city the light is dim He saw her had she followed him Hurried home with a quickened pace He wanted to smash her face"
  },
  {
    "objectID": "music/spectrum/index.html#ii-truth",
    "href": "music/spectrum/index.html#ii-truth",
    "title": "Spectrum",
    "section": "(ii) Truth?",
    "text": "(ii) Truth?\nThey came face to face She fell in the corridor he helped her up She put something into his hand\nCould it be true? On the flat piece of paper when he flattened it out Were the words “I love you”\nThey couldn’t meet People all around and the telescreen notice Every move and every sound\nVictory square They made their plans on the crowded street For a moment she held his hand\nMany weeks had passed They’d fallen in love but they knew it could not last\nLife was never the same Hiding off the streets…a dangerous game\n“We are the dead This game we’re playing…we can’t win”\n“Six months a year Five years…Death and life are the same thing”\n“Stop talking about death Don’t you enjoy life? Don’t you like feeling?”\n“We’re not dead yet I’m real, I’m solid I’m alive”\n“We are the dead”…”We are the dead”\n“YOU ARE THE DEAD”"
  },
  {
    "objectID": "music/spectrum/index.html#iii-you-are-the-dead",
    "href": "music/spectrum/index.html#iii-you-are-the-dead",
    "title": "Spectrum",
    "section": "(iii) You are the Dead",
    "text": "(iii) You are the Dead\nDeep inside the ministry of love The walls are white the light is bright\nOthers come and go as the day turns to night Or is it night to day there’s no way to say\nIt’s the place of no darkness and the truncheon blows You’re at their mercy and no one knows\nConfess it all and more on top Anything to make the torture stop"
  },
  {
    "objectID": "music/spectrum/index.html#iv-2-2-5",
    "href": "music/spectrum/index.html#iv-2-2-5",
    "title": "Spectrum",
    "section": "(iv) 2 + 2 = 5",
    "text": "(iv) 2 + 2 = 5\n\n“Freedom is the freedom to say that two plus two make four. If that is granted all else follows”\n\n(instrumental)"
  },
  {
    "objectID": "music/spectrum/index.html#v-the-inevitable",
    "href": "music/spectrum/index.html#v-the-inevitable",
    "title": "Spectrum",
    "section": "(v) The Inevitable",
    "text": "(v) The Inevitable\n\n“Thoughtcrime doesn’t entail death, Thoughtcrime is death”\n\nStrapped into the chair The rules of this are fair The rats would eat his face Unless she could take his place\nIn time they let him go How long he did not know Accepted he was dead With a bullet in his head\nDidn’t know when or where But the hope was always there For a victory of his own To be pure to the bone\nAt last came the day As always is the way Finally it was true He loved him through and through"
  },
  {
    "objectID": "music/spectrum/index.html#part-i",
    "href": "music/spectrum/index.html#part-i",
    "title": "Spectrum",
    "section": "Part I",
    "text": "Part I\nSummer time and the dancing Music playing and the band sings Across the lake you see the lights Colourful glow on a beautiful night\nFootsteps along the path glimmer of white Through the bushes it was love at first sight Her eyes, her arms, her hair…he was lost He helped her hide he showed her where\nWhen the boys came down he stood his ground in the middle of the path He was ready to fight, fight for her that night but it didn’t come to that They cleared off he went across to find her in the dark A few brief words he went back to work she was gone only black\nHolidays came it was still the same She filled his thoughts he dreamt her name Across the street and down the road Was it really her he had to know\nFollowed her home and called her name, she turned around No doubt, it was her, she was found In they went, met the lads…coffee and chat He asked her out for Friday night…simple as that\nFriday at eight he could not wait in a daze all the week University Park instead of the bar just talking by the creek Her past days in a secretive way he loved to hear her speak Drunk with love swimming with love…he was in a dream"
  },
  {
    "objectID": "music/spectrum/index.html#part-ii",
    "href": "music/spectrum/index.html#part-ii",
    "title": "Spectrum",
    "section": "Part II",
    "text": "Part II\nPlywood nailed across the door, left her sleeping on a friend’s floor Nowhere else to go…money running low She wished she’d let him in, explained her past to him Bell rang in the empty hall. No hope at all.\nHe found the deserted house. His heart had been ripped out. He walked the streets alone. He couldn’t stay at home. He longed to see her there. Fought wild without a care. Broken promises and lies, betrayal made him wise."
  },
  {
    "objectID": "music/spectrum/index.html#part-iii",
    "href": "music/spectrum/index.html#part-iii",
    "title": "Spectrum",
    "section": "Part III",
    "text": "Part III\nAnd the memory fades…faded like a dream The morning of the raid…like a sword in between Night and day…searching through the streets Sorrow and dismay…they would never meet\nHow was he to know…? His name it was a lie The white Mercedes rolled…he ran out of time He told him where to go…guilty of the crime Inside she died alone…she couldn’t find the light"
  },
  {
    "objectID": "music/spectrum/index.html#i-natural-selection",
    "href": "music/spectrum/index.html#i-natural-selection",
    "title": "Spectrum",
    "section": "(i) Natural Selection",
    "text": "(i) Natural Selection\nWe grow…Prosperity is unlimited We grow…Exponentially we’re unlimited\nIron bronze and steel, farming and the wheel Farms and walls and towns, we settled down Tools by human hand, we exploit the land Fishing in empty seas, we’re running out of trees\nChorus: Avoiding natural selection Survival of the fittest but it seems we’ve got exemption We may postpone but we can’t escape the question The earth has limits and we’re no exception\nRoads spread near and far for lorry truck and car Factories industry and trade, an easy life was made We can do anything if we try, we even learned to fly New medicines heal the sick, lives saved by a pin prick\nChorus"
  },
  {
    "objectID": "music/spectrum/index.html#ii-final-generation",
    "href": "music/spectrum/index.html#ii-final-generation",
    "title": "Spectrum",
    "section": "(ii) Final Generation",
    "text": "(ii) Final Generation\nGot a job, a car, a home and family A happy life it’s trouble free Famine and wars and deaths on TV Don’t worry about it; it’s not you or me\nIt’s far away and out of sight Not our problem not our fight Global warming and UV light Sure we don’t care we’re doing alright\nWasted time waiting in line, Every day in the way Mine or yours, yours or mine, It’s ourselves that we mind\nTraffic jams in the street;  We’ve all got cars we’ve all got feet Block the roads with empty seats, We all contribute to the heat\nPoverty, inflation, famine devastation Are we the final generation?"
  },
  {
    "objectID": "music/spectrum/index.html#iii-riding-out",
    "href": "music/spectrum/index.html#iii-riding-out",
    "title": "Spectrum",
    "section": "(iii) Riding Out",
    "text": "(iii) Riding Out\n(instrumental)"
  },
  {
    "objectID": "music/spectrum/index.html#iv-we-never-learn",
    "href": "music/spectrum/index.html#iv-we-never-learn",
    "title": "Spectrum",
    "section": "(iv) We Never Learn",
    "text": "(iv) We Never Learn\nIs it too late? Have we missed our chance? Will the world of today burn out or fade away? The answer and the rainbow and the song\nIt can’t go on. Time’s running out. Will we ever listen and will we ever learn? Warnings in time\nWho starts the wars? Who builds the cars? Who cuts down the trees as they poison the air? Who owns the land? Whose God is real? And who gets the food?\nWe need change to save tomorrow To slow the growth and stop killing our world The answer and the rainbow and the song"
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#stars",
    "href": "music/back-to-the-drawing-board/index.html#stars",
    "title": "Back to the Drawing Board",
    "section": "Stars",
    "text": "Stars\nVocals: Ciara McHugh, Cillian McHugh Guitars: Cillian McHugh Electric Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nI watch the sun go down: Purple and red through the scattered clouds. Golden light fills the sky.\nDo you remember by the water? Clouds in the way, sky was grey, It didn’t matter.\nI watch the darkness fall and surround us all. Warmth and light Are faded and gone.\nCold bite on a lonely night. Chill in the air as I sit and stare\nStare at the sky for hours trying to count the stars. So far away, so long ago Does it matter?\nStanding alone, under the moon’s glow Empty night and the starlight. Silver streak shines on your cheek as a star winks out."
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#faith",
    "href": "music/back-to-the-drawing-board/index.html#faith",
    "title": "Back to the Drawing Board",
    "section": "Faith",
    "text": "Faith\nVocals: Cillian McHugh Guitars: Cillian McHugh Electric Piano: Donal McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nWhat if I told you that everyone was wrong? wrong in what they believe… And what if I told you that everyone was right right in what they believe … but wrong…?…\nWould you call me mad if I told you right was wrong and wrong was right?\nGood and Evil invented by men trying to explain what we cannot comprehend. Don’t search for the answer ’cause it’s right there to be found… and not there… and right there.\nWould you call me mad if I told you there’s no point? and that’s the point… so what’s the point?\nFollow your heart and believe in what you believe… let it’s music reach you, just listen. And live your life ’cause it’s all that you can know… take good with bad and let the balance flow.\nWould you call me mad if I told you that I knew…?…and didn’t know…because I know…?.."
  },
  {
    "objectID": "music/back-to-the-drawing-board/index.html#section",
    "href": "music/back-to-the-drawing-board/index.html#section",
    "title": "Back to the Drawing Board",
    "section": "24",
    "text": "24\nVocals: Ciara McHugh, Cillian McHugh Guitars: Cillian McHugh Drums: Donal McHugh Bass: Cillian McHugh\n\n  \n    \n  \n\n\n\n\nVerse 1 Reverse the answer, It’s 24 But what is the question? No one cares anymore God is Forgotten, Who needs heaven’s door We’ve got money\nChorus And the more we learn, Learn about the earth The more we seem to forget Technologies and good careers, we try to beat our peer And 24 is all the time we get\nVerse 2 The edge of the rainbow goes up to 24 On the other side we’re getting fried, more than ever before Chainsaws and trucks expose the forest floor And we let it happen\n\n Intro: \n        F#m     Bm9    F#m      Bm9     F#m7  D  D/A    F#m  Bm9 Bm96\n  F#|---0---|---0---|--0----|---0---|---3---|-0--3--|---0---|-0--0--|\n  C#|---0---|---0---|--0----|---0---|---3---|-0--3--|---0---|-0--0--|\n  A-|---0---|---5---|--0----|---5---|---0---|-5--5--|---0---|-5--4--|\n  E-|---2---|---5---|--2----|---5---|---2---|-5--5--|---2---|-5--4--|\n  B-|---2---|---0---|--2-0h2|---0---|---2---|-3--3--|---2---|-0--0--|\n  F#|---0---|-------|--0----|-------|---0---|----3--|---0---|-------|\n\n  Verse: \n        F#m  Bm9 Bm96   F#m  Bm9 Bm96   F#m  Bm9 Bm96   F#m  Bm9Bm96Bm9\n  F#|---0---|-0--0--|---0---|-0--0--|---0---|-0--0--|---0---|-0--0--0-\n  C#|---0---|-0--0--|---0---|-0--0--|---0---|-0--0--|---0---|-0--0--0-\n  A-|---0---|-5--4--|---0---|-5--4--|---0---|-5--4--|---0---|-5--4--2-\n  E-|---2---|-5--4--|---2---|-5--4--|---2---|-5--4--|---2---|-5--4--2-\n  B-|---2---|-0--0--|---2---|-0--0--|---2---|-0--0--|---2---|-0--0--0-\n  F#|---0---|-------|---0---|-------|---0---|-------|---0---|---------\n\n  Chorus:                                  (not sure of the written timing here)\n       Dmaj7    A      Dmaj7  A  F#m   Dmaj7    A     Bm A      C#m \n  F#|---3---|---3---|---3---|-3--0--|---3---|---3---|-0--3--|---2---|\n  C#|---0---|---0---|---0---|-0--0--|---0---|---0---|-1--3--|---3---|\n  A-|---0---|---0---|---0---|-0--0--|---0---|---0---|-2--0--|---4---|\n  E-|---2---|---0---|---2---|-0--2--|---2---|---0---|-2--0--|---4---|\n  B-|---3---|---2---|---3---|-2--2--|---3---|---2---|-0--2--|---2---|\n  F#|-------|-------|-------|----0--|-------|-------|----3--|-------|"
  },
  {
    "objectID": "posts/2018-10-20-a_lazy_function/index.html",
    "href": "posts/2018-10-20-a_lazy_function/index.html",
    "title": "A Lazy Function",
    "section": "",
    "text": "It has been quite a while since I posted, but I haven’t been idle, I completed my PhD since the last post, and I’m due to graduate next Thursday. I am also delighted to have recently been added to R-bloggers.com so I’m keen to get back into it."
  },
  {
    "objectID": "posts/2018-10-20-a_lazy_function/index.html#a-lazy-function",
    "href": "posts/2018-10-20-a_lazy_function/index.html#a-lazy-function",
    "title": "A Lazy Function",
    "section": "A Lazy Function",
    "text": "A Lazy Function\nI have already written 2 posts about writing functions, and I will try to diversify my content. That said, I won’t refrain from sharing something that has been helpful to me. The function(s) I describe in this post is an artefact left over from before I started using R Markdown. It is a product of its time but may still be of use to people who haven’t switched to R Markdown yet. It is lazy (and quite imperfect) solution to a tedious task.\n\nThe Problem\nAt the time I wrote this function I was using R for my statistics and Libreoffice for writing. I would run a test in R and then write it up in Libreoffice. Each value that needed reporting had to be transferred from my R output to Libreoffice - and for each test there are a number of values that need reporting. Writing up these tests is pretty formulaic. There’s a set structure to the sentence, for example writing up a t-test with a significant result nearly always looks something like this:\n\nAn independent samples t-test revealed a significant difference in X between the Y sample, (M = [ ], SD = [ ]), and the Z sample, (M = [ ], SD = [ ]), t([df]) = [ ], p = [ ].\n\nAnd the write up of a non-significant result looks something like this:\n\nAn independent samples t-test revealed no significant difference in X between the Y sample, (M = [ ], SD = [ ]), and the Z sample, (M = [ ], SD = [ ]), t([df]) = [ ], p = [ ].\n\nSeven values (the square [ ] brackets) need to be reported for this single test. Whether you copy and paste or type each value, the reporting of such tests can be very tedious, and leave you prone to errors in reporting.\n\n\nThe Solution\nIn order to make reporting values easier (and more accurate) I wrote the t_paragraph() function (and the related t_paired_paragraph() function). This provided an output that I could copy and paste into a Word (Libreoffice) document. This function is part of the desnum1 package (McHugh 2017).\n\nThe t_parapgraph() Function\nThe t_parapgraph() function runs a t-test and generates an output that can be copied and pasted into a word document. The code for the function is as follows:\n\n# Create the function t_paragraph with arguments x, y, and measure\n# x is the dependent variable\n# y is the independent (grouping) variable\n# measure is the name of dependent variable inputted as string\n\nt_paragraph &lt;- function (x, y, measure){\n  \n  # Run a t-test and store it as an object t\n  \n  t &lt;- t.test(x ~ y)\n  \n  \n  # If your grouping variable has labelled levels, the next line will store them for reporting at a later stage\n  \n  labels &lt;- levels(y)\n  \n  # Create an object for each value to be reported\n  \n  tsl &lt;- as.vector(t$statistic)\n  ts &lt;- round(tsl, digits = 3)\n  tpl &lt;- as.vector(t$p.value)\n  tp &lt;- round(tpl, digits = 3)\n  d_fl &lt;- as.vector(t$parameter)\n  d_f &lt;- round(d_fl, digits = 2)\n  ml &lt;- as.vector(tapply(x, y, mean))\n  m &lt;- round(ml, digits = 2)\n  sdl &lt;- as.vector(tapply(x, y, sd))\n  sd &lt;- round(sdl, digits = 2)\n  \n  # Use print(paste0()) to combine the objects above and create two potential outputs\n  # The output that is generated will depend on the result of the test\n  \n  \n  # wording if significant difference is observed\n  \n  if (tp &lt; 0.05) \n    print(paste0(\"An independent samples t-test revealed a significant difference in \", \n                 measure, \" between the \", labels[1], \" sample, (M = \", \n                 m[1], \", SD = \", sd[1], \"), and the \", labels[2], \n                 \" sample, (M =\", m[2], \", SD =\", sd[2], \"), t(\", \n                 d_f, \") = \", ts, \", p = \", tp, \".\"), quote = FALSE, \n          digits = 2)\n  \n  # wording if no significant difference is observed      \n  \n  if (tp &gt; 0.05) \n    print(paste0(\"An independent samples t-test revealed no difference in \", \n                 measure, \" between the \", labels[1], \" sample, (M = \", \n                 m[1], \", SD = \", sd[1], \"), and the \", labels[2], \n                 \" sample, (M = \", m[2], \", SD =\", sd[2], \"), t(\", \n                 d_f, \") = \", ts, \", p = \", tp, \".\"), quote = FALSE, \n          digits = 2)\n}\n\nWhen using t_paragraph(), x is your DV, y is your grouping variable while measure is a string value that the name of the dependent variable. To illustrate the function I’ll use the mtcars dataset.\n\n\nApplications of the t_parapgraph() Function\nThe mtcars dataset is comes with R. For information on it simply type help(mtcars). The variables of interest here are am (transmission; 0 = automatic, 1 = manual), mpg (miles per gallon), qsec (1/4 mile time). The two questions I’m going to look at are:\n\nIs there a difference in miles per gallon depending on transmission?\nIs there a difference in 1/4 mile time depending on transmission?\n\nBefore running the test it is a good idea to look at the data2. Because we’re going to look at differences between groups we want to run descriptives for each group separately. To do this I’m going to combine the the descriptives() function which I previously covered here (also part of the desnum package) and the tapply() function.\nThe tapply() function allows you to run a function on subsets of a dataset using a grouping variable (or index). The arguments are as follows tapply(vector, index, function). vector is the variable you want to pass through function; and index is the grouping variable. The examples below will make this clearer.\nWe want to run descriptives on mtcars$mpg and on mtcars$qsec and for each we want to group by transmission (mtcars$am). This can be done using tapply() and descriptives() together as follows:\n\ntapply(mtcars$mpg, mtcars$am, descriptives)\n\n$`0`\n      mean       sd  min  max len\n1 17.14737 3.833966 10.4 24.4  19\n\n$`1`\n      mean       sd min  max len\n1 24.39231 6.166504  15 33.9  13\n\n\nRecall that 0 = automatic, and 1 = manual. Replace mpg with qsec and run again:\n\ntapply(mtcars$qsec, mtcars$am, descriptives)\n\n$`0`\n      mean       sd   min  max len\n1 18.18316 1.751308 15.41 22.9  19\n\n$`1`\n   mean       sd  min  max len\n1 17.36 1.792359 14.5 19.9  13\n\n\n\n\n\nRunning t_paragraph()\nNow that we know the values for automatic vs manual cars we can run our t-tests using t_paragraph(). Our first question:\n\nIs there a difference in miles per gallon depeding on transmission?\n\n\nt_paragraph(mtcars$mpg, mtcars$am, \"miles per gallon\")\n\n[1] An independent samples t-test revealed a significant difference in miles per gallon between the  sample, (M = 17.15, SD = 3.83), and the  sample, (M =24.39, SD =6.17), t(18.33) = -3.767, p = 0.001.\n\n\nThere is a difference, and the output above can be copied and pasted into a word document with minimal changes required.\nOur second question was:\n\nIs there a difference in 1/4 mile time depending on transmission?\n\n\nt_paragraph(mtcars$qsec, mtcars$am, \"quarter-mile time\")\n\n[1] An independent samples t-test revealed no difference in quarter-mile time between the  sample, (M = 18.18, SD = 1.75), and the  sample, (M = 17.36, SD =1.79), t(25.53) = 1.288, p = 0.209.\n\n\nThis time there was no significant difference, and again the output can be copied and pasted into word with minimal changes.\n\n\nLimitations\nThe function described was written a long time ago, and could be updated. However I no longer copy and paste into word (having switched to R markdown instead). The reporting of the p value is not always to APA standards. If p is &lt; .001 this is what should be reported. The code for t_paragraph() could be updated to include the p_report function (described here) which would address this. Another limitation is that the formatting of the text isn’t perfect, the letters (N,M,SD,t,p) should all be italicised, but having to manually fix this formatting is still easier than manually transferring individual values.\n\n\nConclusion\nDespite the limitations the functions t_paragraph() and t_paired_paragraph()3 have made my life easier. I still use them occasionally. I hope they can be of use to anyone who is using R but has not switched to R Markdown yet."
  },
  {
    "objectID": "posts/2018-10-20-a_lazy_function/index.html#footnotes",
    "href": "posts/2018-10-20-a_lazy_function/index.html#footnotes",
    "title": "A Lazy Function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo install desnum just run devtools::install_github(\"cillianmiltown/R_desnum\")↩︎\nIn this case this is particularly useful because there are no value labels for mtcars$am, so it won’t be clear from the output which values refer to the automatic group and which refer to the manual group. Running descriptives will help with this.↩︎\nIf you want to see the code for t_paired_paragraph() just load desnum and run t_paired_paragraph (without parenthesis)↩︎"
  },
  {
    "objectID": "other.html",
    "href": "other.html",
    "title": "Other",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nMoral Networks\n\n\n\n\n\nA Shiny App for Making Moral Networks\n\n\n\n\n\n\nDec 10, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nJS Mediation\n\n\n\n\n\nA Shiny App for running JS Mediation\n\n\n\n\n\n\nMay 15, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nChess\n\n\n\n\n\nIn my spare time I also enjoy playing chess…\n\n\n\n\n\n\nMar 18, 2019\n\n\nCillian McHugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "rblog.html",
    "href": "rblog.html",
    "title": "Rblog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nA Shiny App for JS Mediation\n\n\n\n\n\nA decription of how to make a Shiny App for mediation analyses\n\n\n\n\n\n\nAug 23, 2019\n\n\nCillianMacAodh (Cillian McHugh)\n\n\n\n\n\n\n  \n\n\n\n\nR Markdown Workshop\n\n\n\n\n\nMaterials from a workshop on using RMarkdown\n\n\n\n\n\n\nAug 3, 2019\n\n\nCillianMacAodh (Cillian McHugh)\n\n\n\n\n\n\n  \n\n\n\n\nA Lazy Function\n\n\n\n\n\nA description of a function I wrote that does a specific job\n\n\n\n\n\n\nOct 20, 2018\n\n\nCillianMacAodh (Cillian McHugh)\n\n\n\n\n\n\n  \n\n\n\n\nWriting functions - Part two\n\n\n\n\n\n\n\n\n\n\n\n\nAug 27, 2017\n\n\nCillian McHugh\n\n\n\n\n\n\n  \n\n\n\n\nWriting functions - Part one\n\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2017\n\n\nCillian McHugh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2019-08-03-rmarkdown_workshop/index.html",
    "href": "posts/2019-08-03-rmarkdown_workshop/index.html",
    "title": "R Markdown Workshop",
    "section": "",
    "text": "This is an unusual post for me, I have avoided writing about R Markdown because there are so many resources already available on the topic (e.g., here, here, and here). However, recently I ran a session on using RMarkdown for my colleagues in the Centre for Social Issues Research. The aim of this was to demonstrate the usefulness of R Markdown (and hopefully convert a few people). For this session I created a set of resources1 aimed at making the transition from SPSS to R Markdown a bit easier. The statistics content of these resources is mainly just some of the simpler standard tests taught to psychology undergraduate students.\nThe complete resources are available on this project page on the OSF. The main purpose of the exercise was to provide people with the tools to create this pdf using this R Markdown template. My hope is that by using this template, SPSS users might make the tranistion to R, and R Markdown (with the help of the wonderful papaja package Aust (2017))."
  },
  {
    "objectID": "posts/2019-08-03-rmarkdown_workshop/index.html#working-with-dataframes",
    "href": "posts/2019-08-03-rmarkdown_workshop/index.html#working-with-dataframes",
    "title": "R Markdown Workshop",
    "section": "Working with dataframes",
    "text": "Working with dataframes\nA dataframe is structured much like an SPSS file. There are rows and columns, the columns are named and generally represent variables. The rows (can also be named) generally represent cases. You can have multiple data frames loaded with different names, although they are commonly saved as df (and these can be numbered df1 df2 df3. If your document/code is well organised, it can be useful have a generic name for dataframes that you are working with. This means that much of your code can be recycled (particularly if the variable names are the same - if you run repeated studies, or studies with only minor changes, you will find that there is massive scope for recycling code - both chunks and in-line)\n\nSome basics:\n\nThe entire dataframe can be printed to the console by running the name of the data frame\nThe dollar sign can be used to call specific variables from the data frame i.e., df$variable_name\nA function has the form “function name” followed by parenthesis: function_name().\nThe object that you want to run the function on goes in the parenthesis. e.g., if our dataframe was called df, and age was called age and we wanted to get the mean age we would run mean(df$age).\nSometimes missing data denoted by NA can mess with some functions, to account for this it is helpful to include the argument na.rm = TRUE in the function, e.g., mean(df$age, na.rm = TRUE)\n\nThe mtcars dataset comes with R. For information on it simply type help(mtcars). The variables of interest here are am (transmission; 0 = automatic, 1 = manual), mpg (miles per gallon), qsec (1/4 mile time). Below we practice a few simple functions to find out information about the dataset.\n\nExample code and output:\n\nLoad the mtcars dataset into an object called df using the command df &lt;- mtcars\nView the variable names associated with df by running variable.names(df)\n\n\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\nThe mean miles per gallon can be calculated using mean(df$mpg)\n\n\n\n[1] 20.09062\n\n\n\nThe standard deviation of the same variable is calculated using sd(df$mpg)\n\n\n\n[1] 6.026948\n\n\n\nOr if you want to see basic descriptives use descriptives(df$mpg)3\n\n\n\n      mean       sd  min  max len\n1 20.09062 6.026948 10.4 33.9  32\n\n\n\nTo index by a variable we use square brackets [] and the which() function.\n\nThe following command gets the mean miles per gallon for all cars with manual transmission:\n\nmean(df$mpg[which(df$am==1)])\n\n\n\n\n\n[1] 24.39231"
  },
  {
    "objectID": "posts/2019-08-03-rmarkdown_workshop/index.html#t-test-transmission-and-mpg",
    "href": "posts/2019-08-03-rmarkdown_workshop/index.html#t-test-transmission-and-mpg",
    "title": "R Markdown Workshop",
    "section": "T-test: Transmission and MPG",
    "text": "T-test: Transmission and MPG\n\nLoad mtcars and save it in your environment using df &lt;- mtcars\nCreate a new dataframe with a generic name e.g., x using the command: x &lt;- df\nThis command runs the t-test and you can see the output in the console t.test(x$mpg~x$am)\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  x$mpg by x$am\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231 \n\n\n\nThe following code runs the t-test but saves the output as a list t that can be called later: t &lt;- t.test(x$mpg~x$am)\n\n\nAs with dataframes, specific variables within a list can be called using the dollar sign\nTo call the p value simply type t$p.value\n\n\n\n[1] 0.001373638\n\n\n\nTo call the t statistic, type t$statistic\n\n\n\n        t \n-3.767123 \n\n\n\nAnd to call the degrees of freedom, type t$parameter\n\n\n\n      df \n18.33225 \n\n\n\nFinally, to calculate the effect size and save it to an object type td &lt;- cohensD(mpg~am, data=x)\n\nFrom the above we can call each value we need using in-line code to write up our results section as follows\n\nThis is what the paragraph will look like in your Rmd document:\nAn independent samples t-test revealed a significant difference in miles per gallon between cars with automatic transmission (*M* = `r mean(x$mpg[which(x$am==0)])`, *SD* = `r sd(x$mpg[which(x$am==0)])`), and cars with manual transmission, (*M* = `r mean(x$mpg[which(x$am==1)])`, *SD* = `r sd(x$mpg[which(x$am==1)])`), *t*(`r t$parameter`) = `r t$statistic`, *p* `r paste(p_report(t$p.value))`, *d* = `r td.\n\n\nThe above syntax will return the following:\nAn independent samples t-test revealed a significant difference in miles per gallon between cars with automatic transmission (M = 17.15, SD = 3.83), and cars with manual transmission, (M = 24.39, SD = 3.83), t(18.33) = -3.767, p = .001, d = 1.48.\nIf you want to run another t-test later on in your document you simply run it in a code chunk and create new objects (t and td) with the same names as before and you can use the same write up as above to report it."
  },
  {
    "objectID": "posts/2019-08-03-rmarkdown_workshop/index.html#footnotes",
    "href": "posts/2019-08-03-rmarkdown_workshop/index.html#footnotes",
    "title": "R Markdown Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe aim of this post is to help make these resources more accessible. As such, there will likely be a lot of duplication between this post and the resources on the OSF.↩︎\nItalics are achieved by placing a star either side of the text you want italicised *italics* = italics; Bold is achieved by placing a double star either side of the text you want italicised **bold** = bold↩︎\nfrom the desnum package↩︎\nThis test, and all tests that follow are for illustration purposes only, I have not checked any assumptions to see if I can run the tests, I just want to provide sample code that you can use for your own analyses.↩︎"
  },
  {
    "objectID": "posts/2019-08-11-js_mediation/index.html",
    "href": "posts/2019-08-11-js_mediation/index.html",
    "title": "A Shiny App for JS Mediation",
    "section": "",
    "text": "This is a brief post about making my first Shiny App (see also). I made this app following a meeting of the Advancing Social Cognition lab (ASC-Lab) where we discussed this paper by Yzerbyt et al. (2018) proposing a new method for mediation analysis. Any attempt to detail the differences in methods is well beyond the scope of a blog post. The take home message is that the method proposed by Yzerbyt et al. (2018) is less prone to Type I errors (or false positives) than the most commonly used methods (e.g., Hayes 2017). In addition to identifying a problem and proposing a solution, the authors also provide the tools to implement their solution with an R package (Batailler et al. 2019). Unfortunately, not everyone uses R, and this is why I set about developing a simple way for SPSS users to access this new method."
  },
  {
    "objectID": "posts/2019-08-11-js_mediation/index.html#set-up-the-dataframe",
    "href": "posts/2019-08-11-js_mediation/index.html#set-up-the-dataframe",
    "title": "A Shiny App for JS Mediation",
    "section": "Set up the dataframe",
    "text": "Set up the dataframe\nFor ease of reusing code (particularly later on) I’ll save mtcars as a dataframe df and rename the variables of interest as iv (predictor variable), dv (outcome variable), and mediator.\n\ndf &lt;- mtcars          # create df from mtcars\n\n# create new variables with generic names\ndf$dv &lt;- df$qsec      # save 1/4 mile time as dv\ndf$iv &lt;- df$hp        # save horsepower as iv\ndf$mediator &lt;- df$wt  # save weight as mediator"
  },
  {
    "objectID": "posts/2019-08-11-js_mediation/index.html#simple-regression",
    "href": "posts/2019-08-11-js_mediation/index.html#simple-regression",
    "title": "A Shiny App for JS Mediation",
    "section": "Simple Regression",
    "text": "Simple Regression\nBefore running the mediation I’ll run a quick regression to assess the nature of the relationship between the variables.\n\nfit &lt;- lm(dv ~ iv + mediator, data=df)  # save the regression in an object 'fit'\nsummary(fit)                            # show the results\n\n\nCall:\nlm(formula = dv ~ iv + mediator, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8283 -0.4055 -0.1464  0.3519  3.7030 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.825585   0.671867  28.020  &lt; 2e-16 ***\niv          -0.027310   0.003795  -7.197 6.36e-08 ***\nmediator     0.941532   0.265897   3.541  0.00137 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.09 on 29 degrees of freedom\nMultiple R-squared:  0.652, Adjusted R-squared:  0.628 \nF-statistic: 27.17 on 2 and 29 DF,  p-value: 2.251e-07\n\n\nAs you can see from the output, 1/4 mile time is predicted by both horsepower and by weight."
  },
  {
    "objectID": "posts/2019-08-11-js_mediation/index.html#simple-mediation",
    "href": "posts/2019-08-11-js_mediation/index.html#simple-mediation",
    "title": "A Shiny App for JS Mediation",
    "section": "Simple Mediation",
    "text": "Simple Mediation\nNow that we have a picture of the relationships between the variables we can run the mediation analysis. The code for this is detailed below.\n\nJS_model &lt;- mdt_simple(data = df, # create an object 'JS_model'\n                       DV = dv,\n                       IV = iv,\n                       M  = mediator)\nadd_index(JS_model)               # display the results of the mediation\n\nTest of mediation (simple mediation)\n==============================================\n\nVariables:\n\n- IV: iv \n- DV: dv \n- M: mediator \n\nPaths:\n\n====  ==============  =====  ======================\nPath  Point estimate     SE  APA                   \n====  ==============  =====  ======================\na              0.009  0.002  t(30) = 4.80, p &lt; .001\nb              0.942  0.266  t(29) = 3.54, p = .001\nc             -0.018  0.003  t(30) = 5.49, p &lt; .001\nc'            -0.027  0.004  t(29) = 7.20, p &lt; .001\n====  ==============  =====  ======================\n\nIndirect effect index:\n\n- type: Indirect effect \n- point estimate: 0.00885 \n- confidence interval:\n  - method: Monte Carlo (5000 iterations)\n  - level: 0.05 \n  - CI: [0.00347; 0.0158]\n\nFitted models:\n\n- X -&gt; Y \n- X -&gt; M \n- X + M -&gt; Y \n\n\n\nHere we can see that horsepower predicts both 1/4 mile time and weight.\nThere is also an indirect effect of horsepower on 1/4 mile time through weight."
  },
  {
    "objectID": "posts/2019-08-11-js_mediation/index.html#the-geography-of-the-shiny-app",
    "href": "posts/2019-08-11-js_mediation/index.html#the-geography-of-the-shiny-app",
    "title": "A Shiny App for JS Mediation",
    "section": "The Geography of the Shiny App",
    "text": "The Geography of the Shiny App\nThe Shiny App has two panels.\n\nOn the left we have:\n\nThe data upload option\nA dropdown menu for selecting the data you wish to use (the uploaded file, the mtcars data set, or the iris data set)\nDropdown menus for defining each of your variables,\nText describing the App\n\nOn the right we have:\n\nThe output of the regression\nThe output from the mediation analysis\n\n\nThe code for generating these panels is below (comments above relevant lines describe the purpose of the various sections):\n# UI for app\nui&lt;-(pageWithSidebar(\n\n# We use headerPanel() to give a title to our app \n  headerPanel(\"JS Mediation\"),\n  \n# use sidebarPanel() to create the content of the side panel (panel on the left)\n  sidebarPanel\n  (\n# use fileInput() to create a dialogue for inputting a file\n    fileInput(\"file1\", \"Upload SPSS File\",\n              multiple = TRUE,\n              accept = c(\".sav\")),\n# create a horizontal line break\n    tags$hr(),\n    \n# create a dropdown menu for selecting the dataset to be used\n    selectInput(\"dataset\",\"Data:\",\n                choices =list(iris = \"iris\",\n                              mtcars = \"mtcars\",\n                              uploaded_file = \"inFile\"), selected=NULL),\n# create a dropdown menu for selecting the dependent variable to be used\n    selectInput(\"dv\",\"Dependent Variable:\", choices = NULL),\n# create a dropdown menu for selecting the Independent variable to be used\n    selectInput(\"iv\",\"Independent Variable:\", choices = NULL),\n# create a dropdown menu for selecting the mediator to be used\n    selectInput(\"mediator\",\"Mediator:\", choices = NULL) #,\n    \n# use HTML() to input formatted text describing the App\n    ,HTML('In response to \n    &lt;a href=\"https://perso.uclouvain.be/vincent.yzerbyt/Yzerbyt%20et%20al.%20JPSP%202018.pdf\"&gt;this&lt;/a&gt;\n    paper by Yzerbyt, Batailler and Judd (2018) which outined a new method of conducting mediation analyses\n    (with less susceptability to false positives than Hayes’ PROCESS) I created a ShinyApp so that their\n    R-package could be used by SPSS users. Upload your SPSS file above and select the variables you wish\n    to compare.')\n    ,br(),br(),br()\n    ,HTML('&lt;p&gt;Yzerbyt, V., Muller, D., Batailler, C., &amp; Judd, C. M. (2018). New Recommendations for\n    Testing Indirect  Effects in Mediational Models: The Need to Report and Test Component Paths.\n    &lt;em&gt;Journal of Personality and Social Psychology: Attitudes and Social Cognition&lt;/em&gt;, 115(6), \n    929–943. &lt;a href=\"http://dx.doi.org/10.1037/pspa0000132\"\n    class=\"uri\"&gt;http://dx.doi.org/10.1037/pspa0000132&lt;/a&gt;&lt;/p&gt;')\n  ),\n  \n# use mainPanel() to create the panel on the right where the output of our tests will be\n  mainPanel(\n# give a title to the the first output\n    h3(\"Summary of Regression Model\"),\n# report the result of the regression, saved in the object 'fit'\n    verbatimTextOutput(\"fit\"),\n# give a title for the second output\n    h3(\"Mediation Results\"),\n# report the result of the mediation, saved in the object 'mediation'\n    verbatimTextOutput(\"mediation\")\n  )\n))"
  },
  {
    "objectID": "posts/2019-08-11-js_mediation/index.html#the-backend-of-the-shiny-app",
    "href": "posts/2019-08-11-js_mediation/index.html#the-backend-of-the-shiny-app",
    "title": "A Shiny App for JS Mediation",
    "section": "The Backend of the Shiny App",
    "text": "The Backend of the Shiny App\nAbove we have the code for setting up and modifying the look and feel of our app. Below we go through the code for making the app do what it is supposed to do. The code in full is at the bottom of this post, however I have isolated specific sections of code to describe their function.\n\nInputting data from file\nThe code below runs read.spss() on whatever file you have uploaded using the dialogue box in the side panel and creates a dataframe called inFile.\n upload_data&lt;-reactive({\n    inFile &lt;- input$file1\n    if (is.null(inFile))\n      return(NULL)\n    read.spss(input$file1$datapath, to.data.frame = TRUE)\n  })\n  \n  observeEvent(input$file1,{\n    inFile&lt;&lt;-upload_data()\n  })\n\n\n\nSelecting data and variables\nThe code below retrieves information about the dataset that is selected, and displays the variables associated with the selected dataset in the dropdown menus for each of your variables (IV, DV, & mediator).\n# update variables based on the data\n  observe({\n# make sure upload exists\n    if(!exists(input$dataset)) return() \n# retrieve names of columns (variable names) and save as 'var.opts'\n    var.opts&lt;-colnames(get(input$dataset))\n# set var.opts as the options for the drop down menus\n    updateSelectInput(session, \"dv\", choices = var.opts)\n    updateSelectInput(session, \"iv\", choices = var.opts)\n    updateSelectInput(session, \"mediator\", choices = var.opts)\n  })\n\n\nSetting up data for analysis\nBelow we extract the data and variables selected in the dropdown menus and save them as objects that we can use in functions. Specifically we create a list obj which contains the vectors dv, iv, and mediator.\n \n# get data object\n  get_data&lt;-reactive({\n    if(!exists(input$dataset)) return() # if no upload\n    check&lt;-function(x){is.null(x) || x==\"\"}\n    if(check(input$dataset)) return()\n# retrieve the selected data and create objects and     \n    obj&lt;-list(data=get(input$dataset),\n              dv=input$dv,\n              iv=input$iv,\n              mediator=input$mediator\n    )\n    \n# require all to be set to proceed\n    if(any(sapply(obj,check))) return()\n# make sure choices had a chance to update\n    check&lt;-function(obj){\n      !all(c(obj$dv,obj$iv,obj$mediator) %in% colnames(obj$data))\n    }\n    if(check(obj)) return()\n# return 'obj' on completion     \n    obj\n  })\n  \n\n\nRunning the analyses\nNow that we can retrieve the selected data and variables, we can turn them into a dataframe and run our analyses on them.\n\nRegression\nThe code below creates an object output$fit which contains the output of the regression.\n  output$fit &lt;- renderPrint({\n# create an object 'data_list', which is a list that contains the selected data and variables\n    dataset_list &lt;- get_data()\n    \n# isloate the elements in the list as separate objects    \n    a &lt;- dataset_list$dv\n    b &lt;- dataset_list$iv\n    m &lt;- dataset_list$mediator\n    c &lt;- dataset_list$data\n   \n# create a dataframe 'df' from the object 'c' the selected dataset    \n    df &lt;- `colnames&lt;-`(\n      cbind.data.frame(\n# we extract and use the variables from 'c' that have the same names as those selected\n        c[which(colnames(c)==a)],\n        c[which(colnames(c)==b)],\n        c[which(colnames(c)==m)]\n      ), c(\"dv\",\"iv\",\"mediator\"))\n# now we have a dataframe df with 3 variables named 'dv', 'iv', and 'mediator'\n\n# we need to ensure data is numeric\n    df$dv &lt;- suppressWarnings(as.numeric(df$dv))\n    df$iv &lt;- suppressWarnings(as.numeric(df$iv))\n    df$mediator &lt;- suppressWarnings(as.numeric(df$mediator))\n    \n# using the same code previously discussed, we run the regression    \n    fit &lt;- lm(dv ~ iv + mediator, data=df)\n    summary(fit) # show results\n    \n  })\n\n\nMediation\nBelow we follow mostly the same steps to create our dataframe, and this time we run the mediation instead of the regression.\n  output$mediation &lt;- renderPrint({\n# create an object 'data_list', which is a list that contains the selected data and variables\n    dataset_list &lt;- get_data()\n    \n# isloate the elements in the list as separate objects    \n    a &lt;- dataset_list$dv\n    b &lt;- dataset_list$iv\n    m &lt;- dataset_list$mediator\n    c &lt;- dataset_list$data\n    \n# create a dataframe 'df' from the object 'c' the selected dataset    \n    df &lt;- `colnames&lt;-`(\n      cbind.data.frame(\n# we extract and use the variables from 'c' that have the same names as those selected\n        c[which(colnames(c)==a)],\n        c[which(colnames(c)==b)],\n        c[which(colnames(c)==m)]\n      ), c(\"dv\",\"iv\",\"mediator\"))\n# now we have a dataframe df with 3 variables named 'dv', 'iv', and 'mediator'\n    \n# we need to ensure data is numeric\n    df$dv &lt;- suppressWarnings(as.numeric(df$dv))\n    df$iv &lt;- suppressWarnings(as.numeric(df$iv))\n    df$mediator &lt;- suppressWarnings(as.numeric(df$mediator))\n\n# and we run the mediation using the same code as at the beginning of this post    \n    JS_model &lt;- mdt_simple(data = df,\n                           DV = dv,\n                           IV = iv,\n                           M  = mediator)\n    add_index(JS_model)\n  })"
  },
  {
    "objectID": "posts/2019-08-11-js_mediation/index.html#footnotes",
    "href": "posts/2019-08-11-js_mediation/index.html#footnotes",
    "title": "A Shiny App for JS Mediation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe purpose of this post is to demonstrate the code for these analyses, as such there may be issues with the analyses reported - I haven’t checked any assumptions or anything.↩︎\nIn order to enable people to use the app for their own analysis I needed a way for them to upload their data into the app. After a bit of googling I found this example, for uploading .csv files. I copied the code and modified it to include read.spss() from the package foreign instead of read.csv()↩︎"
  },
  {
    "objectID": "posts/2017-08-27-writing_functions_2/index.html",
    "href": "posts/2017-08-27-writing_functions_2/index.html",
    "title": "Writing functions - Part two",
    "section": "",
    "text": "(This post originally appeared on my R blog)\nThe current post will follow on from the previous post and describe another use for writing functions."
  },
  {
    "objectID": "posts/2017-08-27-writing_functions_2/index.html#r-markdown-and-reporting-p-values-in-apa-format",
    "href": "posts/2017-08-27-writing_functions_2/index.html#r-markdown-and-reporting-p-values-in-apa-format",
    "title": "Writing functions - Part two",
    "section": "R Markdown and reporting p values in APA format",
    "text": "R Markdown and reporting p values in APA format\nThe function described here is designed for use with R Markdown. I would write a post about how great R Markdown is, and how to use it, but there is already a wealth of information out there; see here, here, and here for a sample. This post relates to producing an APA formatted pdf using the papaja package (Aust 2017). Specifically, I describe a function that can be used to report p values correctly according to APA guidelines.\n\nThe problem\nOne of the great things about R Markdown is the “in-line code” option, whereby, instead of typing numbers, you can insert the code for the value you wish to report, and when the document is compiled, the correct number is reported.\nHowever, the reporting of a p value in APA format varies depending on what the p value actually is. It is consistently reported to three decimal places, with no “zero” preceding the decimal point. Values less than “.001” are reported as: “p &lt; .001.” For example, a p value of “.8368621” would be reported as “p = .837”; while a p value of “.0000725” would be reported as “p &lt; .001”.\nThe specific formatting requirements, and the variation in the reporting of the p value depending on the value being reported means that simply including in-line code to generate the p value is not always sufficient.\n\n\nThe solution\nIn order to remove the need tweak the formatting each time I report a new p value, I have created a function to do it for me.1\n\nThe p_report() function\nThe p_report() function takes any number less than 1, and reports it as an APA formatted p value. Let’s say you run a test, and save the p value from that test in the object p1, all you need to type in your R Markdown document then is\n*p* `r paste(p_report(p1))`\nThe p_report() function will remove the preceding zero, correctly identify whether “=” or “&lt;” is needed, and report p1 to three decimal places. Nesting it within paste() ensures that its output is included in the compiled pdf.\nAs in the previous post, the code for creating the function is below, and each line of code within the function is explained in the comment above (denoted with the # symbol). Again, this code can be copied and pasted into your R session to create the p_report() function.\n\np_report &lt;- function(x){\n\n      # create an object \"e\" which contains x, the p value you are reporting,\n      # rounded to 3 decimal places\n\n  e &lt;- round(x, digits = 3)\n\n      # the next two lines of code prints \"&lt; .001\" if x is indeed less than .001\n\n  if (x &lt; 0.001)\n    print(paste0(\"&lt;\", \" \", \".001\"))\n\n      # if x is greater than .001, the code below prints the object \"e\"\n      # with an \"=\" sign, and with the preceeding zero removed\n\n  else\n    print(\n      paste0(\"=\",\n                 \" \",\n                 sub(\"^(-?)0.\", \"\\\\1.\", sprintf(\"%.3f\",e))))\n\n}"
  },
  {
    "objectID": "posts/2017-08-27-writing_functions_2/index.html#usage",
    "href": "posts/2017-08-27-writing_functions_2/index.html#usage",
    "title": "Writing functions - Part two",
    "section": "Usage",
    "text": "Usage\nThe best way to illustrate the usage of p_report() is through examples. We will use the airquality dataset and compare the variation in temperature (Temp) and wind speed (Wind) depending on the month.\n\nPreparing the dataset\nFirst we need to load the dataset and make it (more) usable.\n\n      # create a dataframe df, containing the airquality dataset\n\ndf &lt;- airquality\n\n      # change the class of df$Month from \"integer\" to \"factor\"\n\ndf$Month &lt;- as.factor(df$Month)\n\n\n\nWind\nWe can test for differences in wind speed depending on Month. Run an anova and save the p value in an object b.\n\n    # create an object \"aov\" containing the summary of the anova\n\naov &lt;- summary(aov(Wind~Month, data = df))\n\n    # create an object \"b\" containing the p value of aov\n\nb &lt;- aov[[1]][[\"Pr(&gt;F)\"]][1]\n\nThe output of aovis:\n\n\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nMonth         4  164.3   41.07   3.529 0.00879 **\nResiduals   148 1722.3   11.64                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs you can see, the p value is 0.00879.\nIncluding b in-line returns 0.0087901, however if we pass b through p_report() by enclosing paste(p_report(b)) in r denoted back ticks. Typing the following in an R Markdown document:\n*p* `r paste(p_report(b))`\nreturns: p = .009.\n\n\nTemp\nSimilarly, we can test for differences in temperature depending on Month. By using the same names for the objects, we can use the same in-line code to report the p values.\n\n    # create an object \"aov\" containing the summary of the anova\n\naov &lt;- summary(aov(Temp~Month, data = df))\n\n    # create an object \"b\" containing the p value of aov\n\nb &lt;- aov[[1]][[\"Pr(&gt;F)\"]][1]\n\nThe output of aovis:\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nMonth         4   7061  1765.3   39.85 &lt;2e-16 ***\nResiduals   148   6557    44.3                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs you can see, the p value is &lt;2e-16.\nWhen this is run through p_report() using:\n*p* `r paste(p_report(b))`\nwhich will return: “p &lt; .001”.\n\n\nConclusion\nThe p_report() function is an example of using R to make your workflow easier. R Markdown replaces the need to type the numbers you report with the option of including in-line code to generate these numbers. p_report() means that you do not have to worry about formatting issues when these numbers are reported. Depending on how you structure your code chunks around your writing, and how name your objects, it may be possible to recycle sections of in-line code, speeding up the writing process. Furthermore, the principle behind p_report() can be applied to the writing of other functions (e.g., reporting F values or \\(\\chi\\)2).\n\n\nReferences"
  },
  {
    "objectID": "posts/2017-08-27-writing_functions_2/index.html#footnotes",
    "href": "posts/2017-08-27-writing_functions_2/index.html#footnotes",
    "title": "Writing functions - Part two",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe function described here, along with the descriptives() function described in the previous post, are part of a package I created called desnum (McHugh 2017). Writing functions as part of a package means that instead of writing the function anew for each session, you can just load the package. Follow up posts will probably describe more functions in the desnum package. If you wish to install the desnum package run the following code:\ndevtools::install_github(\"cillianmiltown/R_desnum\")\n↩︎"
  },
  {
    "objectID": "posts/2017-07-27-writing_functions_1/index.html",
    "href": "posts/2017-07-27-writing_functions_1/index.html",
    "title": "Writing functions - Part one",
    "section": "",
    "text": "(This post originally appeared on my R blog)"
  },
  {
    "objectID": "posts/2017-07-27-writing_functions_1/index.html#writing-functions",
    "href": "posts/2017-07-27-writing_functions_1/index.html#writing-functions",
    "title": "Writing functions - Part one",
    "section": "Writing functions",
    "text": "Writing functions\nThis post outlines the writing of a basic function. Writing functions in R (R Core Team 2021) is fairly simple, and the usefulness of function writing cannot be conveyed in a single post. I have included “Part one” in the title, and I will add follow-up posts in time.\nThe basic code to write a function looks like this:\nfunction_name &lt;- function(){}\nThe code for the task you want your function to perform goes inside the curly brackets {}, and the object you wish the function to work on goes inside the parenthesis().\n\nThe problem\nI have often found myself using a number of different functions together for multiple variables. For each variable, I need re-type each function. For example, when looking at a variable, I would often run the functions mean(), sd(), min(), max(), and length() together. Each time I wanted to inspect a new variable, I had to type all five functions for the variable in question. For example, looking at the Temp variable, from the airquality dataset in the datasets package, would require typing the following: mean(airquality$Temp), sd(airquality$Temp), min(airquality$Temp), max(airquality$Temp), length(airquality$Temp). This can get very tedious and repetitive.\n\n\nThe solution\nIn response to repeatedly typing these functions together, I created the descriptives() function which combines these frequently used functions into a single function.\n\nThe descriptives() function\nThe descriptives() function combines the functions mean(), sd(), min(), max(), and length() to return a table displaying the mean, standard deviation, minimum, maximum, and length of a vector.1 The code for creating this function is below, each line of code within the function is explained in the comment above (denoted with the # symbol). The code below can be copied and pasted into your R session to create the descriptives() function.\ndescriptives &lt;- function(x){\n\n      # create an object \"mean\" which contains the mean of x\n  mean &lt;- mean(x, na.rm = TRUE)\n\n      # create an object \"sd\" which contains the sd of x\n  sd &lt;- sd(x, na.rm = TRUE)\n\n      # create an object \"min\" which contains the min of x\n  min &lt;- min(x, na.rm = TRUE)\n\n      # create an object \"max\" which contains the max of x\n  max &lt;- max(x, na.rm = TRUE)\n\n      # create an object \"len\" which contains the length of x\n  len &lt;- length(x)\n\n      # combine the objects created into a table\n  data.frame(mean, sd, min, max, len)\n}\nWhen you pass a vector x through the function descriptives(), it creates 5 objects which are then combined into a table. Running the function returns the table:\ndescriptives(airquality$Temp)\n##       mean      sd min max len\n## 1 77.88235 9.46527  56  97 153\n\n\n\nThings to bear in mind when writing functions\n\nTry to give your function a name that is short and easy to remember.\nIf you are writing a longer more complex function, it may be useful to test it line-by-line, before seeing if it “works”; this will help to identify any errors before they cause your function to fail.\nIf the function returns an error, testing the code line by line will help you find the source of the error.\nThe final line of code in a function will be the “output” of the function.\nObjects created within the function are not saved in the global environment: in the descriptives() function, all that is returned is a table containing the variables specified. The individual objects that were created disappear when the function has finished running.\nThe disappearing of objects created within a function described above can be very useful for keeping a tidy working environment.\n\n\n\nConclusion\nI find myself writing functions regularly, for various tasks. Often a function may be specific to a particular task, or even to a particular dataset. One example of such a function builds on the previous post, in which I described how to create a dataframe from multiple files. In practice, I rarely create data frames exactly as described. I usually nest the “read.csv” function within a larger function that also sorts the data, creating a more manageable dataframe, better suited to my purposes; e.g., removing variables that are of no interest or computing/recoding variables. I can then run this function to build my dataframe at the start of a session.\n\n\nReferences"
  },
  {
    "objectID": "posts/2017-07-27-writing_functions_1/index.html#footnotes",
    "href": "posts/2017-07-27-writing_functions_1/index.html#footnotes",
    "title": "Writing functions - Part one",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMost of what descriptives() does can also be achieved by the summary() function, however sd() and length() are missing.↩︎"
  },
  {
    "objectID": "rblog/2017-07-27-writing_functions_1/index.html",
    "href": "rblog/2017-07-27-writing_functions_1/index.html",
    "title": "Writing functions - Part one",
    "section": "",
    "text": "(This post originally appeared on my R blog)"
  },
  {
    "objectID": "rblog/2017-07-27-writing_functions_1/index.html#writing-functions",
    "href": "rblog/2017-07-27-writing_functions_1/index.html#writing-functions",
    "title": "Writing functions - Part one",
    "section": "Writing functions",
    "text": "Writing functions\nThis post outlines the writing of a basic function. Writing functions in R (R Core Team 2021) is fairly simple, and the usefulness of function writing cannot be conveyed in a single post. I have included “Part one” in the title, and I will add follow-up posts in time.\nThe basic code to write a function looks like this:\nfunction_name &lt;- function(){}\nThe code for the task you want your function to perform goes inside the curly brackets {}, and the object you wish the function to work on goes inside the parenthesis().\n\nThe problem\nI have often found myself using a number of different functions together for multiple variables. For each variable, I need re-type each function. For example, when looking at a variable, I would often run the functions mean(), sd(), min(), max(), and length() together. Each time I wanted to inspect a new variable, I had to type all five functions for the variable in question. For example, looking at the Temp variable, from the airquality dataset in the datasets package, would require typing the following: mean(airquality$Temp), sd(airquality$Temp), min(airquality$Temp), max(airquality$Temp), length(airquality$Temp). This can get very tedious and repetitive.\n\n\nThe solution\nIn response to repeatedly typing these functions together, I created the descriptives() function which combines these frequently used functions into a single function.\n\nThe descriptives() function\nThe descriptives() function combines the functions mean(), sd(), min(), max(), and length() to return a table displaying the mean, standard deviation, minimum, maximum, and length of a vector.1 The code for creating this function is below, each line of code within the function is explained in the comment above (denoted with the # symbol). The code below can be copied and pasted into your R session to create the descriptives() function.\ndescriptives &lt;- function(x){\n\n      # create an object \"mean\" which contains the mean of x\n  mean &lt;- mean(x, na.rm = TRUE)\n\n      # create an object \"sd\" which contains the sd of x\n  sd &lt;- sd(x, na.rm = TRUE)\n\n      # create an object \"min\" which contains the min of x\n  min &lt;- min(x, na.rm = TRUE)\n\n      # create an object \"max\" which contains the max of x\n  max &lt;- max(x, na.rm = TRUE)\n\n      # create an object \"len\" which contains the length of x\n  len &lt;- length(x)\n\n      # combine the objects created into a table\n  data.frame(mean, sd, min, max, len)\n}\nWhen you pass a vector x through the function descriptives(), it creates 5 objects which are then combined into a table. Running the function returns the table:\ndescriptives(airquality$Temp)\n##       mean      sd min max len\n## 1 77.88235 9.46527  56  97 153\n\n\n\nThings to bear in mind when writing functions\n\nTry to give your function a name that is short and easy to remember.\nIf you are writing a longer more complex function, it may be useful to test it line-by-line, before seeing if it “works”; this will help to identify any errors before they cause your function to fail.\nIf the function returns an error, testing the code line by line will help you find the source of the error.\nThe final line of code in a function will be the “output” of the function.\nObjects created within the function are not saved in the global environment: in the descriptives() function, all that is returned is a table containing the variables specified. The individual objects that were created disappear when the function has finished running.\nThe disappearing of objects created within a function described above can be very useful for keeping a tidy working environment.\n\n\n\nConclusion\nI find myself writing functions regularly, for various tasks. Often a function may be specific to a particular task, or even to a particular dataset. One example of such a function builds on the previous post, in which I described how to create a dataframe from multiple files. In practice, I rarely create data frames exactly as described. I usually nest the “read.csv” function within a larger function that also sorts the data, creating a more manageable dataframe, better suited to my purposes; e.g., removing variables that are of no interest or computing/recoding variables. I can then run this function to build my dataframe at the start of a session.\n\n\nReferences"
  },
  {
    "objectID": "rblog/2017-07-27-writing_functions_1/index.html#footnotes",
    "href": "rblog/2017-07-27-writing_functions_1/index.html#footnotes",
    "title": "Writing functions - Part one",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMost of what descriptives() does can also be achieved by the summary() function, however sd() and length() are missing.↩︎"
  },
  {
    "objectID": "rblog/2017-08-27-writing_functions_2/index.html",
    "href": "rblog/2017-08-27-writing_functions_2/index.html",
    "title": "Writing functions - Part two",
    "section": "",
    "text": "(This post originally appeared on my R blog)\nThe current post will follow on from the previous post and describe another use for writing functions."
  },
  {
    "objectID": "rblog/2017-08-27-writing_functions_2/index.html#r-markdown-and-reporting-p-values-in-apa-format",
    "href": "rblog/2017-08-27-writing_functions_2/index.html#r-markdown-and-reporting-p-values-in-apa-format",
    "title": "Writing functions - Part two",
    "section": "R Markdown and reporting p values in APA format",
    "text": "R Markdown and reporting p values in APA format\nThe function described here is designed for use with R Markdown. I would write a post about how great R Markdown is, and how to use it, but there is already a wealth of information out there; see here, here, and here for a sample. This post relates to producing an APA formatted pdf using the papaja package (Aust 2017). Specifically, I describe a function that can be used to report p values correctly according to APA guidelines.\n\nThe problem\nOne of the great things about R Markdown is the “in-line code” option, whereby, instead of typing numbers, you can insert the code for the value you wish to report, and when the document is compiled, the correct number is reported.\nHowever, the reporting of a p value in APA format varies depending on what the p value actually is. It is consistently reported to three decimal places, with no “zero” preceding the decimal point. Values less than “.001” are reported as: “p &lt; .001.” For example, a p value of “.8368621” would be reported as “p = .837”; while a p value of “.0000725” would be reported as “p &lt; .001”.\nThe specific formatting requirements, and the variation in the reporting of the p value depending on the value being reported means that simply including in-line code to generate the p value is not always sufficient.\n\n\nThe solution\nIn order to remove the need tweak the formatting each time I report a new p value, I have created a function to do it for me.1\n\nThe p_report() function\nThe p_report() function takes any number less than 1, and reports it as an APA formatted p value. Let’s say you run a test, and save the p value from that test in the object p1, all you need to type in your R Markdown document then is\n*p* `r paste(p_report(p1))`\nThe p_report() function will remove the preceding zero, correctly identify whether “=” or “&lt;” is needed, and report p1 to three decimal places. Nesting it within paste() ensures that its output is included in the compiled pdf.\nAs in the previous post, the code for creating the function is below, and each line of code within the function is explained in the comment above (denoted with the # symbol). Again, this code can be copied and pasted into your R session to create the p_report() function.\n\np_report &lt;- function(x){\n\n      # create an object \"e\" which contains x, the p value you are reporting,\n      # rounded to 3 decimal places\n\n  e &lt;- round(x, digits = 3)\n\n      # the next two lines of code prints \"&lt; .001\" if x is indeed less than .001\n\n  if (x &lt; 0.001)\n    print(paste0(\"&lt;\", \" \", \".001\"))\n\n      # if x is greater than .001, the code below prints the object \"e\"\n      # with an \"=\" sign, and with the preceeding zero removed\n\n  else\n    print(\n      paste0(\"=\",\n                 \" \",\n                 sub(\"^(-?)0.\", \"\\\\1.\", sprintf(\"%.3f\",e))))\n\n}"
  },
  {
    "objectID": "rblog/2017-08-27-writing_functions_2/index.html#usage",
    "href": "rblog/2017-08-27-writing_functions_2/index.html#usage",
    "title": "Writing functions - Part two",
    "section": "Usage",
    "text": "Usage\nThe best way to illustrate the usage of p_report() is through examples. We will use the airquality dataset and compare the variation in temperature (Temp) and wind speed (Wind) depending on the month.\n\nPreparing the dataset\nFirst we need to load the dataset and make it (more) usable.\n\n      # create a dataframe df, containing the airquality dataset\n\ndf &lt;- airquality\n\n      # change the class of df$Month from \"integer\" to \"factor\"\n\ndf$Month &lt;- as.factor(df$Month)\n\n\n\nWind\nWe can test for differences in wind speed depending on Month. Run an anova and save the p value in an object b.\n\n    # create an object \"aov\" containing the summary of the anova\n\naov &lt;- summary(aov(Wind~Month, data = df))\n\n    # create an object \"b\" containing the p value of aov\n\nb &lt;- aov[[1]][[\"Pr(&gt;F)\"]][1]\n\nThe output of aovis:\n\n\n             Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nMonth         4  164.3   41.07   3.529 0.00879 **\nResiduals   148 1722.3   11.64                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs you can see, the p value is 0.00879.\nIncluding b in-line returns 0.0087901, however if we pass b through p_report() by enclosing paste(p_report(b)) in r denoted back ticks. Typing the following in an R Markdown document:\n*p* `r paste(p_report(b))`\nreturns: p = .009.\n\n\nTemp\nSimilarly, we can test for differences in temperature depending on Month. By using the same names for the objects, we can use the same in-line code to report the p values.\n\n    # create an object \"aov\" containing the summary of the anova\n\naov &lt;- summary(aov(Temp~Month, data = df))\n\n    # create an object \"b\" containing the p value of aov\n\nb &lt;- aov[[1]][[\"Pr(&gt;F)\"]][1]\n\nThe output of aovis:\n\n\n             Df Sum Sq Mean Sq F value Pr(&gt;F)    \nMonth         4   7061  1765.3   39.85 &lt;2e-16 ***\nResiduals   148   6557    44.3                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAs you can see, the p value is &lt;2e-16.\nWhen this is run through p_report() using:\n*p* `r paste(p_report(b))`\nwhich will return: “p &lt; .001”.\n\n\nConclusion\nThe p_report() function is an example of using R to make your workflow easier. R Markdown replaces the need to type the numbers you report with the option of including in-line code to generate these numbers. p_report() means that you do not have to worry about formatting issues when these numbers are reported. Depending on how you structure your code chunks around your writing, and how name your objects, it may be possible to recycle sections of in-line code, speeding up the writing process. Furthermore, the principle behind p_report() can be applied to the writing of other functions (e.g., reporting F values or \\(\\chi\\)2).\n\n\nReferences"
  },
  {
    "objectID": "rblog/2017-08-27-writing_functions_2/index.html#footnotes",
    "href": "rblog/2017-08-27-writing_functions_2/index.html#footnotes",
    "title": "Writing functions - Part two",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe function described here, along with the descriptives() function described in the previous post, are part of a package I created called desnum (McHugh 2017). Writing functions as part of a package means that instead of writing the function anew for each session, you can just load the package. Follow up posts will probably describe more functions in the desnum package. If you wish to install the desnum package run the following code:\ndevtools::install_github(\"cillianmiltown/R_desnum\")\n↩︎"
  },
  {
    "objectID": "rblog/2018-10-20-a_lazy_function/index.html",
    "href": "rblog/2018-10-20-a_lazy_function/index.html",
    "title": "A Lazy Function",
    "section": "",
    "text": "It has been quite a while since I posted, but I haven’t been idle, I completed my PhD since the last post, and I’m due to graduate next Thursday. I am also delighted to have recently been added to R-bloggers.com so I’m keen to get back into it."
  },
  {
    "objectID": "rblog/2018-10-20-a_lazy_function/index.html#a-lazy-function",
    "href": "rblog/2018-10-20-a_lazy_function/index.html#a-lazy-function",
    "title": "A Lazy Function",
    "section": "A Lazy Function",
    "text": "A Lazy Function\nI have already written 2 posts about writing functions, and I will try to diversify my content. That said, I won’t refrain from sharing something that has been helpful to me. The function(s) I describe in this post is an artefact left over from before I started using R Markdown. It is a product of its time but may still be of use to people who haven’t switched to R Markdown yet. It is lazy (and quite imperfect) solution to a tedious task.\n\nThe Problem\nAt the time I wrote this function I was using R for my statistics and Libreoffice for writing. I would run a test in R and then write it up in Libreoffice. Each value that needed reporting had to be transferred from my R output to Libreoffice - and for each test there are a number of values that need reporting. Writing up these tests is pretty formulaic. There’s a set structure to the sentence, for example writing up a t-test with a significant result nearly always looks something like this:\n\nAn independent samples t-test revealed a significant difference in X between the Y sample, (M = [ ], SD = [ ]), and the Z sample, (M = [ ], SD = [ ]), t([df]) = [ ], p = [ ].\n\nAnd the write up of a non-significant result looks something like this:\n\nAn independent samples t-test revealed no significant difference in X between the Y sample, (M = [ ], SD = [ ]), and the Z sample, (M = [ ], SD = [ ]), t([df]) = [ ], p = [ ].\n\nSeven values (the square [ ] brackets) need to be reported for this single test. Whether you copy and paste or type each value, the reporting of such tests can be very tedious, and leave you prone to errors in reporting.\n\n\nThe Solution\nIn order to make reporting values easier (and more accurate) I wrote the t_paragraph() function (and the related t_paired_paragraph() function). This provided an output that I could copy and paste into a Word (Libreoffice) document. This function is part of the desnum1 package (McHugh 2017).\n\nThe t_parapgraph() Function\nThe t_parapgraph() function runs a t-test and generates an output that can be copied and pasted into a word document. The code for the function is as follows:\n\n# Create the function t_paragraph with arguments x, y, and measure\n# x is the dependent variable\n# y is the independent (grouping) variable\n# measure is the name of dependent variable inputted as string\n\nt_paragraph &lt;- function (x, y, measure){\n  \n  # Run a t-test and store it as an object t\n  \n  t &lt;- t.test(x ~ y)\n  \n  \n  # If your grouping variable has labelled levels, the next line will store them for reporting at a later stage\n  \n  labels &lt;- levels(y)\n  \n  # Create an object for each value to be reported\n  \n  tsl &lt;- as.vector(t$statistic)\n  ts &lt;- round(tsl, digits = 3)\n  tpl &lt;- as.vector(t$p.value)\n  tp &lt;- round(tpl, digits = 3)\n  d_fl &lt;- as.vector(t$parameter)\n  d_f &lt;- round(d_fl, digits = 2)\n  ml &lt;- as.vector(tapply(x, y, mean))\n  m &lt;- round(ml, digits = 2)\n  sdl &lt;- as.vector(tapply(x, y, sd))\n  sd &lt;- round(sdl, digits = 2)\n  \n  # Use print(paste0()) to combine the objects above and create two potential outputs\n  # The output that is generated will depend on the result of the test\n  \n  \n  # wording if significant difference is observed\n  \n  if (tp &lt; 0.05) \n    print(paste0(\"An independent samples t-test revealed a significant difference in \", \n                 measure, \" between the \", labels[1], \" sample, (M = \", \n                 m[1], \", SD = \", sd[1], \"), and the \", labels[2], \n                 \" sample, (M =\", m[2], \", SD =\", sd[2], \"), t(\", \n                 d_f, \") = \", ts, \", p = \", tp, \".\"), quote = FALSE, \n          digits = 2)\n  \n  # wording if no significant difference is observed      \n  \n  if (tp &gt; 0.05) \n    print(paste0(\"An independent samples t-test revealed no difference in \", \n                 measure, \" between the \", labels[1], \" sample, (M = \", \n                 m[1], \", SD = \", sd[1], \"), and the \", labels[2], \n                 \" sample, (M = \", m[2], \", SD =\", sd[2], \"), t(\", \n                 d_f, \") = \", ts, \", p = \", tp, \".\"), quote = FALSE, \n          digits = 2)\n}\n\nWhen using t_paragraph(), x is your DV, y is your grouping variable while measure is a string value that the name of the dependent variable. To illustrate the function I’ll use the mtcars dataset.\n\n\nApplications of the t_parapgraph() Function\nThe mtcars dataset is comes with R. For information on it simply type help(mtcars). The variables of interest here are am (transmission; 0 = automatic, 1 = manual), mpg (miles per gallon), qsec (1/4 mile time). The two questions I’m going to look at are:\n\nIs there a difference in miles per gallon depending on transmission?\nIs there a difference in 1/4 mile time depending on transmission?\n\nBefore running the test it is a good idea to look at the data2. Because we’re going to look at differences between groups we want to run descriptives for each group separately. To do this I’m going to combine the the descriptives() function which I previously covered here (also part of the desnum package) and the tapply() function.\nThe tapply() function allows you to run a function on subsets of a dataset using a grouping variable (or index). The arguments are as follows tapply(vector, index, function). vector is the variable you want to pass through function; and index is the grouping variable. The examples below will make this clearer.\nWe want to run descriptives on mtcars$mpg and on mtcars$qsec and for each we want to group by transmission (mtcars$am). This can be done using tapply() and descriptives() together as follows:\n\ntapply(mtcars$mpg, mtcars$am, descriptives)\n\n$`0`\n      mean       sd  min  max len\n1 17.14737 3.833966 10.4 24.4  19\n\n$`1`\n      mean       sd min  max len\n1 24.39231 6.166504  15 33.9  13\n\n\nRecall that 0 = automatic, and 1 = manual. Replace mpg with qsec and run again:\n\ntapply(mtcars$qsec, mtcars$am, descriptives)\n\n$`0`\n      mean       sd   min  max len\n1 18.18316 1.751308 15.41 22.9  19\n\n$`1`\n   mean       sd  min  max len\n1 17.36 1.792359 14.5 19.9  13\n\n\n\n\n\nRunning t_paragraph()\nNow that we know the values for automatic vs manual cars we can run our t-tests using t_paragraph(). Our first question:\n\nIs there a difference in miles per gallon depeding on transmission?\n\n\nt_paragraph(mtcars$mpg, mtcars$am, \"miles per gallon\")\n\n[1] An independent samples t-test revealed a significant difference in miles per gallon between the  sample, (M = 17.15, SD = 3.83), and the  sample, (M =24.39, SD =6.17), t(18.33) = -3.767, p = 0.001.\n\n\nThere is a difference, and the output above can be copied and pasted into a word document with minimal changes required.\nOur second question was:\n\nIs there a difference in 1/4 mile time depending on transmission?\n\n\nt_paragraph(mtcars$qsec, mtcars$am, \"quarter-mile time\")\n\n[1] An independent samples t-test revealed no difference in quarter-mile time between the  sample, (M = 18.18, SD = 1.75), and the  sample, (M = 17.36, SD =1.79), t(25.53) = 1.288, p = 0.209.\n\n\nThis time there was no significant difference, and again the output can be copied and pasted into word with minimal changes.\n\n\nLimitations\nThe function described was written a long time ago, and could be updated. However I no longer copy and paste into word (having switched to R markdown instead). The reporting of the p value is not always to APA standards. If p is &lt; .001 this is what should be reported. The code for t_paragraph() could be updated to include the p_report function (described here) which would address this. Another limitation is that the formatting of the text isn’t perfect, the letters (N,M,SD,t,p) should all be italicised, but having to manually fix this formatting is still easier than manually transferring individual values.\n\n\nConclusion\nDespite the limitations the functions t_paragraph() and t_paired_paragraph()3 have made my life easier. I still use them occasionally. I hope they can be of use to anyone who is using R but has not switched to R Markdown yet."
  },
  {
    "objectID": "rblog/2018-10-20-a_lazy_function/index.html#footnotes",
    "href": "rblog/2018-10-20-a_lazy_function/index.html#footnotes",
    "title": "A Lazy Function",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo install desnum just run devtools::install_github(\"cillianmiltown/R_desnum\")↩︎\nIn this case this is particularly useful because there are no value labels for mtcars$am, so it won’t be clear from the output which values refer to the automatic group and which refer to the manual group. Running descriptives will help with this.↩︎\nIf you want to see the code for t_paired_paragraph() just load desnum and run t_paired_paragraph (without parenthesis)↩︎"
  },
  {
    "objectID": "rblog/2019-08-03-rmarkdown_workshop/index.html",
    "href": "rblog/2019-08-03-rmarkdown_workshop/index.html",
    "title": "R Markdown Workshop",
    "section": "",
    "text": "This is an unusual post for me, I have avoided writing about R Markdown because there are so many resources already available on the topic (e.g., here, here, and here). However, recently I ran a session on using RMarkdown for my colleagues in the Centre for Social Issues Research. The aim of this was to demonstrate the usefulness of R Markdown (and hopefully convert a few people). For this session I created a set of resources1 aimed at making the transition from SPSS to R Markdown a bit easier. The statistics content of these resources is mainly just some of the simpler standard tests taught to psychology undergraduate students.\nThe complete resources are available on this project page on the OSF. The main purpose of the exercise was to provide people with the tools to create this pdf using this R Markdown template. My hope is that by using this template, SPSS users might make the tranistion to R, and R Markdown (with the help of the wonderful papaja package Aust (2017))."
  },
  {
    "objectID": "rblog/2019-08-03-rmarkdown_workshop/index.html#working-with-dataframes",
    "href": "rblog/2019-08-03-rmarkdown_workshop/index.html#working-with-dataframes",
    "title": "R Markdown Workshop",
    "section": "Working with dataframes",
    "text": "Working with dataframes\nA dataframe is structured much like an SPSS file. There are rows and columns, the columns are named and generally represent variables. The rows (can also be named) generally represent cases. You can have multiple data frames loaded with different names, although they are commonly saved as df (and these can be numbered df1 df2 df3. If your document/code is well organised, it can be useful have a generic name for dataframes that you are working with. This means that much of your code can be recycled (particularly if the variable names are the same - if you run repeated studies, or studies with only minor changes, you will find that there is massive scope for recycling code - both chunks and in-line)\n\nSome basics:\n\nThe entire dataframe can be printed to the console by running the name of the data frame\nThe dollar sign can be used to call specific variables from the data frame i.e., df$variable_name\nA function has the form “function name” followed by parenthesis: function_name().\nThe object that you want to run the function on goes in the parenthesis. e.g., if our dataframe was called df, and age was called age and we wanted to get the mean age we would run mean(df$age).\nSometimes missing data denoted by NA can mess with some functions, to account for this it is helpful to include the argument na.rm = TRUE in the function, e.g., mean(df$age, na.rm = TRUE)\n\nThe mtcars dataset comes with R. For information on it simply type help(mtcars). The variables of interest here are am (transmission; 0 = automatic, 1 = manual), mpg (miles per gallon), qsec (1/4 mile time). Below we practice a few simple functions to find out information about the dataset.\n\nExample code and output:\n\nLoad the mtcars dataset into an object called df using the command df &lt;- mtcars\nView the variable names associated with df by running variable.names(df)\n\n\n\n [1] \"mpg\"  \"cyl\"  \"disp\" \"hp\"   \"drat\" \"wt\"   \"qsec\" \"vs\"   \"am\"   \"gear\"\n[11] \"carb\"\n\n\n\nThe mean miles per gallon can be calculated using mean(df$mpg)\n\n\n\n[1] 20.09062\n\n\n\nThe standard deviation of the same variable is calculated using sd(df$mpg)\n\n\n\n[1] 6.026948\n\n\n\nOr if you want to see basic descriptives use descriptives(df$mpg)3\n\n\n\n      mean       sd  min  max len\n1 20.09062 6.026948 10.4 33.9  32\n\n\n\nTo index by a variable we use square brackets [] and the which() function.\n\nThe following command gets the mean miles per gallon for all cars with manual transmission:\n\nmean(df$mpg[which(df$am==1)])\n\n\n\n\n\n[1] 24.39231"
  },
  {
    "objectID": "rblog/2019-08-03-rmarkdown_workshop/index.html#t-test-transmission-and-mpg",
    "href": "rblog/2019-08-03-rmarkdown_workshop/index.html#t-test-transmission-and-mpg",
    "title": "R Markdown Workshop",
    "section": "T-test: Transmission and MPG",
    "text": "T-test: Transmission and MPG\n\nLoad mtcars and save it in your environment using df &lt;- mtcars\nCreate a new dataframe with a generic name e.g., x using the command: x &lt;- df\nThis command runs the t-test and you can see the output in the console t.test(x$mpg~x$am)\n\n\n\n\n    Welch Two Sample t-test\n\ndata:  x$mpg by x$am\nt = -3.7671, df = 18.332, p-value = 0.001374\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -11.280194  -3.209684\nsample estimates:\nmean in group 0 mean in group 1 \n       17.14737        24.39231 \n\n\n\nThe following code runs the t-test but saves the output as a list t that can be called later: t &lt;- t.test(x$mpg~x$am)\n\n\nAs with dataframes, specific variables within a list can be called using the dollar sign\nTo call the p value simply type t$p.value\n\n\n\n[1] 0.001373638\n\n\n\nTo call the t statistic, type t$statistic\n\n\n\n        t \n-3.767123 \n\n\n\nAnd to call the degrees of freedom, type t$parameter\n\n\n\n      df \n18.33225 \n\n\n\nFinally, to calculate the effect size and save it to an object type td &lt;- cohensD(mpg~am, data=x)\n\nFrom the above we can call each value we need using in-line code to write up our results section as follows\n\nThis is what the paragraph will look like in your Rmd document:\nAn independent samples t-test revealed a significant difference in miles per gallon between cars with automatic transmission (*M* = `r mean(x$mpg[which(x$am==0)])`, *SD* = `r sd(x$mpg[which(x$am==0)])`), and cars with manual transmission, (*M* = `r mean(x$mpg[which(x$am==1)])`, *SD* = `r sd(x$mpg[which(x$am==1)])`), *t*(`r t$parameter`) = `r t$statistic`, *p* `r paste(p_report(t$p.value))`, *d* = `r td.\n\n\nThe above syntax will return the following:\nAn independent samples t-test revealed a significant difference in miles per gallon between cars with automatic transmission (M = 17.15, SD = 3.83), and cars with manual transmission, (M = 24.39, SD = 3.83), t(18.33) = -3.767, p = .001, d = 1.48.\nIf you want to run another t-test later on in your document you simply run it in a code chunk and create new objects (t and td) with the same names as before and you can use the same write up as above to report it."
  },
  {
    "objectID": "rblog/2019-08-03-rmarkdown_workshop/index.html#footnotes",
    "href": "rblog/2019-08-03-rmarkdown_workshop/index.html#footnotes",
    "title": "R Markdown Workshop",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe aim of this post is to help make these resources more accessible. As such, there will likely be a lot of duplication between this post and the resources on the OSF.↩︎\nItalics are achieved by placing a star either side of the text you want italicised *italics* = italics; Bold is achieved by placing a double star either side of the text you want italicised **bold** = bold↩︎\nfrom the desnum package↩︎\nThis test, and all tests that follow are for illustration purposes only, I have not checked any assumptions to see if I can run the tests, I just want to provide sample code that you can use for your own analyses.↩︎"
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html",
    "href": "rblog/2019-08-11-js_mediation/index.html",
    "title": "A Shiny App for JS Mediation",
    "section": "",
    "text": "This is a brief post about making my first Shiny App (see also). I made this app following a meeting of the Advancing Social Cognition lab (ASC-Lab) where we discussed this paper by Yzerbyt et al. (2018) proposing a new method for mediation analysis. Any attempt to detail the differences in methods is well beyond the scope of a blog post. The take home message is that the method proposed by Yzerbyt et al. (2018) is less prone to Type I errors (or false positives) than the most commonly used methods (e.g., Hayes 2017). In addition to identifying a problem and proposing a solution, the authors also provide the tools to implement their solution with an R package (Batailler et al. 2019). Unfortunately, not everyone uses R, and this is why I set about developing a simple way for SPSS users to access this new method."
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#set-up-the-dataframe",
    "href": "rblog/2019-08-11-js_mediation/index.html#set-up-the-dataframe",
    "title": "A Shiny App for JS Mediation",
    "section": "Set up the dataframe",
    "text": "Set up the dataframe\nFor ease of reusing code (particularly later on) I’ll save mtcars as a dataframe df and rename the variables of interest as iv (predictor variable), dv (outcome variable), and mediator.\n\ndf &lt;- mtcars          # create df from mtcars\n\n# create new variables with generic names\ndf$dv &lt;- df$qsec      # save 1/4 mile time as dv\ndf$iv &lt;- df$hp        # save horsepower as iv\ndf$mediator &lt;- df$wt  # save weight as mediator"
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#simple-regression",
    "href": "rblog/2019-08-11-js_mediation/index.html#simple-regression",
    "title": "A Shiny App for JS Mediation",
    "section": "Simple Regression",
    "text": "Simple Regression\nBefore running the mediation I’ll run a quick regression to assess the nature of the relationship between the variables.\n\nfit &lt;- lm(dv ~ iv + mediator, data=df)  # save the regression in an object 'fit'\nsummary(fit)                            # show the results\n\n\nCall:\nlm(formula = dv ~ iv + mediator, data = df)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8283 -0.4055 -0.1464  0.3519  3.7030 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 18.825585   0.671867  28.020  &lt; 2e-16 ***\niv          -0.027310   0.003795  -7.197 6.36e-08 ***\nmediator     0.941532   0.265897   3.541  0.00137 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.09 on 29 degrees of freedom\nMultiple R-squared:  0.652, Adjusted R-squared:  0.628 \nF-statistic: 27.17 on 2 and 29 DF,  p-value: 2.251e-07\n\n\nAs you can see from the output, 1/4 mile time is predicted by both horsepower and by weight."
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#simple-mediation",
    "href": "rblog/2019-08-11-js_mediation/index.html#simple-mediation",
    "title": "A Shiny App for JS Mediation",
    "section": "Simple Mediation",
    "text": "Simple Mediation\nNow that we have a picture of the relationships between the variables we can run the mediation analysis. The code for this is detailed below.\n\nJS_model &lt;- mdt_simple(data = df, # create an object 'JS_model'\n                       DV = dv,\n                       IV = iv,\n                       M  = mediator)\nadd_index(JS_model)               # display the results of the mediation\n\nTest of mediation (simple mediation)\n==============================================\n\nVariables:\n\n- IV: iv \n- DV: dv \n- M: mediator \n\nPaths:\n\n====  ==============  =====  ======================\nPath  Point estimate     SE  APA                   \n====  ==============  =====  ======================\na              0.009  0.002  t(30) = 4.80, p &lt; .001\nb              0.942  0.266  t(29) = 3.54, p = .001\nc             -0.018  0.003  t(30) = 5.49, p &lt; .001\nc'            -0.027  0.004  t(29) = 7.20, p &lt; .001\n====  ==============  =====  ======================\n\nIndirect effect index:\n\n- type: Indirect effect \n- point estimate: 0.00885 \n- confidence interval:\n  - method: Monte Carlo (5000 iterations)\n  - level: 0.05 \n  - CI: [0.00351; 0.0157]\n\nFitted models:\n\n- X -&gt; Y \n- X -&gt; M \n- X + M -&gt; Y \n\n\n\nHere we can see that horsepower predicts both 1/4 mile time and weight.\nThere is also an indirect effect of horsepower on 1/4 mile time through weight."
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#the-geography-of-the-shiny-app",
    "href": "rblog/2019-08-11-js_mediation/index.html#the-geography-of-the-shiny-app",
    "title": "A Shiny App for JS Mediation",
    "section": "The Geography of the Shiny App",
    "text": "The Geography of the Shiny App\nThe Shiny App has two panels.\n\nOn the left we have:\n\nThe data upload option\nA dropdown menu for selecting the data you wish to use (the uploaded file, the mtcars data set, or the iris data set)\nDropdown menus for defining each of your variables,\nText describing the App\n\nOn the right we have:\n\nThe output of the regression\nThe output from the mediation analysis\n\n\nThe code for generating these panels is below (comments above relevant lines describe the purpose of the various sections):\n# UI for app\nui&lt;-(pageWithSidebar(\n\n# We use headerPanel() to give a title to our app \n  headerPanel(\"JS Mediation\"),\n  \n# use sidebarPanel() to create the content of the side panel (panel on the left)\n  sidebarPanel\n  (\n# use fileInput() to create a dialogue for inputting a file\n    fileInput(\"file1\", \"Upload SPSS File\",\n              multiple = TRUE,\n              accept = c(\".sav\")),\n# create a horizontal line break\n    tags$hr(),\n    \n# create a dropdown menu for selecting the dataset to be used\n    selectInput(\"dataset\",\"Data:\",\n                choices =list(iris = \"iris\",\n                              mtcars = \"mtcars\",\n                              uploaded_file = \"inFile\"), selected=NULL),\n# create a dropdown menu for selecting the dependent variable to be used\n    selectInput(\"dv\",\"Dependent Variable:\", choices = NULL),\n# create a dropdown menu for selecting the Independent variable to be used\n    selectInput(\"iv\",\"Independent Variable:\", choices = NULL),\n# create a dropdown menu for selecting the mediator to be used\n    selectInput(\"mediator\",\"Mediator:\", choices = NULL) #,\n    \n# use HTML() to input formatted text describing the App\n    ,HTML('In response to \n    &lt;a href=\"https://perso.uclouvain.be/vincent.yzerbyt/Yzerbyt%20et%20al.%20JPSP%202018.pdf\"&gt;this&lt;/a&gt;\n    paper by Yzerbyt, Batailler and Judd (2018) which outined a new method of conducting mediation analyses\n    (with less susceptability to false positives than Hayes’ PROCESS) I created a ShinyApp so that their\n    R-package could be used by SPSS users. Upload your SPSS file above and select the variables you wish\n    to compare.')\n    ,br(),br(),br()\n    ,HTML('&lt;p&gt;Yzerbyt, V., Muller, D., Batailler, C., &amp; Judd, C. M. (2018). New Recommendations for\n    Testing Indirect  Effects in Mediational Models: The Need to Report and Test Component Paths.\n    &lt;em&gt;Journal of Personality and Social Psychology: Attitudes and Social Cognition&lt;/em&gt;, 115(6), \n    929–943. &lt;a href=\"http://dx.doi.org/10.1037/pspa0000132\"\n    class=\"uri\"&gt;http://dx.doi.org/10.1037/pspa0000132&lt;/a&gt;&lt;/p&gt;')\n  ),\n  \n# use mainPanel() to create the panel on the right where the output of our tests will be\n  mainPanel(\n# give a title to the the first output\n    h3(\"Summary of Regression Model\"),\n# report the result of the regression, saved in the object 'fit'\n    verbatimTextOutput(\"fit\"),\n# give a title for the second output\n    h3(\"Mediation Results\"),\n# report the result of the mediation, saved in the object 'mediation'\n    verbatimTextOutput(\"mediation\")\n  )\n))"
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#the-backend-of-the-shiny-app",
    "href": "rblog/2019-08-11-js_mediation/index.html#the-backend-of-the-shiny-app",
    "title": "A Shiny App for JS Mediation",
    "section": "The Backend of the Shiny App",
    "text": "The Backend of the Shiny App\nAbove we have the code for setting up and modifying the look and feel of our app. Below we go through the code for making the app do what it is supposed to do. The code in full is at the bottom of this post, however I have isolated specific sections of code to describe their function.\n\nInputting data from file\nThe code below runs read.spss() on whatever file you have uploaded using the dialogue box in the side panel and creates a dataframe called inFile.\n upload_data&lt;-reactive({\n    inFile &lt;- input$file1\n    if (is.null(inFile))\n      return(NULL)\n    read.spss(input$file1$datapath, to.data.frame = TRUE)\n  })\n  \n  observeEvent(input$file1,{\n    inFile&lt;&lt;-upload_data()\n  })\n\n\n\nSelecting data and variables\nThe code below retrieves information about the dataset that is selected, and displays the variables associated with the selected dataset in the dropdown menus for each of your variables (IV, DV, & mediator).\n# update variables based on the data\n  observe({\n# make sure upload exists\n    if(!exists(input$dataset)) return() \n# retrieve names of columns (variable names) and save as 'var.opts'\n    var.opts&lt;-colnames(get(input$dataset))\n# set var.opts as the options for the drop down menus\n    updateSelectInput(session, \"dv\", choices = var.opts)\n    updateSelectInput(session, \"iv\", choices = var.opts)\n    updateSelectInput(session, \"mediator\", choices = var.opts)\n  })\n\n\nSetting up data for analysis\nBelow we extract the data and variables selected in the dropdown menus and save them as objects that we can use in functions. Specifically we create a list obj which contains the vectors dv, iv, and mediator.\n \n# get data object\n  get_data&lt;-reactive({\n    if(!exists(input$dataset)) return() # if no upload\n    check&lt;-function(x){is.null(x) || x==\"\"}\n    if(check(input$dataset)) return()\n# retrieve the selected data and create objects and     \n    obj&lt;-list(data=get(input$dataset),\n              dv=input$dv,\n              iv=input$iv,\n              mediator=input$mediator\n    )\n    \n# require all to be set to proceed\n    if(any(sapply(obj,check))) return()\n# make sure choices had a chance to update\n    check&lt;-function(obj){\n      !all(c(obj$dv,obj$iv,obj$mediator) %in% colnames(obj$data))\n    }\n    if(check(obj)) return()\n# return 'obj' on completion     \n    obj\n  })\n  \n\n\nRunning the analyses\nNow that we can retrieve the selected data and variables, we can turn them into a dataframe and run our analyses on them.\n\nRegression\nThe code below creates an object output$fit which contains the output of the regression.\n  output$fit &lt;- renderPrint({\n# create an object 'data_list', which is a list that contains the selected data and variables\n    dataset_list &lt;- get_data()\n    \n# isloate the elements in the list as separate objects    \n    a &lt;- dataset_list$dv\n    b &lt;- dataset_list$iv\n    m &lt;- dataset_list$mediator\n    c &lt;- dataset_list$data\n   \n# create a dataframe 'df' from the object 'c' the selected dataset    \n    df &lt;- `colnames&lt;-`(\n      cbind.data.frame(\n# we extract and use the variables from 'c' that have the same names as those selected\n        c[which(colnames(c)==a)],\n        c[which(colnames(c)==b)],\n        c[which(colnames(c)==m)]\n      ), c(\"dv\",\"iv\",\"mediator\"))\n# now we have a dataframe df with 3 variables named 'dv', 'iv', and 'mediator'\n\n# we need to ensure data is numeric\n    df$dv &lt;- suppressWarnings(as.numeric(df$dv))\n    df$iv &lt;- suppressWarnings(as.numeric(df$iv))\n    df$mediator &lt;- suppressWarnings(as.numeric(df$mediator))\n    \n# using the same code previously discussed, we run the regression    \n    fit &lt;- lm(dv ~ iv + mediator, data=df)\n    summary(fit) # show results\n    \n  })\n\n\nMediation\nBelow we follow mostly the same steps to create our dataframe, and this time we run the mediation instead of the regression.\n  output$mediation &lt;- renderPrint({\n# create an object 'data_list', which is a list that contains the selected data and variables\n    dataset_list &lt;- get_data()\n    \n# isloate the elements in the list as separate objects    \n    a &lt;- dataset_list$dv\n    b &lt;- dataset_list$iv\n    m &lt;- dataset_list$mediator\n    c &lt;- dataset_list$data\n    \n# create a dataframe 'df' from the object 'c' the selected dataset    \n    df &lt;- `colnames&lt;-`(\n      cbind.data.frame(\n# we extract and use the variables from 'c' that have the same names as those selected\n        c[which(colnames(c)==a)],\n        c[which(colnames(c)==b)],\n        c[which(colnames(c)==m)]\n      ), c(\"dv\",\"iv\",\"mediator\"))\n# now we have a dataframe df with 3 variables named 'dv', 'iv', and 'mediator'\n    \n# we need to ensure data is numeric\n    df$dv &lt;- suppressWarnings(as.numeric(df$dv))\n    df$iv &lt;- suppressWarnings(as.numeric(df$iv))\n    df$mediator &lt;- suppressWarnings(as.numeric(df$mediator))\n\n# and we run the mediation using the same code as at the beginning of this post    \n    JS_model &lt;- mdt_simple(data = df,\n                           DV = dv,\n                           IV = iv,\n                           M  = mediator)\n    add_index(JS_model)\n  })"
  },
  {
    "objectID": "rblog/2019-08-11-js_mediation/index.html#footnotes",
    "href": "rblog/2019-08-11-js_mediation/index.html#footnotes",
    "title": "A Shiny App for JS Mediation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe purpose of this post is to demonstrate the code for these analyses, as such there may be issues with the analyses reported - I haven’t checked any assumptions or anything.↩︎\nIn order to enable people to use the app for their own analysis I needed a way for them to upload their data into the app. After a bit of googling I found this example, for uploading .csv files. I copied the code and modified it to include read.spss() from the package foreign instead of read.csv()↩︎"
  },
  {
    "objectID": "posts/mjac/index.html",
    "href": "posts/mjac/index.html",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "",
    "text": "In what follows I will attempt to provide an overview of my recently published theoretical work, covering the main ideas behind the theory, while also describing the process of its development, from initial conception to final publication.\n(presentation to IMP seminar series 31st May 2021)\n(Poster presentation to SPSP 2022)"
  },
  {
    "objectID": "posts/mjac/index.html#applying-mjac",
    "href": "posts/mjac/index.html#applying-mjac",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "Applying MJAC",
    "text": "Applying MJAC\nThis approach to understanding moral judgements provides a novel perspective on how we understand particular moral phenomena.\n\nMoral Dumbfounding\nMoral dumbfounding occurs when people defend a moral judgment even though they cannot provide a reason to support it (Haidt, Björklund, and Murphy 2000; Haidt 2001; McHugh et al. 2017, 2020). Moral dumbfounding is most frequently obeserved for harmless taboo behaviors (consensual incest, cannibalism involving a body that is already dead). The taboo nature of these topics means that they are consistently identified as morally wrong without much discussion [the Scottish public petitions committee notably dismissed a call to legalize incest with no discussion at all; see Sim (2016)]. This leads to a high degree of stability in categorizing them as wrong. However, the taboo nature of these behaviors prevents them from being discussed. This means that a typical encounter with such behavior involves little more than identifying it as wrong, possibly with an expression of disgust, and changing the subject. Because of this combination of stability and This process logically leads to moral dumbfounding.\n\n\nCategorizing people versus categorizing actions\nMJAC predicts people’s judgements will focus on the actor or on the action depending on the situation. Consider the following two scenarios:\n\n\nYou find out that a colleague has been fired for stealing from your employer—they have been bringing home office equipment for their own personal use, and they have been exaggerating their expense claims.\n\n\n\n\nA close friend of yours reveals to you that they have been stealing from their employer—they have been bringing home office equipment for their own personal use, and they have been exaggerating their expense claims.\n\n\nMJAC predicts that people will be more lenient in their judgments of the person in the second scenario than the in first scenario. Indeed this is consistent with what is found in the literature (Heiphetz and Craig 2020; Forbes 2018; Lee and Holyoak 2020; Hofmann et al. 2014; Weidman et al. 2020).\nA further prediction is that for the second scenario, people will focus on the action rather than the actor. People are motivated to see close others positively (Forbes 2018; Murray, Holmes, and Griffin 1996a, 1996b). If faced with a situation in which a close other did something wrong, people would try to avoid making a negative judgment of the person (Ditto, Pizarro, and Tannenbaum 2009; Murray, Holmes, and Griffin 1996a, 1996b). One way to avoid this is to focus on the action rather than the actor. Relatedly, for favorable judgments, we expect the opposite effect. If a close other does something praiseworthy, people are likely to focus on the actor rather than the action, helping to maintain a positive view of the close other (Forbes 2018; Murray, Holmes, and Griffin 1996a, 1996b).\nA key goal of moral categorization is to distinguish ‘good’ people from ‘bad’ people, to help us navigate the social world, and effectively guide our social interactions. Learning about people’s moral character or moral ‘essence’, enables us to establish relationships with ‘good’ people, and to limit our interactions with ‘bad’ people (or at least treat interactions with ‘bad’ people with caution). This means that for strangers, we are likely to show a bias for categorizing the actor rather than the action (Uhlmann, Pizarro, and Diermeier 2015; Dunlea and Heiphetz 2020; Siegel, Crockett, and Dolan 2017; Siegel et al. 2018)."
  },
  {
    "objectID": "posts/mjac/index.html#contrasting-mjac-with-existing-approaches",
    "href": "posts/mjac/index.html#contrasting-mjac-with-existing-approaches",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "Contrasting MJAC with Existing Approaches",
    "text": "Contrasting MJAC with Existing Approaches\nPerhaps the most important difference between MJAC and existing approaches is that the focus of MJAC is on the cognitive processes, rather than on the content of moral judgements. According to the dominant dual-process approaches, different types of moral judgements are grounded in different types of cognitive processes. Deontological (principled or rule based) moral judgements are grounded in intuitive/automatic/emotional/model-free processes, while utilitarian or consequentialist judgements (where the aim is to maximise positive outcomes), are grounded in deliberative/controlled/cognitive/model-based processes. These different processes mean that our judgements are susceptible to specific kinds of contexual influences, e.g., how persona/impersonal an action is (Greene 2008, 2016), the relative amount of emotionality (Byrd and Conway 2019; Conway et al. 2018; Conway and Gawronski 2013; Goldstein-Greenwood et al. 2020), or whether the focus is on the action or the outcome (Cushman 2013; Crockett 2013). An overview of these approaches is displayed in Figure 3. Despite these important insights, there are a range of other context effects known to influence moral judgements that are not accounted for by these models. These other context effects are detailed in the green boxes in Figure 3.1\n\n\n\nFigure 3: A sketch of dual-process approaches (contextual influences not directly addressed by these approaches highlighted in green)\n\n\nMJAC does not make assumptions based on the content of moral judgements. However, MJAC predicts a distinction between habitual (or skilled, or intuitive) responses, and deliberative (or controlled) responses. This distinction does not make assumptions about specific content of moral judgments. However, thinking about the contexts in which deontological vs utilitarian judgements are generally made, it makes sense that deontological rules might become more habitual (think It’s wrong to hurt people, Thou shalt not kill, You shouldn’t hit your sister), while utilitarian judgements may require more deliberation (e.g., how should we divide these resources in the fairest manner?). MJAC therefore integrates the insights of dual-process accounts, while also allowing for greater variability, and a more diverse range of context effects. Figure 4 outlines the various influences on moral judgement according to MJAC.\n\n\n\nFigure 4: Influences on moral judgements according to MJAC\n\n\nThese differences in assumptions between MJAC and other approaches lead to differences in explanations and predictions. Above I have outlined moral dumbfounding as an example of such an explanation. The differences in assumptions and explanations are listed in Table 2. To avoid making this post too long and drawn out, I will not go into detail on these differences, however I point you to the relevant section in the main article for more detailed discussion on this.\n\nTable 2: Contrasting MJAC with other approaches\n\n\n\n\n\n\n\n\n\n\n\nGreene’s Dual-process theory\n“Soft” dual-process theory\nModel-based accounts\nTDM\nMJAC\n\n\n\n\nAssumptions:\n\n\n\n\n\n\n\nContent\nDeontology-utilitarianism / personal-impersonal\nDeontology-utilitarianism\nAction-outcome\nHarm-based, dyadic\nDynamical Context-dependent Goal-directed\n\n\nMoral “Essence”\n(Implicit)\n(Not discussed)\n(Implicit)\nExplicit\nRejected\n\n\nProcesses\nDual-processes\nDual-processes\nDual-processes\n(implicitly dual-process)\nContinuum\n\n\nMechanisms\nIntuition (emotion) / cognition\nEmotion / cognition\nModel-based / model-free\nCategorization (unspecified)\nType-token interpretation\n\n\nPhenomena Explained:\n\n\n\n\n\n\n\nDumbfounding (harmless wrongs)\n(Not discussed)\n(Not discussed)\nExplained\nDenied\nExplained: learning history\n\n\nWrongless harms\n(Not discussed)\n(Not discussed)\n(Not discussed)\nDenied\nExplained: learning history\n\n\nTypicality\n(Not discussed)\n(Not discussed)\n(Not discussed)\nMatching of “prototype”\nContext-dependent\n\n\nContextual influences\nSpecific: Personal-impersonal\nSpecific: Emotion / cognition\n\nSpecific: Action-outcome\nSpecific: Harm-based\n\n\n\nIn the opening sections I outlined two general predictions of MJAC. We have also identified various specific predictions (e.g., the categorizing of actors vs actions described above). For brevity I do not go into detail on these specific predictions, but point you to the main article for this more detailed discussion (here is probably a good place to start)."
  },
  {
    "objectID": "posts/mjac/index.html#rejections",
    "href": "posts/mjac/index.html#rejections",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "Rejections",
    "text": "Rejections\nBy July 2019 it was ready for submission. It received a fairly quick desk reject from the first outlet.\nIt went to review in the second journal we tried. I’ve had some difficult rejections, and rejections that I disagreed with, but this one was really rough. Reviewer 1 really did not like the idea. The review from Reviewer 1 contained some of the harshest review comments I have seen. A few excerpts from the (very long) review are below.\n\nThe authors compile a great deal of research findings. They argue that moral judgment is about categorization, a position that flies in the face of and does not account for the decades of research on moral judgment in developmental and educational psychology.  The paper is incoherent in narrative, inadequate and misleading in explanation, and overall does not advance the field.  The paper seems to bounce from one thing to another without a clear, coherent story. Perhaps their thin version of moral judgment is a type of categorization, one based on perceiver making the other into a dead object. But so what? What does it buy the field? How does it contribute to scholarship?\n\n\n\n…. And rejected…some good constructive feedback, and some fairly harsh words… Skin is getting thicker with each paper #AcademicTwitter #rejected … I guess I'll try again tomorrow, for now I'm going to watch The Deuce #thedeuce https://t.co/VDKDwORhev pic.twitter.com/I6wGjajslL\n\n— Cillian McHugh ((CillianMacAodh?)) November 7, 2019\n\n\nThe experience with Reviewer 1 was a bit of a blow to the confidence. We brought it to our lab and made some fairly minor changes before sending it out again. Just before the Christmas break in 2019 I submitted it to Perspectives on Psychological Science (fully expecting to receive another reject). In February 2020 I went to SPSP in New Orleans, where I also was due to present MJAC as a poster at the Justice and Morality pre-conference. Upon landing I checked my email, and was very surprised to have received an R&R."
  },
  {
    "objectID": "posts/mjac/index.html#revisions",
    "href": "posts/mjac/index.html#revisions",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "Revisions",
    "text": "Revisions\nThe reviews were really fair, but extensive. The original framing relied heavily on the parallels provided in Table 1 above. The reviewers were not convinced by this argument. We were encouraged to clearly differentiate MJAC from existing approaches, with instructions to identify and better engage with specific dual-process accounts, and with the Theory of Dyadic Morality (Schein and Gray 2018).\nSo the revisions were really quite tough. I approached them systematically, addressing each comment as well as was possible, and documenting how the changes made addressed each comment. Many of the comments addressed fairly deep conceptual questions, requiring extensive reading before I could begin attempting to address them. And, naturally, I shared my progress on social media:\n\n\nYup! Starting to see progress!Green is done ✅ lighter green is done, but not sure if it's good enough 🤷‍♂️ Blue means there was nothing to be done 👌🏼 red…. well there's still time 😬🙈 https://t.co/zPehbIWlYr pic.twitter.com/YVNVtonRfW\n\n— Cillian McHugh ((CillianMacAodh?)) May 13, 2020\n\n\nI was also able to call on social media for help in addressing specific issues that came up while doing the revisions. The replies came in quickly, and really provided excellent resources to help with the revisions.\n\n\nSimpson et al. (2016) look at third-party observers who witnessed a transgression between two targets who share a relationship, and Hofmann et al. (2018) look at social closeness and differences in desire to punish (2/2)\n\n— Rachel Forbes ((rc_forbes?)) May 10, 2020\n\n\nFollowing weeks of work, we had finally addressed all the reviewer comments. I was sick of it at this stage, and ready to submit. Unfortunately, the extent of the revisions meant that the manuscript was too long to submit. I noted that there is technically no word limit for submissions at Perspectives on Psychological Science, but my co-authors wisely convinced me to ask for an extension so we could cut down the words to something more manageable.\n\n\nSo working on major revisions of a manuscript, all comments are pretty much addressed…but now we're facing the problem of length…Original submission between 8,000 and 9,000 wordsCurrent version is 20,000 words…Aaaaaah  😫😖#AcademicTwitter #AcademicChatter✂️🤔\n\n— Cillian McHugh ((CillianMacAodh?)) May 20, 2020\n\n\nSo we spent another few weeks cutting words, and restructuring sections to be more streamlined (huge credit to coauthors in this endeavour). And eventually we submitted the revised manuscript. It was unrecognisable from the original submission.\n\n\nAnd revised manuscript submitted! 12,037 in the end!That was exhausting! 😂 https://t.co/c56NeW51Cr\n\n— Cillian McHugh ((CillianMacAodh?)) June 22, 2020\n\n\nWe submitted the revised version in June 2020, and received a decision of Conditional Accept (with minor revisions) in September. It was fully accepted in November 2020, and published online in July 2021 (almost 7 years after the original idea was formed)\n\nKey Points in the Revisions\nI think one of the most important changes that came from the review process was the clarity in the argument. The original submission simply presented the approach, but we didn’t articulate a clear problem the approach was solving. This was a tough question to address. The range of existing approaches available means that trying to go into detail on the relative strengths and weaknesses is not feasible. In contrast, a more general approach risks over-generalizing and potentially mis-representing some approaches. As the revisions progressed this core argument presented itself. We identified the variability and context dependency of moral judgements as a challenge that is not well addressed in the existing literature. Because dynamism and context dependency are core assumptions of MJAC, this means that MJAC is well positioned to address this challenge.\n\n\n\n(Whedon 1997)"
  },
  {
    "objectID": "posts/mjac/index.html#references",
    "href": "posts/mjac/index.html#references",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/mjac/index.html#footnotes",
    "href": "posts/mjac/index.html#footnotes",
    "title": "Moral Judgment as Categorization (MJAC) - Explanation and Development",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe reader is referred to the wealth of literature examining such factors as emotional influences, Cameron, Payne, and Doris (2013); intentionality, evitability, benefit recipient, Christensen et al. (2014); Christensen and Gomila (2012); action-outcome distinction Crockett (2013); cushman_action_2013; trustworthiness and social evaluation Everett, Pizarro, and Crockett (2016); Everett et al. (2018); personal-impersonal distinction, Greene et al. (2001); doctrine of double effect, Mikhail (2000); level of physical contact, Valdesolo and DeSteno (2006); order effects, Wiegmann, Okan, and Nagel (2012)↩︎"
  },
  {
    "objectID": "posts/reasons/index.html",
    "href": "posts/reasons/index.html",
    "title": "Reasons or Rationalisations",
    "section": "",
    "text": "This post provides a brief overview of a paper where we tested alternative explanations of moral dumbfounding proposed by critics (McHugh et al. 2020).\n(full text available here)"
  },
  {
    "objectID": "posts/reasons/index.html#evidence-for-judgements-based-on-principles-or-reasons",
    "href": "posts/reasons/index.html#evidence-for-judgements-based-on-principles-or-reasons",
    "title": "Reasons or Rationalisations",
    "section": "Evidence for Judgements based on Principles or Reasons",
    "text": "Evidence for Judgements based on Principles or Reasons\nThe strongest challenge to moral dumbfounding came from a set of studies by Royzman, Kim, and Leeman (2015), which tested moral dumbfounding directly, using the Julie and Mark dilemma which reads:\n\nJulie and Mark, who are brother and sister, are travelling together in France. They are both on summer vacation from college. One night they are staying alone in a cabin near the beach. They decide that it would be interesting and fun if they tried making love. At very least it would be a new experience for each of them. Julie was already taking birth control pills, but Mark uses a condom too, just to be safe. They both enjoy it, but they decide not to do it again. They keep that night as a special secret between them, which makes them feel even closer to each other.\n\n\nRoyzman et al. (2015) identified two principles that appear to be guiding judgements\n\nThe Harm Principle\nThe Norm Principle\n\nParticipants who endorse either principle have a reason to judge the behaviour of Julie and Mark as wrong (and therefore cannot be dumbfounded)\n\n\nExclusion of Participants\nRoyzman et al. (2015) excluded participants who endorsed either principle from analysis, and found that dumbfounding effectively disappeared.\n\nHarm-based reasons\nParticipants were presented with the following two questions (emphasis added here):\n\n“are you able to believe that Julie and Mark’s having sex with each other will not negatively affect the quality of their relationship or how they feel about each other later on?”\n“are you able to believe that Julie and Mark’s having sex with each other will have no bad consequences for them personally and/or for those close to them?”\n\nIf participants responded “no” to either question they were excluded from analysis.\n\n\nNorm-based reasons\nParticipants were presented with the following two statements and asked to select the one they agrred with most (emphasis added here):\n\n“violating an established moral norm just for fun or personal enjoyment is wrong only in situations where someone is harmed as a result, but is acceptable otherwise.”\n“violating an established moral norm just for fun or personal enjoyment is inherently wrong even in situations where no one is harmed as a result.”\n\nParticipants who selected the second statement were excluded from analysis."
  },
  {
    "objectID": "posts/reasons/index.html#three-exclusion-criteria",
    "href": "posts/reasons/index.html#three-exclusion-criteria",
    "title": "Reasons or Rationalisations",
    "section": "Three Exclusion Criteria",
    "text": "Three Exclusion Criteria\nWe adopted the same method as Royzman et al. (2015), and excluded participants whose judgements could be attributed to either the harm principle or the norm principle. However rather than relying on endorsing alone, we developed additional, and more rigorous, exclusion criteria. We compared rates of dumbfounding based on these different criteria. We also tested the relative accuracy of the exclusion criteria (based on false exclusions).\nFirst we assessed whether participants articulated either principle. We provided an open-ended response question and asked participants to provide reasons for their judgements. The reasons provided were coded for mention of either principle.\nSecond we assessed whether participants applied the harm principle across different contexts. We asked participants 3 questions about their judgements of behaviours/activities that could potentially lead to harm2\nAcross three studies we assessed rates of dumbfounding based on these different exclusion criteria.\n\nEndorsing (Royzman et al., 2015, all studies)\nArticulating (all studies)\nApplying (Studies 2 and 3)"
  },
  {
    "objectID": "posts/reasons/index.html#measuring-dumbfounding",
    "href": "posts/reasons/index.html#measuring-dumbfounding",
    "title": "Reasons or Rationalisations",
    "section": "Measuring Dumbfounding",
    "text": "Measuring Dumbfounding\nDumbfounding was measured using the critical slide which read as follows:\n\n“Julie and Mark’s actions did not harm anyone or negatively affect anyone. How can there be anything wrong with what they did?” 1. There is nothing wrong 2. It’s wrong but I can’t think of a reason 3. It’s wrong and I can provide a valid reason\n\n(The selecting of option 2, the admission of not having reasons, was taken to be a dumbfounded response)"
  },
  {
    "objectID": "posts/reasons/index.html#rates-of-dumbfounding-depending-on-exclusion-type",
    "href": "posts/reasons/index.html#rates-of-dumbfounding-depending-on-exclusion-type",
    "title": "Reasons or Rationalisations",
    "section": "Rates of Dumbfounding depending on Exclusion type",
    "text": "Rates of Dumbfounding depending on Exclusion type\nOn the graphs below you can see the frequency of each response to the critical slide for each study. The furthest left bars represent the full sample. The responses for each of the sub-samples following the relevant exclusion criterion are displayed and labelled separately.\nThe response we are intersted in is the red bar denoting a dumbfounded response. Percentages of the full sample are displayed within the plot. Percentages of the relevant sub-samples are displayed in parenthesis below the count.\n\nStudy 1:\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nStudy 1 - Responses to critical slide for the full sample and for each exclusion criterion\n\n\n\n\n\n\nStudy 2:\n\n\n\n\n\nStudy 2 - Responses to critical slide for the full sample and for each exclusion criterion\n\n\n\n\n\n\nStudy 3:\n\n\n\n\n\nStudy 3 - Responses to critical slide for the full sample and for each exclusion criterion\n\n\n\n\nAs we can see above, dumbfounded responding is found in the full sample for each study.\nReplicating the finding by Royzman et al. (2015), if we exclude participants who endorse either principle, rates of dumbfounding are negligible.\nHowever, if we also account for whether people articulate or apply either principle, dumbfounded responding is observed. So whether or not dumbfounding is real, depends on which exclusion criterion is more accurate."
  },
  {
    "objectID": "posts/reasons/index.html#relative-accuracy-of-exclusion-criteria",
    "href": "posts/reasons/index.html#relative-accuracy-of-exclusion-criteria",
    "title": "Reasons or Rationalisations",
    "section": "Relative Accuracy of Exclusion Criteria",
    "text": "Relative Accuracy of Exclusion Criteria\nWhile we do not have a direct measure of the relative accuracy of the different exclusion criteria, we can assess the relative rates of false exclusions. All exclusions are based on attributing participants’ judgements to either principle. In this case, the judgements should be consistent with the relevant principle. As such, participants who selected “There is nothing wrong” (blue bars above) should not be excluded from analysis. Any participant who selected “There is nothing wrong” and was excluded, was falsely excluded.\nBelow we have subsetted the participants who selected “There is nothing wrong” across each study, and plot the rates of rates of false exclusion based on each exclusion criterion. The percentage of participants who selected “There is nothing wrong” is displayed within the plot, and the percentage of the full sample is displayed in parenthesis below the count.\n\n\n\n\n\nExclusion of participants who selected ‘There is nothing wrong’\n\n\n\n\nAs can be seen from the above, rates of false exclusion are much higher when endorsing is the only exclusion criterion. Some false exclusion remains for each of the other exclusion criteria, however it is a considerable improvement. Based on this we conclude that these revised criteria are more robust, and our studies provide evidence that Moral Dumbfounding is indeed real."
  },
  {
    "objectID": "posts/reasons/index.html#references",
    "href": "posts/reasons/index.html#references",
    "title": "Reasons or Rationalisations",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/reasons/index.html#footnotes",
    "href": "posts/reasons/index.html#footnotes",
    "title": "Reasons or Rationalisations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMoral dumbfounding occurs when people stubbornly maintain a moral judgement, even though they can provide no reason to support their judgements (Haidt 2001; Haidt, Björklund, and Murphy 2000; Prinz 2005; McHugh et al. 2017). Dumbfounded responses may include: (a) an admission of not having a reason for a judgement, (b) the use of an unsupported declarations (“It’s just wrong!”) to defend a judgement.↩︎\n“How would you rate the behavior of two people who engage in an activity that could potentially result in harmful consequences for either of them?”; “Do you think boxing is wrong?”; “Do you think playing contact team sports (e.g. rugby; ice-hockey; American football) is wrong?”↩︎"
  },
  {
    "objectID": "posts/searching/index.html",
    "href": "posts/searching/index.html",
    "title": "Searching for Moral Dumbfounding",
    "section": "",
    "text": "This post provides a brief overview of a paper where we developed methods for measuring and testing moral dumbfounding (McHugh et al. 2017)."
  },
  {
    "objectID": "posts/searching/index.html#results",
    "href": "posts/searching/index.html#results",
    "title": "Searching for Moral Dumbfounding",
    "section": "Results",
    "text": "Results\nTwenty two of the 31 participants (70.97%) produced a dumbfounded response at least once. (admissions of not having reasons; or the use of an unsupported declaration as a justification)\n\nExamples of such responses included:\n\n“It just seems wrong and I cannot explain why, I don’t know”\n“because I just think it’s wrong, oh God, I don’t know why, it’s just [pause] wrong”.\n\n\nThe rates of each type of response for each Scenario are displayed in Figure @ref(fig:figdumb1Interview)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\nUse of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\nWarning: The dot-dot notation (`..count..`) was deprecated in ggplot2 3.4.0.\nℹ Please use `after_stat(count)` instead.\n\n\n\n\n\nRates of each type of response for each scenario"
  },
  {
    "objectID": "posts/searching/index.html#behavioural-responses",
    "href": "posts/searching/index.html#behavioural-responses",
    "title": "Searching for Moral Dumbfounding",
    "section": "Behavioural Responses",
    "text": "Behavioural Responses\nIn addition to coding the videos for dumbfounded responding, we investigated a range of verbal and non-verbal behavioural responses. We found significant differences between dumbfounded participants and non-dumbfounded participants across a small number of variables. There were significant differences in time spent\n\nLaughing\nSmiling\nin Silence\n\n(with higher rates of each in dumbfounded participants)\nWe found no differences in\n\nverbal hesitations; non-verbal hesitations\nchanging posture; fidgeting/hands on the self\nfrowning\n\nImportantly, there were no differences depending on which measure of dumbfounding was used  (Admissions of not having reasons / Unsupported declarations)"
  },
  {
    "objectID": "posts/searching/index.html#results-1",
    "href": "posts/searching/index.html#results-1",
    "title": "Searching for Moral Dumbfounding",
    "section": "Results",
    "text": "Results\nExtremely high rates of dumbfounded responding were recorded, see Figure @ref(fig:figdumb1comp1).\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\nUse of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\n\n\n\nRates of each type of response for each scenario"
  },
  {
    "objectID": "posts/searching/index.html#results---admissions-only",
    "href": "posts/searching/index.html#results---admissions-only",
    "title": "Searching for Moral Dumbfounding",
    "section": "Results - Admissions Only",
    "text": "Results - Admissions Only\nWe have provided two sets of results below, firstly the results with just the responses to the critical slide. Following this we include the open-ended responses coded for unsupported declarations. Figure @ref(fig:figdumb1comp2) shows the rate of each response to the critical slide for each scenario.\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\nUse of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\n\n\n\nRates of each type of response for each scenario"
  },
  {
    "objectID": "posts/searching/index.html#results---including-open-ended-responses",
    "href": "posts/searching/index.html#results---including-open-ended-responses",
    "title": "Searching for Moral Dumbfounding",
    "section": "Results - including open-ended responses",
    "text": "Results - including open-ended responses\nFigure @ref(fig:figdumb1comp2string) shows the rates of each type of response, when the coded open-ended responses are included.\n\n\nWarning: Use of `test$perc` is discouraged.\nℹ Use `perc` instead.\nUse of `test$perc` is discouraged.\nℹ Use `perc` instead.\n\n\n\n\n\nRates of each type of response for each scenario"
  },
  {
    "objectID": "posts/searching/index.html#combined-tables",
    "href": "posts/searching/index.html#combined-tables",
    "title": "Searching for Moral Dumbfounding",
    "section": "Combined Tables",
    "text": "Combined Tables\n\n\n(#tab:tab1judge)\n\n\nRatings of each scenario for each study\n\n\n\n\nStudy\nJudgement\n\nHeinz\n\nCannibal\n\nIncest\n\nTrolley\n\n\n\n\nStudy 1\nInitial: Wrong\n27\n87.1%\n25\n80.65%\n26\n83.87%\n23\n74.19%\n\n\n\nInitial: Neutral\n0\n0%\n0\n0%\n0\n0%\n0\n0%\n\n\n\nInitial: OK\n4\n12.9%\n6\n19.35%\n5\n16.13%\n8\n25.81%\n\n\n\nRevised: Wrong\n26\n83.87%\n23\n74.19%\n20\n64.52%\n22\n70.97%\n\n\n\nRevised: Neutral\n0\n0%\n0\n0%\n0\n0%\n1\n3.23%\n\n\n\nReviesd: OK\n5\n16.13%\n8\n25.81%\n11\n35.48%\n8\n25.81%\n\n\nStudy 2\nInitial: Wrong\n53\n73.61%\n68\n94.44%\n63\n87.5%\n50\n69.44%\n\n\n\nInitial: Neutral\n9\n12.5%\n3\n4.17%\n3\n4.17%\n6\n8.33%\n\n\n\nInitial: OK\n10\n13.89%\n1\n1.39%\n6\n8.33%\n16\n22.22%\n\n\n\nRevised: Wrong\n51\n70.83%\n67\n93.06%\n66\n91.67%\n48\n66.67%\n\n\n\nRevised: Neutral\n7\n9.72%\n3\n4.17%\n3\n4.17%\n9\n12.5%\n\n\n\nReviesd: OK\n14\n19.44%\n2\n2.78%\n3\n4.17%\n15\n20.83%\n\n\nStudy 3a\nInitial: Wrong\n54\n75%\n67\n93.06%\n61\n84.72%\n48\n66.67%\n\n\n\nInitial: Neutral\n6\n8.33%\n3\n4.17%\n7\n9.72%\n10\n13.89%\n\n\n\nInitial: OK\n12\n16.67%\n2\n2.78%\n4\n5.56%\n14\n19.44%\n\n\n\nRevised: Wrong\n53\n73.61%\n67\n93.06%\n57\n79.17%\n43\n59.72%\n\n\n\nRevised: Neutral\n11\n15.28%\n4\n5.56%\n12\n16.67%\n15\n20.83%\n\n\n\nReviesd: OK\n8\n11.11%\n1\n1.39%\n3\n4.17%\n14\n19.44%\n\n\nStudy 3b\nInitial: Wrong\n81\n80.2%\n85\n84.16%\n71\n70.3%\n66\n65.35%\n\n\n\nInitial: Neutral\n9\n8.91%\n13\n12.87%\n20\n19.8%\n14\n13.86%\n\n\n\nInitial: OK\n11\n10.89%\n3\n2.97%\n10\n9.9%\n21\n20.79%\n\n\n\nRevised: Wrong\n87\n86.14%\n82\n81.19%\n73\n72.28%\n59\n58.42%\n\n\n\nRevised: Neutral\n10\n9.9%\n15\n14.85%\n19\n18.81%\n17\n16.83%\n\n\n\nReviesd: OK\n4\n3.96%\n4\n3.96%\n9\n8.91%\n25\n24.75%\n\n\n\n\n\n\n(#tab:tab2dumb)\n\n\nObserved frequency and percentage of each of the responses: dumbfounded, nothing wrong, and reasons provided\n\n\n\n\n\n\n\nHeinz\n\nCannibal\n\nIncest\n\nTrolley\n\n\n\n\nStudy 1\nNothing wrong\n6\n19.35%\n8\n25.81%\n11\n35.48%\n8\n25.81%\n\n\n\nDumbfounded\n0\n0%\n11\n35.48%\n18\n58.06%\n3\n9.68%\n\n\n\n(admissions)\n0\n0%\n8\n25.81%\n10\n32.26%\n3\n9.68%\n\n\n\n(declarations)\n0\n0%\n3\n9.68%\n8\n25.81%\n0\n0%\n\n\n\nReasons\n25\n80.65%\n12\n38.71%\n2\n6.45%\n20\n64.52%\n\n\nStudy 2\nNothing wrong\n8\n11.11%\n4\n5.56%\n2\n2.78%\n10\n13.89%\n\n\n\nDumbfounded\n45\n62.5%\n46\n63.89%\n54\n75%\n45\n62.5%\n\n\n\nReasons\n19\n26.39%\n22\n30.56%\n16\n22.22%\n17\n23.61%\n\n\nStudy 3a\nNothing wrong\n14\n19.44%\n4\n5.56%\n12\n16.67%\n15\n20.83%\n\n\n(critical slide)\nDumbfounded\n13\n18.06%\n14\n19.44%\n18\n25%\n14\n19.44%\n\n\n\nReasons\n45\n62.5%\n54\n75%\n42\n58.33%\n43\n59.72%\n\n\nStudy 3a\nNothing wrong\n14\n19.44%\n4\n5.56%\n12\n16.67%\n15\n20.83%\n\n\n(coded)\nDumbfounded\n19\n26.39%\n21\n29.17%\n31\n43.06%\n22\n30.56%\n\n\n\nReasons\n39\n54.17%\n47\n65.28%\n29\n40.28%\n35\n48.61%\n\n\nStudy 3b\nNothing wrong\n21\n20.79%\n10\n9.9%\n31\n30.69%\n24\n23.76%\n\n\n(critical slide)\nDumbfounded\n12\n11.88%\n19\n18.81%\n16\n15.84%\n16\n15.84%\n\n\n\nReasons\n68\n67.33%\n72\n71.29%\n54\n53.47%\n61\n60.4%\n\n\nStudy 3b\nNothing wrong\n21\n20.79%\n10\n9.9%\n31\n30.69%\n24\n23.76%\n\n\n(coded)\nDumbfounded\n16\n15.84%\n30\n29.7%\n28\n27.72%\n22\n21.78%\n\n\n\nReasons\n64\n63.37%\n61\n60.4%\n42\n41.58%\n55\n54.46%\n\n\n\n\n\n\n(#tab:tab3Qs)\n\n\nResponses to post-discussion questionnaire questions\n\n\n\n\nStudy\nQuestion\nHeinz\nCannibal\nIncest\nTrolley\n\n\n\n\nStudy 1\nChanged mind\n2.87\n3.40\n2.63\n2.60\n\n\n\nConfidence\n5.30\n4.77\n5.40\n5.07\n\n\n\nConfused\n3.00\n3.67\n3.33\n3.70\n\n\n\nIrritated\n3.00\n3.33\n3.13\n3.37\n\n\n\n‘Gut’\n5.23\n5.20\n4.97\n5.07\n\n\n\n‘Reason’\n4.83\n4.40\n4.43\n4.77\n\n\n\nGut minus Reason\n0.40\n0.80\n0.53\n0.30\n\n\nStudy 2\nConfidence\n6.10\n5.86\n5.62\n5.26\n\n\n\nConfused\n2.40\n3.08\n4.14\n3.17\n\n\n\nIrritated\n4.58\n4.68\n4.32\n4.28\n\n\n\n‘Gut’\n5.29\n5.54\n5.82\n4.96\n\n\n\n‘Reason’\n4.89\n5.19\n4.89\n4.93\n\n\n\nGut minus Reason\n0.40\n0.35\n0.93\n0.03\n\n\nStudy 3a\nChanged mind\n2.38\n1.67\n2.00\n2.00\n\n\n\nConfidence\n5.22\n5.50\n5.38\n4.81\n\n\n\nConfused\n2.75\n2.96\n3.25\n2.89\n\n\n\nIrritated\n3.94\n4.64\n4.07\n3.60\n\n\n\n‘Gut’\n4.78\n5.44\n5.44\n4.92\n\n\n\n‘Reason’\n5.07\n5.26\n5.11\n5.06\n\n\n\nGut minus Reason\n-0.29\n0.18\n0.33\n-0.14\n\n\nStudy 3b\nChanged mind\n1.74\n1.60\n1.57\n1.83\n\n\n\nConfidence\n5.78\n6.16\n5.81\n5.36\n\n\n\nConfused\n2.06\n2.07\n2.12\n2.22\n\n\n\nIrritated\n4.42\n4.01\n3.56\n3.39\n\n\n\n‘Gut’\n4.42\n4.43\n4.47\n4.01\n\n\n\n‘Reason’\n5.46\n5.69\n5.26\n5.58\n\n\n\nGut minus Reason\n-1.04\n-1.27\n-0.79\n-1.57"
  },
  {
    "objectID": "other/2019-05-15-js-mediation-shinyapp/index.html",
    "href": "other/2019-05-15-js-mediation-shinyapp/index.html",
    "title": "JS Mediation",
    "section": "",
    "text": "In response to this paper by Yzerbyt, Batailler and Judd (2018) which outined a new method of conducting mediation analyses (with less susceptability to false positives than Hayes’ PROCESS) I created a ShinyApp so that their R-package could be used by SPSS users. Upload your SPSS file below and select the variables you wish to compare.\n\n\n \n\n(see https://cillianmacaodh.shinyapps.io/JS_mediation/”)\nYzerbyt, V., Muller, D., Batailler, C., & Judd, C. M. (2018). New Recommendations for Testing Indirect Effects in Mediational Models: The Need to Report and Test Component Paths. Journal of Personality and Social Psychology: Attitudes and Social Cognition, 115(6), 929–943. http://dx.doi.org/10.1037/pspa0000132"
  },
  {
    "objectID": "other/2019-12-10-moral-networks-shinyapp/index.html",
    "href": "other/2019-12-10-moral-networks-shinyapp/index.html",
    "title": "Moral Networks",
    "section": "",
    "text": "Following discussions with members of the DAFINET team, I have become increasingly interested in the idea that moral values might be strengthened by the way in which people might be connected by their values. To investigate this, I started playing around with building networks in R. I downloaded some existing data on Moral Foundations Theory from the OSF (see here). In order to play with it more interactively, I built this ShinyApp."
  },
  {
    "objectID": "other/2019-12-10-moral-networks-shinyapp/index.html#instructions",
    "href": "other/2019-12-10-moral-networks-shinyapp/index.html#instructions",
    "title": "Moral Networks",
    "section": "Instructions",
    "text": "Instructions\n\nSelect the range of participants you want included in your network; there are 522 participants, numbered 1-522, you can select the range by inputting the first and last participants in the range.\nSelect the Moral Foundations, or the specific questions you want to look at.\n\n\nNotes\n\nWhen it first loads it will show an error because nothing is selected;\nIf there is no network to be built (e.g., 2 participants who disagree on the only item selected) it will also show up an error\nIf too many participants and too many items are selected it will probably crash (I tried to create a full network of all participants and all items, and after 3 hours I gave up)\nDue to the amount of content in the App it doesn’t fit too well embedded below, so it might going directly to https://cillianmacaodh.shinyapps.io/moral_networks/ to play with it.\n\n\n \n\n(see https://cillianmacaodh.shinyapps.io/moral_networks/)"
  },
  {
    "objectID": "other/chess/index.html",
    "href": "other/chess/index.html",
    "title": "Chess",
    "section": "",
    "text": "In my spare time I also enjoy playing chess. Check out the puzzle of the day from lichess below. Find me on lichess and challenge me to a game at CillianMacAodh.\n\n\n\nchess\n\n\n\n\n\nchess\n\n\n\n\n\nchess\n\n\n\n\n\nchess"
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html",
    "href": "publications/searching-for-moral-dumbfounding/index.html",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "",
    "text": "Moral dumbfounding is defined as maintaining a moral judgement, without supporting reasons. The most cited demonstration of dumbfounding does not identify a specific measure of dumbfounding and has not been published in peer-review form, or directly replicated. Despite limited empirical examination, dumbfounding has been widely discussed in moral psychology. The present research examines the reliability with which dumbfounding can be elicited, and aims to identify measureable indicators of dumbfounding. Study 1 aimed at establishing the effect that is reported in the literature. Participants read four scenarios and judged the actions described. An Interviewer challenged participants’ stated reasons for judgements. Dumbfounding was evoked, as measured by two indicators, admissions of not having reasons (17%), unsupported declarations (9%) with differences between scenarios. Study 2 measured dumbfounding as the selecting of an unsupported declaration as part of a computerised task. We observed high rates of dumbfounding across all scenarios. Studies 3a (college sample) and 3b (MTurk sample), addressing limitations in Study 2, replaced the unsupported declaration with an admission of having no reason, and included open-ended responses that were coded for unsupported declarations. As predicted, lower rates of dumbfounding were observed (3a 20%; 3b 16%; or 3a 32%; 3b 24% including unsupported declarations in open-ended responses). Two measures provided evidence for dumbfounding across three studies; rates varied with task type (interview/computer task), and with the particular measure being employed (admissions of not having reasons/unsupported declarations). Possible cognitive processes underlying dumbfounding and limitations of methodologies used are discussed as a means to account for this variability."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#method",
    "href": "publications/searching-for-moral-dumbfounding/index.html#method",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Method",
    "text": "Method\n\n\nParticipants and design\n\n\nStudy 1 was a frequency based attempted replication. The aim was to identify if dumbfounded responding could be evoked. All participants were presented with the same four moral vignettes. Results are primarily descriptive. Any further analysis tested for differences in responding depending on the vignette, or type of vignette, presented.\n\n\nA sample of 31 participants (15 female, 16 male) with a mean age of Mage = 28.83 (min = 19, max = 64, SD = 10.99) took part in this study. Participants were undergraduate students, postgraduate students, and alumni from Mary Immaculate College (MIC), and University of Limerick (UL). Participation was voluntary and participants were not reimbursed for their participation.\n\n\n\nProcedure and materials\n\nFour moral judgement vignettes were used (Appendix A). Three of the vignettes (Heinz, Incest, and Cannibal) were taken from Haidt, Björklund, and Murphy (2000). Incest was taken directly from the original study however Cannibal and Heinz were modified slightly, following piloting.\n\n\nThe original version of Cannibal stated that people had “donated their body to science for research”; participants during piloting were able to argue that eating does not constitute “research”. In order to remove this as a possible argument, the modified version stated that bodies had been donated for “the general use of the researchers in the lab” and that the “bodies are normally cremated, however, severed cuts may be disposed of at the discretion of lab researchers”.\n\n\nSimilarly, piloting suggested that participants agreed with the actions of Heinz and condemned the actions of the druggist. The original wording of Heinz suggested that any discussion related to Heinz as opposed to the druggist meaning that, for Heinz, participants would typically be defending an approval of the character’s actions. However, for Incest and Cannibal participants generally condemn the actions of the character and as such are defending a judgement of “morally wrong”. In order to ensure that participants were consistently defending a judgement of “morally wrong” across all scenarios, Heinz was modified to include “The druggist had Heinz arrested and charged”. Any discussion on Heinz then related to the character whose behaviour participants thought was wrong.\n\n\nIn the original study by Haidt, Björklund, and Murphy (2000), Incest and Cannibal are presented as “intuition” stories, and contrasted against a single “reasoning” dilemma: Heinz. In order for a more balanced comparison, a bridge variant of the classic trolley dilemma (Trolley) was included as a second “reasoning” dilemma. In this vignette, participants judge the actions of Paul, who pushes a large man off a bridge to stop a trolley and save five lives. The inclusion of Trolley meant that there were two “reasoning” dilemmas to be contrasted with the two “intuition” stories.\n\n\nSample counter arguments were prepared for each scenario. To ensure that participants were only pushed to defend a judgement of “morally wrong” these counter arguments exclusively defended the potentially questionable behaviour of the characters. A list of prepared counter arguments can be seen in Appendix B. A post-discussion questionnaire, taken from Haidt, Björklund, and Murphy (2000) was administered after discussion of each scenario (Appendix C).\n\n\nTwo other measures were also taken for exploratory purposes.: Firstly, in response to a possible link between meaning and morality (e.g., Bellin 2012; Schnell 2011), the Meaning in Life questionnaire (MLQ; Steger et al. 2008) was included. This ten item scale, is made up of two five item sub scales: presence (e.g., “I understand my life’s meaning”) and search (e.g., “I am looking for something that makes my life feel meaningful”). Responses were recorded using a seven point Likert scale ranging from 1 (strongly disagree) to 7 (strongly agree). Secondly, in line with Haidt’s (2007; see also, Haidt and Hersh 2001) work, describing a link between religious conservatism and moral views, it was hypothesised that incidences of dumbfounding may be moderated by individual differences in religiosity . As such, the seven item CRSi7 scale, taken from The Centrality of Religiosity Scale (Huber and Huber 2012) was also included. Participants responded to questions relating to the frequency with which they engage in religious or spiritual activity (e.g., “How often do you think about religious issues?”). Responses were recorded using a five point Likert scale ranging from 1 (never) to 5 (very often).\n\n\nThe interviews took place in a designated psychology lab in MIC and were recorded on a digital video recording device. Participants were presented with an information sheet and a consent form. The consent form required two signatures: firstly, participants consented to take part in the study (including consent to be video recorded); the second signature related to use of the video for any presentation of the research (with voice distorted and face pixelated). Only two participants opted not to sign the second part.\n\n\nParticipants read brief vignettes describing each scenario, and were subsequently interviewed regarding the protagonists. All four scenarios were discussed in a single interview session, with a brief pause between each discussion for the participant to complete a questionnaire about their judgements, and to read the next scenario. The conversation continued when they were happy to do so. Each of the four moral dilemmas Heinz, Trolley, Cannibal and Incest (Appendix A) were presented in this way and participants asked to judge the behaviour of the characters in the dilemmas. The order of presenting the scenarios was randomised. Judgements made by participants were challenged by the experimenter (“Nobody was harmed, how can there be anything wrong?”; “Do you still think it was wrong? Why?”; “Why do you think it is wrong?”; “Have you got a reason for your judgement?”). The resulting discussion continued until participants could not articulate any further arguments. Participants filled in a brief questionnaire after discussing each dilemma. In this they were asked to rate, on a seven point Likert scale, how right/wrong they thought the behaviour was; how confident they were in their judgement, how confused they were; how irritated they were; how much their judgement had changed; how much their judgement was based on reason; and how much their judgement was based on “gut” feeling. Participants completed a longer questionnaire at the end of the interview. This contained the MLQ (Steger et al. 2008), the Centrality of Religiosity Scale (Huber and Huber 2012), and some questions relating to demographics The entire study lasted approximately 20 to 25 minutes. The videos were analysed using BORIS – Behavioural Observation Research Interactive Software (Friard and Gamba 2015). All statistical analysis was conducted using R (3.4.0, R Core Team 2017b)4; SPSS (IBM Corp 2015) was also used."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion",
    "href": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nThe videos of the interviews were analysed and participants were identified as dumbfounded if they ( a ) admitted to not having reasons for their judgements; or ( b ) resorted to using unsupported declarations (“It’s just wrong!”) as justification for their judgements, and subsequently failed to provide reasons when questioned further. Table 1 shows the initial and revised ratings of the behaviours for each scenario.\n\n\nTable 1\n\n\nRatings of each scenario for each study\n\n\n\n\n\n\nStudy\n\n\nJudgement\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\n\n\n\n\nStudy 1\n\n\nInitial: Wrong\n\n\n27\n\n\n87.1%\n\n\n25\n\n\n80.65%\n\n\n26\n\n\n83.87%\n\n\n23\n\n\n74.19%\n\n\n\n\n\n\nInitial: Neutral\n\n\n0\n\n\n0%\n\n\n0\n\n\n0%\n\n\n0\n\n\n0%\n\n\n0\n\n\n0%\n\n\n\n\n\n\nInitial: OK\n\n\n4\n\n\n12.9%\n\n\n6\n\n\n19.35%\n\n\n5\n\n\n16.13%\n\n\n8\n\n\n25.81%\n\n\n\n\n\n\nRevised: Wrong\n\n\n26\n\n\n83.87%\n\n\n23\n\n\n74.19%\n\n\n20\n\n\n64.52%\n\n\n22\n\n\n70.97%\n\n\n\n\n\n\nRevised: Neutral\n\n\n0\n\n\n0%\n\n\n0\n\n\n0%\n\n\n0\n\n\n0%\n\n\n1\n\n\n3.23%\n\n\n\n\n\n\nReviesd: OK\n\n\n5\n\n\n16.13%\n\n\n8\n\n\n25.81%\n\n\n11\n\n\n35.48%\n\n\n8\n\n\n25.81%\n\n\n\n\nStudy 2\n\n\nInitial: Wrong\n\n\n53\n\n\n73.61%\n\n\n68\n\n\n94.44%\n\n\n63\n\n\n87.5%\n\n\n50\n\n\n69.44%\n\n\n\n\n\n\nInitial: Neutral\n\n\n9\n\n\n12.5%\n\n\n3\n\n\n4.17%\n\n\n3\n\n\n4.17%\n\n\n6\n\n\n8.33%\n\n\n\n\n\n\nInitial: OK\n\n\n10\n\n\n13.89%\n\n\n1\n\n\n1.39%\n\n\n6\n\n\n8.33%\n\n\n16\n\n\n22.22%\n\n\n\n\n\n\nRevised: Wrong\n\n\n51\n\n\n70.83%\n\n\n67\n\n\n93.06%\n\n\n66\n\n\n91.67%\n\n\n48\n\n\n66.67%\n\n\n\n\n\n\nRevised: Neutral\n\n\n7\n\n\n9.72%\n\n\n3\n\n\n4.17%\n\n\n3\n\n\n4.17%\n\n\n9\n\n\n12.5%\n\n\n\n\n\n\nReviesd: OK\n\n\n14\n\n\n19.44%\n\n\n2\n\n\n2.78%\n\n\n3\n\n\n4.17%\n\n\n15\n\n\n20.83%\n\n\n\n\nStudy 3a\n\n\nInitial: Wrong\n\n\n54\n\n\n75%\n\n\n67\n\n\n93.06%\n\n\n61\n\n\n84.72%\n\n\n48\n\n\n66.67%\n\n\n\n\n\n\nInitial: Neutral\n\n\n6\n\n\n8.33%\n\n\n3\n\n\n4.17%\n\n\n7\n\n\n9.72%\n\n\n10\n\n\n13.89%\n\n\n\n\n\n\nInitial: OK\n\n\n12\n\n\n16.67%\n\n\n2\n\n\n2.78%\n\n\n4\n\n\n5.56%\n\n\n14\n\n\n19.44%\n\n\n\n\n\n\nRevised: Wrong\n\n\n53\n\n\n73.61%\n\n\n67\n\n\n93.06%\n\n\n57\n\n\n79.17%\n\n\n43\n\n\n59.72%\n\n\n\n\n\n\nRevised: Neutral\n\n\n11\n\n\n15.28%\n\n\n4\n\n\n5.56%\n\n\n12\n\n\n16.67%\n\n\n15\n\n\n20.83%\n\n\n\n\n\n\nReviesd: OK\n\n\n8\n\n\n11.11%\n\n\n1\n\n\n1.39%\n\n\n3\n\n\n4.17%\n\n\n14\n\n\n19.44%\n\n\n\n\nStudy 3b\n\n\nInitial: Wrong\n\n\n81\n\n\n80.2%\n\n\n85\n\n\n84.16%\n\n\n71\n\n\n70.3%\n\n\n66\n\n\n65.35%\n\n\n\n\n\n\nInitial: Neutral\n\n\n9\n\n\n8.91%\n\n\n13\n\n\n12.87%\n\n\n20\n\n\n19.8%\n\n\n14\n\n\n13.86%\n\n\n\n\n\n\nInitial: OK\n\n\n11\n\n\n10.89%\n\n\n3\n\n\n2.97%\n\n\n10\n\n\n9.9%\n\n\n21\n\n\n20.79%\n\n\n\n\n\n\nRevised: Wrong\n\n\n87\n\n\n86.14%\n\n\n82\n\n\n81.19%\n\n\n73\n\n\n72.28%\n\n\n59\n\n\n58.42%\n\n\n\n\n\n\nRevised: Neutral\n\n\n10\n\n\n9.9%\n\n\n15\n\n\n14.85%\n\n\n19\n\n\n18.81%\n\n\n17\n\n\n16.83%\n\n\n\n\n\n\nReviesd: OK\n\n\n4\n\n\n3.96%\n\n\n4\n\n\n3.96%\n\n\n9\n\n\n8.91%\n\n\n25\n\n\n24.75%\n\n\n\n\n\n\nTwenty two of the 31 participants (70.97%) produced a dumbfounded response (admission of having no reasons; or the use of an unsupported declaration as a justification for a judgement, with a failure to provide any alternative reason when the unsupported declaration was questioned) at least once. Examples of such responses included “It just seems wrong and I cannot explain why, I don’t know”, “because I just think it’s wrong, oh God, I don’t know why, it’s just [pause] wrong”. Table 2 shows the number, and percentage, of participants who displayed dumbfounded responses and non-dumbfounded responses for each dilemma. The rates of each type of dumbfounded response are also displayed. Figure 1 shows the percentage of participants displaying dumbfounded responses for each dilemma. Table 3 shows the responses to the questionnaires presented between dilemmas.\n\n\nTable 2\n\n\nObserved frequency and percentage of each of the responses: dumbfounded, nothing wrong, and reasons provided\n\n\n\n\n\n\n\n\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\n\n\n\n\nStudy 1\n\n\nNothing wrong\n\n\n6\n\n\n19.35%\n\n\n8\n\n\n25.81%\n\n\n11\n\n\n35.48%\n\n\n8\n\n\n25.81%\n\n\n\n\n\n\nDumbfounded\n\n\n0\n\n\n0%\n\n\n11\n\n\n35.48%\n\n\n18\n\n\n58.06%\n\n\n3\n\n\n9.68%\n\n\n\n\n\n\n(admissions)\n\n\n0\n\n\n0%\n\n\n8\n\n\n25.81%\n\n\n10\n\n\n32.26%\n\n\n3\n\n\n9.68%\n\n\n\n\n\n\n(declarations)\n\n\n0\n\n\n0%\n\n\n3\n\n\n9.68%\n\n\n8\n\n\n25.81%\n\n\n0\n\n\n0%\n\n\n\n\n\n\nReasons\n\n\n25\n\n\n80.65%\n\n\n12\n\n\n38.71%\n\n\n2\n\n\n6.45%\n\n\n20\n\n\n64.52%\n\n\n\n\nStudy 2\n\n\nNothing wrong\n\n\n8\n\n\n11.11%\n\n\n4\n\n\n5.56%\n\n\n2\n\n\n2.78%\n\n\n10\n\n\n13.89%\n\n\n\n\n\n\nDumbfounded\n\n\n45\n\n\n62.5%\n\n\n46\n\n\n63.89%\n\n\n54\n\n\n75%\n\n\n45\n\n\n62.5%\n\n\n\n\n\n\nReasons\n\n\n19\n\n\n26.39%\n\n\n22\n\n\n30.56%\n\n\n16\n\n\n22.22%\n\n\n17\n\n\n23.61%\n\n\n\n\nStudy 3a\n\n\nNothing wrong\n\n\n14\n\n\n19.44%\n\n\n4\n\n\n5.56%\n\n\n12\n\n\n16.67%\n\n\n15\n\n\n20.83%\n\n\n\n\n(critical slide)\n\n\nDumbfounded\n\n\n13\n\n\n18.06%\n\n\n14\n\n\n19.44%\n\n\n18\n\n\n25%\n\n\n14\n\n\n19.44%\n\n\n\n\n\n\nReasons\n\n\n45\n\n\n62.5%\n\n\n54\n\n\n75%\n\n\n42\n\n\n58.33%\n\n\n43\n\n\n59.72%\n\n\n\n\nStudy 3a\n\n\nNothing wrong\n\n\n14\n\n\n19.44%\n\n\n4\n\n\n5.56%\n\n\n12\n\n\n16.67%\n\n\n15\n\n\n20.83%\n\n\n\n\n(coded)\n\n\nDumbfounded\n\n\n19\n\n\n26.39%\n\n\n21\n\n\n29.17%\n\n\n31\n\n\n43.06%\n\n\n22\n\n\n30.56%\n\n\n\n\n\n\nReasons\n\n\n39\n\n\n54.17%\n\n\n47\n\n\n65.28%\n\n\n29\n\n\n40.28%\n\n\n35\n\n\n48.61%\n\n\n\n\nStudy 3b\n\n\nNothing wrong\n\n\n21\n\n\n20.79%\n\n\n10\n\n\n9.9%\n\n\n31\n\n\n30.69%\n\n\n24\n\n\n23.76%\n\n\n\n\n(critical slide)\n\n\nDumbfounded\n\n\n12\n\n\n11.88%\n\n\n19\n\n\n18.81%\n\n\n16\n\n\n15.84%\n\n\n16\n\n\n15.84%\n\n\n\n\n\n\nReasons\n\n\n68\n\n\n67.33%\n\n\n72\n\n\n71.29%\n\n\n54\n\n\n53.47%\n\n\n61\n\n\n60.4%\n\n\n\n\nStudy 3b\n\n\nNothing wrong\n\n\n21\n\n\n20.79%\n\n\n10\n\n\n9.9%\n\n\n31\n\n\n30.69%\n\n\n24\n\n\n23.76%\n\n\n\n\n(coded)\n\n\nDumbfounded\n\n\n16\n\n\n15.84%\n\n\n30\n\n\n29.7%\n\n\n28\n\n\n27.72%\n\n\n22\n\n\n21.78%\n\n\n\n\n\n\nReasons\n\n\n64\n\n\n63.37%\n\n\n61\n\n\n60.4%\n\n\n42\n\n\n41.58%\n\n\n55\n\n\n54.46%\n\n\n\n\n\n\n\n\nRates of observed dumbfounding for each scenario across each study.\n\n\n\nTable 3\n\n\nResponses to post-discussion questionnaire questions\n\n\n\n\n\n\nStudy\n\n\nQuestion\n\n\nHeinz\n\n\nCannibal\n\n\nIncest\n\n\nTrolley\n\n\n\n\n\n\nStudy 1\n\n\nChanged mind\n\n\n2.87\n\n\n3.40\n\n\n2.63\n\n\n2.60\n\n\n\n\n\n\nConfidence\n\n\n5.30\n\n\n4.77\n\n\n5.40\n\n\n5.07\n\n\n\n\n\n\nConfused\n\n\n3.00\n\n\n3.67\n\n\n3.33\n\n\n3.70\n\n\n\n\n\n\nIrritated\n\n\n3.00\n\n\n3.33\n\n\n3.13\n\n\n3.37\n\n\n\n\n\n\n‘Gut’\n\n\n5.23\n\n\n5.20\n\n\n4.97\n\n\n5.07\n\n\n\n\n\n\n‘Reason’\n\n\n4.83\n\n\n4.40\n\n\n4.43\n\n\n4.77\n\n\n\n\n\n\nGut minus Reason\n\n\n0.40\n\n\n0.80\n\n\n0.53\n\n\n0.30\n\n\n\n\nStudy 2\n\n\nConfidence\n\n\n6.10\n\n\n5.86\n\n\n5.62\n\n\n5.26\n\n\n\n\n\n\nConfused\n\n\n2.40\n\n\n3.08\n\n\n4.14\n\n\n3.17\n\n\n\n\n\n\nIrritated\n\n\n4.58\n\n\n4.68\n\n\n4.32\n\n\n4.28\n\n\n\n\n\n\n‘Gut’\n\n\n5.29\n\n\n5.54\n\n\n5.82\n\n\n4.96\n\n\n\n\n\n\n‘Reason’\n\n\n4.89\n\n\n5.19\n\n\n4.89\n\n\n4.93\n\n\n\n\n\n\nGut minus Reason\n\n\n0.40\n\n\n0.35\n\n\n0.93\n\n\n0.03\n\n\n\n\nStudy 3a\n\n\nChanged mind\n\n\n2.38\n\n\n1.67\n\n\n2.00\n\n\n2.00\n\n\n\n\n\n\nConfidence\n\n\n5.22\n\n\n5.50\n\n\n5.38\n\n\n4.81\n\n\n\n\n\n\nConfused\n\n\n2.75\n\n\n2.96\n\n\n3.25\n\n\n2.89\n\n\n\n\n\n\nIrritated\n\n\n3.94\n\n\n4.64\n\n\n4.07\n\n\n3.60\n\n\n\n\n\n\n‘Gut’\n\n\n4.78\n\n\n5.44\n\n\n5.44\n\n\n4.92\n\n\n\n\n\n\n‘Reason’\n\n\n5.07\n\n\n5.26\n\n\n5.11\n\n\n5.06\n\n\n\n\n\n\nGut minus Reason\n\n\n-0.29\n\n\n0.18\n\n\n0.33\n\n\n-0.14\n\n\n\n\nStudy 3b\n\n\nChanged mind\n\n\n1.74\n\n\n1.60\n\n\n1.57\n\n\n1.83\n\n\n\n\n\n\nConfidence\n\n\n5.78\n\n\n6.16\n\n\n5.81\n\n\n5.36\n\n\n\n\n\n\nConfused\n\n\n2.06\n\n\n2.07\n\n\n2.12\n\n\n2.22\n\n\n\n\n\n\nIrritated\n\n\n4.42\n\n\n4.01\n\n\n3.56\n\n\n3.39\n\n\n\n\n\n\n‘Gut’\n\n\n4.42\n\n\n4.43\n\n\n4.47\n\n\n4.01\n\n\n\n\n\n\n‘Reason’\n\n\n5.46\n\n\n5.69\n\n\n5.26\n\n\n5.58\n\n\n\n\n\n\nGut minus Reason\n\n\n-1.04\n\n\n-1.27\n\n\n-0.79\n\n\n-1.57\n\n\n\n\n\n\nIn line with the original study (Haidt et al., 2000), the videos were also coded, by the primary researcher, across a range of measures. Haidt, Björklund, and Murphy (2000) report differences, between intuition and reasoning scenarios. They do not, however, report comparisons between participants identified as dumbfounded and participants not identified as dumbfounded. The current research, aiming to identify measurable indicators of dumbfounding, categorised participants as dumbfounded according to the two types of verbal responses (admissions and unsupported declaration) and compared these groups with participants who were not identified as dumbfounded, across a range of measures. There were two stages in this analysis. Firstly, all participants identified as dumbfounded were compared against participants who provided reasons only. Secondly, participants identified as dumbfounded were grouped according to type of dumbfounded response, and participants who did not rate the behaviour as wrong were also included in the analysis.\n\n\nJudgement variables reported by Haidt, Björklund, and Murphy (2000) included the length of time until the first argument, the length of time until the first evaluation, the length of time between the first evaluation and the first argument. The current research reports the same judgement variables.\n\n\nA range of “argument variables” were also reported. Identifying specific objectively verifiable measurable indicators for some of the “argument variables” reported by Haidt, Björklund, and Murphy (2000) was problematic (e.g., “dead-ends”, “argument kept”, “argument dropped”). The current research coded each verbal utterance according to a relevance for forming an argument. As such some of the argument variables reported by Haidt, Björklund, and Murphy (2000) are not reported here in the same way, however, related measures are reported.\n\n\nParalinguistic variables reported by Haidt, Björklund, and Murphy (2000) include frequency (per minute) of: “ums, uhs, hmms”, “turns with laughter”, “turns with face touch”, “doubt faces”, and “turns with pen fiddle”. As with the argument variables, the coding of the non-verbal/paralinguistic responses also varies slightly from what was reported by Haidt, Björklund, and Murphy (2000). We coded for both verbal hesitations (“um/em/uh”) and non-verbal hesitations/stuttering. “Turns” was coded independently of other behaviours as changing position. Laughter was coded for independently of changing position. The coding of hands touching the self was not limited to the face. Participants did not have pens to fiddle with, however we coded for generic fidgeting. The term “doubt faces” presented as problematic to code for rigorously across different individuals. As such, two distinctive and opposing facial expressions were coded for: smiling and frowning.\n\n\nDumbfounded versus reasons\n\nFifty nine cases of participants providing reasons, were compared with 32 cases of dumbfounded responding. There was no difference in time until first judgement between the dumbfounded group, (M = 14.89, SD = 20.41) and the group who provided reasons (M = 15.19, SD = 40.54), p = .969. Similarly, there was no difference in time until first argument between the dumbfounded group, (M = 39.20, SD = 28.90) and the group who provided reasons (M = 30.49, SD = 32.30), F(1, ,, , 81) = 1.42, p = .237, partial ()2 = .017. There was no difference in time from first judgement to time of first argument between the dumbfounded group, (M = 20.60, SD = 36.76) and the group who provided reasons (M = 15.65, SD = 46.42), p = .634.\n\n\nThere was a significant difference in frequency (per minute) of utterances whereby participants were working towards a reason between the dumbfounded group, (M = 1.47, SD = 1.45) and the group who provided reasons (M = 2.70, SD = 1.53), F(1, ,, , 89) = 13.82, p &lt; .001, partial ()2 = .134. There was no difference in frequency (per minute) of irrelevant arguments between the dumbfounded group, (M = 1.03, SD = .74) and the group who provided reasons (M = .86, SD = .77), F(1, ,, , 89) = 1.05, p = .308, partial ()2 = .012. There was a significant difference in frequency (per minute) of expressions of doubt between the dumbfounded group, (M = .63, SD = .65) and the group who provided reasons (M = .31, SD = .58), F(1, ,, , 89) = 5.87, p = .017, partial ()2 = .062.\n\n\nA one-way ANOVA revealed a significant difference in number of times per minute participants laughed between the dumbfounded group, (M = 2.81, SD = 2.84) and the group who provided reasons (M = 1.18, SD = 1.25), F(1, ,, , 89) = 14.35, p &lt; .001, partial ()2 = .139. Similarly, a one-way ANOVA revealed a significant difference relative amount of time spent smiling (as a proportion of the total time spent on the given scenario) between the dumbfounded group, (M = .32, SD = .15) and the group who provided reasons (M = .16, SD = .14), F(1, ,, , 89) = 25.24, p &lt; .001, partial ()2 = .221. Consistent with the results reported by Haidt, Björklund, and Murphy (2000), a series of one-way ANOVAs revealed no differences in verbal hesitations, F(1, ,, , 89) = 2.35, p = .129, partial ()2 = .026, non-verbal hesitations, p = .074, changing posture, p = .485, hands on the self, p = .864, frowning, p = .958, and fidgeting, F(1, ,, , 89) = 1.66, p = .201, partial ()2 = .018. A one-way ANOVA revealed a significant difference relative amount of time spent in silence (as a proportion of the total time spent on the given scenario) between the dumbfounded group, (M = .14, SD = .08) and the group who provided reasons (M = .09, SD = .06), F(1, ,, , 89) = 9.72, p = .002, partial ()2 = .098.\n\n\nFrom the above analysis, it appears that, working towards reasons, expressions of doubt, laughter, smiling, and silence were the only measures that varied significantly depending on whether a person was identified as dumbfounded or provided reasons. Having identified differences between dumbfounded participants and participants providing reasons, the following analysis investigates if there are differences depending the type of dumbfounded response provided. participants who did not rate the behaviour as wrong are also included in the following analysis.\n\n\n\nVariation between different types of dumbfounded responses\n\nFour groups, based on overall reaction to scenarios, were identified: participants who did not rate the behaviour as wrong, participants who provided reasons, participants who provided unsupported declarations, and participants who admitted to not having reasons.\n\n\nA one-way ANOVA revealed a significant difference in relative frequency of utterances whereby participants were working towards a reason depending on overall reaction to scenarios, F(3, ,, , 120) = 7.54, p &lt; .001, partial ()2 = .159. Tukey’s post-hoc pairwise comparison revealed that participants who provided reasons were identified as working towards a reason significantly more frequently (M = 2.70, SD = 1.53) than participants who did not rate the behaviour as wrong (M = 1.76, SD = 1.48), p = .021, and more frequently than participants who provided unsupported declarations as justifications (M = .64, SD = .72), p &lt; .001. There was no difference between participants who admitted to not having reasons (M = 1.90, SD = 1.56) and any of the other groups. A one-way ANOVA revealed no significant difference in relative frequency of expressions of doubt depending on overall reaction to scenarios, F(3, ,, , 120) = 2.17, p = .096, partial ()2 = .051.\n\n\nA one-way ANOVA revealed a significant difference in relative frequency laughter depending on overall reaction to scenarios, F(3, ,, , 120) = 8.27, p &lt; .001, partial ()2 = .171. Tukey’s post-hoc pairwise comparison revealed that participants who admitted to not having reasons laughed significantly more frequently (M = 2.41, SD = 2.00), than participants who provided reasons (M = 1.18, SD = 1.25), p = .039, and more frequently than participants who provided did not rate the behaviour as wrong (M = .97, SD = 1.29), p = .025. Similarly, participants who provided unsupported declarations laughed significantly more frequently (M = 3.57, SD = 4.00), than participants who provided reasons, p &lt; .001, and more frequently than participants who did not rate the behaviour as wrong, p &lt; .001. There was no difference between participants who provided reasons, and participants who did not rate the behaviour as wrong p = .951. Interestingly, there was no difference between participants who admitted to not having reasons and participants who provided unsupported declarations, p = .305.\n\n\nA similar pattern of results was found for time spent smiling. A one-way ANOVA revealed a significant difference in relative time spent smiling depending on overall reaction to scenarios, F(3, ,, , 120) = 9.97, p &lt; .001, partial ()2 = .200. Tukey’s post-hoc pairwise comparison revealed that participants who admitted to not having reasons spent significantly more time smiling (M = .33, SD = .14), than participants who provided reasons (M = .16, SD = .14), p &lt; .001, and more time smiling than participants who provided did not rate the behaviour as wrong (M = .16, SD = .13), p &lt; .001. Participants who provided unsupported declarations spent significantly more time smiling (M = .31, SD = .17), than participants who provided reasons, p = .008, and participants who did not rate the behaviour as wrong, p = .014. There was no difference between participants who provided reasons, and participants who did not rate the behaviour as wrong, p = 1.000. Again, there was no difference between participants who admitted to not having reasons and participants who provided unsupported declarations, p = .996.\n\n\nA one-way ANOVA revealed a significant difference in relative amount of time spent in silence depending on overall reaction to scenarios, F(3, ,, , 120) = 3.31, p = .023, partial ()2 = .076. Mean proportion of interview time spent in silence are as follows: participants providing reasons, M = .09, SD = .06; participants not rating the behaviour as wrong, M = .12, SD = .07; participants admitting to not having reasons, M = .14, SD = .09; and participants providing unsupported declarations, M = .14, SD = .05. Tukey’s post-hoc pairwise comparison did not reveal any significant differences between specific groups.\n\n\n\nFurther analyses\n\nAn exploratory analysis revealed no association between number of times dumbfounded and score on either measures from the MLQ: Presence, r(31) = 0.74, p = .466, or Search, r(31) = 1.38, p = .179, or the Centrality of Religiosity Scale r(31) = 0.35, p = .726. There was no difference in observed rates of dumbfounded responses depending on the order of scenario presentation, χ2(6, N = 124) = 4.01, p = .676. Rates of dumbfounded responses varied depending on which moral dilemma was being discussed, χ2(6, N = 124) = 46.82, p &lt; .001. The highest rate of dumbfounding was recorded for Incest, with 18 of the 31 (58.06%) participants displaying dumbfounded responses. Eleven participants (35.48%) displayed dumbfounded responses for Cannibal and three participants (9.68%) displayed dumbfounded responses for Trolley. The lowest recorded rate of dumbfounded response was for the Heinz dilemma, with no participants resorting to unsupported declarations as justification or admitting to not having reasons for their judgement. This trend is generally consistent with that which emerged in the original study (with the exception of Trolley, which was not used in the original study). Furthermore, rates of dumbfounded responding varied depending on which type of moral scenario was being discussed. Heinz and Trolley, identified as reasoning scenarios, were contrasted against the intuition scenarios Incest and Cannibal. There was significantly more dumbfounded responding for the intuition scenarios (29 instances) than for the reasoning scenarios (3 instances), χ2(2, N = 124) = 38.17, p &lt; .001.\n\n\nThe aim of Study 1 was to examine the replicability of moral dumbfounding as identified by Haidt, Björklund, and Murphy (2000), and identify specific measurable responses that may be indicative of dumbfounding. The overall pattern of responses, and pattern of inter-scenario variability in responding resembled that observed in the original study. As such, Study 1 successfully replicated the findings of the original moral dumbfounding study (Haidt, Björklund, and Murphy 2000). Participants were identified as dumbfounded according to two specific measures, admissions of having no reasons, and unsupported declarations followed by a failure to provide reasons when questioned further. Both of these responses were accompanied by similar increases incidences of laughter, and time spent smiling, when compared to participants providing reasons, and participants not rating the behaviour as wrong. When taken together, these responses were also accompanied by more silence during the interview, when compared with participants who provided reasons. As such, it appears that identifying incidences of dumbfounding according to unsupported declarations or admissions of not having reasons largely capture dumbfounding as described by Haidt, Björklund, and Murphy (2000).\n\n\nStudy 1 provides evidence supporting the view that moral dumbfounding is a genuine phenomenon and can be elicited in an interview setting when participants are pressed to justify their judgements of particular moral scenarios. Two key limitations have been identified as a result of conducting studies in an interview setting. Firstly, conducting video-recorded interviews, and the accompanying analyses, is particularly labour intensive, which leads to a smaller sample size. The aims of the present research were to examine the replicability of dumbfounding, and to identify specific measurable indicators of dumbfounding. A sample size of thirty-one is not sufficient in fulfilling the first aim. Secondly, an interview setting introduces a social context that may influence the responses of participants, in that, participants may feel a social pressure to behave in a particular way (e.g., Royzman, Kim, and Leeman 2015). Alternative methods are required to examine dumbfounding with a larger sample, and whether it still occurs in the absence of the social pressure that is present in an interview setting. Two responses have been identified as indicators of dumbfounding. The degree to which each of these responses can be elicited in a setting other than an interview is investigated in Studies 2 and 3."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#method-1",
    "href": "publications/searching-for-moral-dumbfounding/index.html#method-1",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "",
    "text": "Participants and design\n\nStudy 3a was a frequency based, modified replication. The aim was to identify if dumbfounded responding could be evoked. All participants were presented with the same four moral vignettes. Results are primarily descriptive. Further analysis tested for differences in responding depending on the vignette, or type of vignette, presented.\n\n\nA sample of 72 participants (46 female, 26 male; Mage = 21.80, min = 18, max = 46, SD = 3.91) took part in this study. Participants were undergraduate students and postgraduate students from MIC. Participation was voluntary and participants were not reimbursed for their participation.\n\n\n\n\nProcedure and materials\n\nThe materials in this study were almost the same as in Study 2 with a change to the “dumbfounded” response option on the critical slide. Extra questions were included following each of the counter-arguments. On the critical slide, the unsupported declaration option was replaced with an admission of not having reasons (“It’s wrong but I can’t think of a reason”). Following each counter-argument, participants were asked if they (still) thought the behaviour was wrong, and if they had a reason for their judgement. There was also a revision to the question on the post-discussion questionnaire asking if participants had changed their judgements was changed: “did your judgement change?” with a binary “yes/no” response option reverted back to “how much did your judgement change?” with a seven point Likert scale response (as in Study 1). The same four dilemmas Heinz, Incest, Cannibal and Trolley (Appendix A) along with the same prepared counter arguments (Appendix B) as in Study 2 were used in Study 3a. Both the MLQ (Steger et al. 2008); and CRSi7 (Huber and Huber 2012) were also used. This study was conducted in a designated psychology computer lab in MIC and was administered entirely on individual computers using OpenSesame (Mathôt, Schreij, and Theeuwes 2012).\n\n\nParticipants were seated, given instructions, and allowed to begin the computer task. The four vignettes from Study 1 Heinz, Incest, Cannibal and Trolley (Appendix A) along with the same pre-prepared counter arguments (Appendix B) were used. Dumbfounding was measured using the critical slide. The updated critical slide contained a statement defending the behaviour and a question as to how the behaviour could be wrong (e.g., “Julie and Mark’s behaviour did not harm anyone, how can there be anything wrong with what they did?”) with three possible response options: ( a ) “There is nothing wrong”; ( b ) “It’s wrong, but I can’t think of a reason”; ( c ) “It’s wrong and I can provide a valid reason”. The order of these response options was randomised. Participants who selected ( c ) were required to provide a reason. The selecting of option ( b ), the admission of not having reasons, was taken to be a dumbfounded response. When participants had completed all questions relating to all four dilemmas they completed the same longer questionnaire as in Studies 1 and 2 containing the MLQ (Steger et al. 2008), the Centrality of Religiosity Scale (Huber and Huber 2012), and some questions relating to demographics. The entire study lasted approximately fifteen to twenty minutes."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-1",
    "href": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-1",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nParticipants who selected the unsupported declaration on the critical slide were identified as dumbfounded. Table 1 shows the ratings of the behaviours across each scenario. Table 2 shows the number, and percentage, of participants who displayed “dumbfounded” responses (identified as the selecting of an unsupported declaration) and non-dumbfounded responses for each dilemma. Figure 1 shows the percentage of participants displaying dumbfounded responses for each dilemma. Table 3 shows the responses to the questionnaires presented between dilemmas. The open-ended responses provided by participants who selected option ( c ) “It’s wrong and I can provide a valid reason” were analysed and coded, by the primary researcher, and unsupported declarations provided here were also identified as dumbfounded responses. Following this coding, one additional participant was identified as dumbfounded for Trolley. Sixty eight of the 72 participants (94%) selected the unsupported declaration at least once. There was no statistically significant difference in responses to the critical slide depending on the order of scenario presentation, χ2(6, N = 288) = 4.13, p = .659. There was no statistically significant difference in responses to the critical slide depending on scenario presented, χ2(6, N = 288) = 9.00, p = .173. Rates of dumbfounded responding did not vary with type of moral scenario (100 instances for intuition scenarios, 90 instances for reasoning scenarios) being discussed, χ2(2, N = 288) = 6.58, p = .037. Forty five participants (62.5%) selected the unsupported for Heinz. Forty six participants (63.89%) selected (or provided) the unsupported declaration for Cannibal and Trolley. Fifty four participants (75%) selected the unsupported declaration for Incest. There was no association between number of times dumbfounded and score on either measure on the Meaning and Life questionnaire; Presence r(72) = -0.44, p = .662, or Search, r(72) = 1.12, p = .268, or the Centrality of Religiosity Scale r(72) = 1.24, p = .220.\n\n\nThe most striking result from this study was the willingness of participants to select the unsupported declaration in response to a challenge to their judgement. This is inconsistent with what was found in in both Study 1 and in the original study by Haidt, Björklund, and Murphy (2000). In these studies, participants did not readily offer an unsupported declaration as justification for their judgement, rather it was a last resort following extensive cross-examining. The exceptionally high rates of dumbfounding observed in Study 2 do not appear to be representative of the phenomenon more generally. There is, therefore, clearly a difference between offering an unsupported declaration as a justification for a judgement during an interview and selecting an unsupported declaration from a list of possible response options during a computerised task. It is possible that, during the interview, participants experienced a social pressure to successfully justify their judgement. This social pressure may also have made participants were more aware of the illegitimacy of using an unsupported declaration as a justification for their judgement. It is also possible that, seeing it written down as a possible answer legitimises selecting it as a justification for the judgement. The unsupported declaration does not provide an acceptable answer to the question on the critical slide, however, its presence in the list of possible response options may imply to participants that it is an acceptable answer, particularly if they do not put too much thought into it. By selecting the unsupported declaration participants can move quickly along to the next stage in the study without necessarily acknowledging any inconsistency in their reasoning, avoiding potentially dissonant cognitions (e.g., Case et al. 2005; Harmon-Jones and Harmon-Jones 2007; see also Heine, Proulx, and Vohs 2006). Selecting the unsupported declaration may also allow the participant to proceed without expending effort trying to think of reasons for their judgement beyond the intuitive justifications that had already been de-bunked.\n\n\nRates of dumbfounded responding in Study 2 were higher than expected. Possible reasons for this could be ( a ) reduced social pressure to appear to have reasons for judgements; ( b ) a failure of participants to comprehend that the unsupported declaration does not provide a logically justifiable response to the question asked in the critical slide; ( c ) the apparent legitimising of the unsupported declaration by its inclusion in the list of possible response options; or (d) the selecting by participants of an “easy way out” option without thinking about it fully (through carelessness/laziness/eagerness to move on to a less taxing task). It appears that the selecting of unsupported declarations is not an accurate measure of dumbfounding. In Study 1, participants were only identified as dumbfounded based on the providing of an unsupported declaration if they subsequently failed to provide further reasons when the unsupported declaration was questioned. However, in some cases, participants who provided unsupported declarations were not identified as dumbfounded, based on subsequent responses. A follow up analysis of the interview data revealed that 23 participants provided an unsupported declaration and proceeded to provide reasons for at least one of their judgements; a further six participants provided an unsupported declaration and proceeded to revise their judgement at least once. A stricter measure of dumbfounding, one by which participants are required to explicitly acknowledge a state of dumbfoundedness is necessary to address the issues with the selecting of an unsupported declaration that may have led to the unusually high rates of dumbfounding observed in Study 2."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#method-2",
    "href": "publications/searching-for-moral-dumbfounding/index.html#method-2",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "",
    "text": "Participants and design\n\nStudy 3b was a frequency based, modified replication. The aim was to identify if dumbfounded responding could be evoked. All participants were presented with the same four moral vignettes. Results are primarily descriptive. Further analysis tested for differences in responding depending on the vignette, or type of vignette, presented.\n\n\nA sample of 101 participants (53 female, 47 male; Mage = 36.58, min = 18, max = 69, SD = 12.45) took part in this study. Participants were recruited online through MTurk (Amazon Web Services Inc. 2016). Participation was voluntary and participants were paid 0.70 US dollars for their participation. Participants were recruited from English speaking countries or from countries where residents generally have a high level of English (e.g., The Netherlands, Denmark, Sweden). Location data for individual participants was not recorded, however, based on other studies, using the same selection criteria, it is likely that 90% of the sample was from the United States.\n\n\n\n\nProcedure and materials\n\nThe materials in this study were almost the same as in Study 3a, however, a different software package was used to present the materials and collect the responses. OpenSesame (Mathôt, Schreij, and Theeuwes 2012) was replaced with Questback (Unipark 2013)in order to facilitate online data collection. This meant that the recording of responses changed from keyboard input to mouse input. It also allowed for multiple questions to be displayed on the screen at the same time. Other than these changes, the materials were the same as in Study 3a.\n\n\nThe computer task in Study 3b was much the same as Study 3a. The four vignettes from Study 1: Heinz, Incest, Cannibal, and Trolley (Appendix A) along with the same pre-prepared counter arguments (Appendix B). Dumbfounding was measured using the critical slide.\n\n\nThe critical slide contained a statement defending the behaviour and a question as to how the behaviour could be wrong, with three possible response options: ( a ) “There is nothing wrong”; ( b ) “It’s wrong but I can’t think of a reason”; ( c ) “It’s wrong and I can provide a valid reason”. Participants who selected ( c ) were required to provide a reason. The order of these response options was randomised. When participants had completed all questions relating to all four dilemmas they completed the same longer questionnaire as in Studies 1 and 2 containing the Meaning and Life questionnaire (Steger et al. 2008), the Centrality of Religiosity Scale (Huber and Huber 2012), and some questions relating to demographics. The entire study lasted approximately fifteen to twenty minutes."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-2",
    "href": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-2",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nParticipants who selected the admission of not having reasons on the critical slide (option b) were identified as dumbfounded. Forty of the 72 participants (56%) selected the admission of not having reasons at least once. Table 1 shows the ratings of the behaviours across each scenario. Table 2 and Figure 1 show the percentage of participants displaying dumbfounded responses for each dilemma. Table 3 shows the responses to the questionnaires presented between dilemmas. Again there was no statistically significant difference in responses to the critical slide depending on the order of scenario presentation, χ2(6, N = 288) = 0.61, p = .996. There was no difference in responses to the critical slide depending on scenario, χ2(6, N = 288) = 9.6, p = .142, , or, type of scenario (32 instances for intuition scenarios, 27 instances for reasoning scenarios), χ2(2, N = 288) = 4.53, p = .104. Thirteen participants (18.06%) selected the admission of having no reasons for Heinz. Fourteen participants (19.44%) selected the admission of not having reasons for Cannibal and Trolley. Eighteen participants (25%) selected the admission of not having reasons for Incest.\n\n\nThe replacing of an unsupported declaration with an admission of having no reasons led to substantially lower rates of dumbfounding than observed in Study 2. As such, it appears that the issues associated with the selecting of an unsupported declaration have been addressed in Study 3a. However, the rates of dumbfounding observed for Incest and Cannibal in Study 3a were considerably lower than those observed in Study 1. This suggests the revised measure may be too strict, measuring only open admissions of not having reasons, but not accounting for a failure to provide reasons. As in the first computerised task, participants who selected “It’s wrong and I can provide a valid reason” were then required to provide a reason. In order to provide a measure of a failure to provide reasons, these responses were analysed and coded, by the primary researcher. Those containing unsupported declarations were taken as evidence for a failure to provide a reason and identified as dumbfounded responses.\n\n\nDuring the coding, another class of dumbfounded response was identified. Participants occasionally provided undefended tautological responses as justification for their judgements, whereby they simply named or described the behaviour in the scenario as justification for their judgement (e.g., “They are related”, “Because it is canibalism” [typographical error in response]). These responses may be viewed as largely equivalent to unsupported declarations (e.g., Mallon and Nichols 2011). In Study 1, they were not identified as dumbfounded responses, because when provided in an interview setting, they were always followed by further questioning. This further questioning could lead to two possible responses: ( a ) a dumbfounded response (unsupported declaration or an admission of not having reasons) or ( b ) an alternative reason. A computerised task does not allow for a follow-up probe to encourage participants to elaborate on such responses. Participants were not placed under time pressure and could articulate and review their typed reason at their own pace. It is reasonable to expect then, that, if participants did have a valid reason for their judgement, they would have provided it along with, or instead of, the undefended tautological response. As such, an undefended tautological reason appears to be evidence of a failure to identify reasons . For this reason, these undefended tautological reasons were also coded as dumbfounded responses, along with the unsupported declarations.\n\n\nTable 2 and Figure 2 show the number and percentage of dumbfounded responses when the coded string responses are included in the analysis. When the coded string responses are included in the analysis, the number of participants displaying a dumbfounded response at least once increased from 40 (56%) to 57 (79%). Observed rates of dumbfounding increased for each scenario when the coded open-ended responses were included, with 19 participants (26.39%) appearing to be dumbfounded by Heinz, 21 (29.17%) by Cannibal, 31 (43.06%) by Incest, and 22 (30.56%) apparently dumbfounded by Trolley. Still, rates of dumbfounded responding did not vary with type of moral scenario (52 instances for intuition scenarios, 41 instances for reasoning scenarios) being discussed, χ2(1, N = 288) = 1.59, p = .208. There was no association between number of times dumbfounded and score on either measure on the Meaning and Life questionnaire; Presence r(72) = 0.82, p = .413, or Search, r(72) = 0.07, p = .945, or the Centrality of Religiosity Scale r(72) = 1.29, p = .201.\n\n\n\n\nRates of observed dumbfounding for each scenario across each study, including coded string responses.\n\n\n\nWhen the coded open-ended responses were included in the analysis, the proportion of participants displaying a dumbfounded response at least once in Study 3a (79%) was much closer to that observed in the interview in Study 1 (74%) than before the open-ended responses were included (56%). The variation in observed rates of dumbfounding between dilemmas that was observed in the interview was not present in the computerised task. As such there remains a difference between the dumbfounding elicited during an interview and that elicited as part of a computerised task. However, it is clear that dumbfounded responses can be elicited as part of a computerised task. The participants in Studies 1, 2, and 3a were all college students (largely from the same institution) and as such, the following study investigated the phenomenon in a more diverse sample."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#method-3",
    "href": "publications/searching-for-moral-dumbfounding/index.html#method-3",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Method",
    "text": "Method\n\n\nParticipants and design\n\nStudy 3b was a frequency based, modified replication. The aim was to identify if dumbfounded responding could be evoked. All participants were presented with the same four moral vignettes. Results are primarily descriptive. Further analysis tested for differences in responding depending on the vignette, or type of vignette, presented.\n\n\nA sample of 101 participants (53 female, 47 male; Mage = 36.58, min = 18, max = 69, SD = 12.45) took part in this study. Participants were recruited online through MTurk (Amazon Web Services Inc. 2016). Participation was voluntary and participants were paid 0.70 US dollars for their participation. Participants were recruited from English speaking countries or from countries where residents generally have a high level of English (e.g., The Netherlands, Denmark, Sweden). Location data for individual participants was not recorded, however, based on other studies, using the same selection criteria, it is likely that 90% of the sample was from the United States.\n\n\n\n\nProcedure and materials\n\nThe materials in this study were almost the same as in Study 3a, however, a different software package was used to present the materials and collect the responses. OpenSesame (Mathôt, Schreij, and Theeuwes 2012) was replaced with Questback (Unipark 2013)in order to facilitate online data collection. This meant that the recording of responses changed from keyboard input to mouse input. It also allowed for multiple questions to be displayed on the screen at the same time. Other than these changes, the materials were the same as in Study 3a.\n\n\nThe computer task in Study 3b was much the same as Study 3a. The four vignettes from Study 1: Heinz, Incest, Cannibal, and Trolley (Appendix A) along with the same pre-prepared counter arguments (Appendix B). Dumbfounding was measured using the critical slide.\n\n\nThe critical slide contained a statement defending the behaviour and a question as to how the behaviour could be wrong, with three possible response options: ( a ) “There is nothing wrong”; ( b ) “It’s wrong but I can’t think of a reason”; ( c ) “It’s wrong and I can provide a valid reason”. Participants who selected ( c ) were required to provide a reason. The order of these response options was randomised. When participants had completed all questions relating to all four dilemmas they completed the same longer questionnaire as in Studies 1 and 2 containing the Meaning and Life questionnaire (Steger et al. 2008), the Centrality of Religiosity Scale (Huber and Huber 2012), and some questions relating to demographics. The entire study lasted approximately fifteen to twenty minutes."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-3",
    "href": "publications/searching-for-moral-dumbfounding/index.html#results-and-discussion-3",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Results and Discussion",
    "text": "Results and Discussion\n\nParticipants who selected the admission of not having reasons on the critical slide (option b) were identified as dumbfounded. Table1 shows the ratings of the behaviours across each scenario. Table 2 and Figure 1 show the percentage of participants displaying dumbfounded responses for each scenario. Table 3 shows the responses to the questionnaires presented between scenario. On this occasion there was a statistically significant difference in responses to the critical slide depending on the order of scenario presentation, χ2(6, N = 404) = 14.77, p = .022. The observed rates of dumbfounded responses were higher for the third scenario, however they went down again for the fourth scenario along with rates of selecting “nothing wrong”, meaning that the rates of participants providing reasons went up again for the fourth scenario. The higher rates of providing reasons observed for the fourth scenario presented means that this fluctuation is unlikely to be due to experimental fatigue, which was the primary reason for testing for order effects. There was also a difference in responses to the critical slide depending on scenario, χ2(6, N = 404) = 15.18, p = .019 with more people selecting “nothing wrong” for Incest and fewer people selecting “nothing wrong” for Cannibal. When dumbfounded responses are isolated and contrasted against other responses this difference is no longer present, χ2(3, N = 404) = 1.86, p = .602. Forty four participants (44%) selected the admission of not having reasons at least once. Twelve participants (11.88%) selected the admission of having no reasons for Heinz. Sixteen participants (15.84%) selected the admission of not having reasons for Incest and Trolley. Nineteen participants (18.81%) selected the admission of not having reasons for Cannibal.\n\n\nAs in Study 3a, participants who selected option ( c ) “It’s wrong and I can provide a valid reason”, were there then required to provide a reason through open-ended response. These open-ended responses were coded, by the primary researcher, for dumbfounded responses, again, identified as unsupported declarations or as undefended tautological responses. Table 2 and Figure 2 show the rates of observed dumbfounding when the coded open-ended responses were included in the analysis. As expected, the number of participants displaying a dumbfounded response at least once increased, from 44 (44%) to 57 (56%). Observed rates of dumbfounding increased for each scenario when the coded reasons were included with 16 participants (15.84%) appearing to be dumbfounded by Heinz, 30 (29.7%) by Cannibal, 28 (27.72%) by Incest, and 22 (21.78%) apparently dumbfounded by Trolley. Taking these revised rates of dumbfounding there was a no significant difference in rates of dumbfounded responding depending on scenario, χ2(3, N = 404) = 6.56, p = .087. There was however, significantly more dumbfounded responding for the intuition scenarios (58 instances) than for the reasoning scenarios (38 instances), χ2(1, N = 404) = 4.93, p = .026.\n\n\nThere was no association between number of times dumbfounded and score on either measure on the Meaning and Life questionnaire; Presence r(101) = -0.78, p = .436, or Search, r(101) = 0.63, p = .532, or the Centrality of Religiosity Scale r(101) = 0.44, p = .662. This is consistent with Studies 1, 2, and 3a. It appears that susceptibility to dumbfounding is not related to either measure."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#evaluating-each-measure-of-dumbfounding",
    "href": "publications/searching-for-moral-dumbfounding/index.html#evaluating-each-measure-of-dumbfounding",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Evaluating each Measure of Dumbfounding",
    "text": "Evaluating each Measure of Dumbfounding\n\nThe current research identifies moral dumbfounding as a rare demonstration of a separation between intuitions and reasons for these intuitions (e.g., Barsalou 2008, 2009, 2003; Crockett 2013; Cushman 2013). Two ways in which this separation may manifest were identified. Firstly participants may acknowledge that they do not have reasons for their judgements, admitting to not having reasons. Secondly, participants may fail to provide reasons when asked, providing responses that fail to answer the question they were asked. Two such responses were identified, unsupported declarations and tautological responses.\n\n\nMeasuring dumbfounding according to an admission of not having reasons only, in Studies 1, 3a and 3b (N = 204), 100 participants (49%) were identified as dumbfounded at least once. When a failure to provide reasons (taken as the providing of unsupported declarations in Study 1, and, unsupported declarations and tautological responses in Study 3) was included as a dumbfounded response, 136 participants (67%) were identified as dumbfounded at least once. When the selecting of an unsupported declaration (Study 2, N = 72) was included (N = 276), 204 participants, (74%) were identified as dumbfounded at least once.\n\n\nThe disparity in results between Study 2 and the other studies suggests that the selection of an unsupported declaration does not provide a good measure of moral dumbfounding. Participants in Studies 1, 3a, and 3b, recognised the illegitimacy unsupported declarations as justifications for their judgement, with the majority of participants avoided resorting to this type of response at all. The vast majority of participants appeared to be willing to ignore the illegitimacy of the response, with large numbers of participants selecting the unsupported declaration. While Study 2 did not identify a means to measure dumbfounding, these results are interesting, and may provide an insight into the cognitive processes that lead to dumbfounding.\n\n\nProviding an unsupported declaration is clearly different to selecting one from a list of possible responses. One possible explanation, is that dumbfounding is an aversive state, similar to experiencing a threat to meaning (Heine, Proulx, and Vohs 2006; Proulx and Inzlicht 2012), or cognitive dissonance (Cooper 2007; Festinger 1957; Harmon-Jones and Harmon-Jones 2007). The selecting of an unsupported declaration without deliberation allows participants to avoid or minimise the impact of this aversive state and move on. Providing an unsupported declaration involves more deliberation, making the illegitimacy of it more salient, reducing its effectiveness in avoiding the aversive state of dumbfoundedness. Furthermore, the relative attractiveness of these different responses to participants may be linked to social desirability (Chung and Monroe 2003; Latif 2000; Morris and McDonald 2013). Follow-up work could investigate these questions directly.\n\n\nThe explicit acknowledgement of an absence of reasons can be measured systematically by the selection of an admission of having no reasons. This is an unambiguous measure of moral dumbfounding, does not account for participants who fail to provide reasons. Measuring a failure to provide reasons, however, is more problematic. What is termed as a valid reason is subjective. The providing of unsupported declarations and tautological responses has been identified here as an indicator of a failure to provide reasons. This is grounded in discussions of dumbfounding in the wider literature (Haidt 2001; Mallon and Nichols 2011; Prinz 2005), and the theoretical framework adopted here. Evidence for equivalence of unsupported declarations and admissions of not having reasons was also found in Study 1 whereby both measures displayed similar variability in non-verbal behaviours when contrasted against participants who provided reasons, and participants who did not rate the behaviour as wrong. However, caution is advised in taking unsupported declarations as evidence for dumbfounding, particularly given the pattern of responses in Study 2, and that a number of participants in Study 1 who provided an unsupported declaration proceeded to provide reasons, or a revised judgement.\n\n\nThe current research identified two measures of dumbfounding. Limitations are associated with each. Relying on admissions of having no reasons only, provides an overly strict measure whereby a failure to provide reasons is not measured. Taking unsupported declarations (and tautological reasons) as a measure of dumbfounding may provide too broad a measure, risks identifying lazy or inattentive participants as dumbfounded. The providing of a type-written response as part of a computerised task requires effort, and the majority of participants avoid the use of unsupported declarations as justifications for their judgements. This suggests that those who provided unsupported declarations did so because they failed to identify alternative reason. It appears that the most practicable means to measure dumbfounding accurately requires each of the responses: providing/selecting admissions of not having reasons, and the providing of an unsupported declaration, to be accounted for. Participants providing either of these responses may be identified as dumbfounded."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#differences-between-scenarios",
    "href": "publications/searching-for-moral-dumbfounding/index.html#differences-between-scenarios",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Differences between Scenarios",
    "text": "Differences between Scenarios\n\nIn Study 1 we found that rates of dumbfounded responding varied depending on the scenario presented. Study 2 recorded high rates of dumbfounded responses for all scenarios. In Studies 3a and 3b, we observed low rates of dumbfounded responding for all scenarios. In Study 1 and Study 3b, we observed varying rates of dumbfounded responses depending on scenario type. When Studies 3a and 3b are analysed together this variation is still observed, with significantly more dumbfounded responses recorded for the intuition scenarios (110 instances) than for the reasoning scenarios (79 instances), χ2(1, N = 288) = 6.55, p = .010. However, this combined analysis may be skewed in favour of Study 3b, due to the larger sample size, 101 participants; Study 3a had only 72 participants. Further research and continued replication is needed to confirm the reliability of this finding. When the open-ended responses coded as tautological were included in the analysis of Studies 3a and 3b, the rates of dumbfounding appeared to be closer to those observed in Study 1.\n\n\nTable 2 and Figure 1 show the initial observed rates of dumbfounding for each study. Table 2 and Figure 2 show the revised rates of observed dumbfound responding in each study once the open-ended coded responses from Studies 3a and 3b are included. Rates of dumbfounding reported by Haidt, Björklund, and Murphy (2000) are also included for comparison. Study 2 was a primarily a pilot study, and, as discussed, the observed rates of dumbfounding do not appear to be representative of the phenomenon being studied, as such Study 2 is not included in Figure 2."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#differences-between-the-samples",
    "href": "publications/searching-for-moral-dumbfounding/index.html#differences-between-the-samples",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Differences between the Samples",
    "text": "Differences between the Samples\n\nThe trend in observed rates of dumbfounded responses, across the dilemmas, identified by Haidt, Björklund, and Murphy (2000) appears to also be present in Study 1 (Interview). There does not appear to be a difference between scenarios in the computerised tasks. When the open-ended responses are included, the rates of observed dumbfounding for Cannibal appear to be similar across all the studies included in Figure 2 (two interviews and two computerised tasks). The computerised tasks appear to have higher rates of dumbfounding for both Heinz and Trolley than the interviews. There is a large degree of variation in the observed rate of dumbfounding for Incest between the four studies.\n\n\nIncest recorded higher rates of dumbfounding than the other scenarios in both interview studies (Study 1 and Haidt, Björklund, and Murphy 2000) and, to some degree, in Study 3a, the computer task with a college sample. The rate of dumbfounding observed for Incest with the online sample, in Study 3b, is lower than that observed with the college sample in Study 3a and is also slightly lower than that observed for Cannibal in the online sample. This is surprising, in that, the Incest dilemma is the most commonly cited example (e.g., Haidt 2001; Prinz 2005; Royzman, Kim, and Leeman 2015), and, in Studies 1, 2, and 3a, is the most reliable for eliciting dumbfounding, consistently eliciting higher rates than the other dilemmas. Looking at the ratings of the behaviours in each dilemma for each study may provide some clue as to where this variation comes from. The online sample were less inclined to rate the behaviour in Incest as wrong relative to the participants in the other studies. The percentage of participants initially rating Incest as wrong for each study are as follows: Study 1: 83.87%; Study 2: 87.5%; Study 3a: 84.72%; Study 3b: 70.3%. Furthermore, on the critical slide, the proportion of participants who selected “nothing wrong” for Incest for Study 3b (30.69%; 31 participants) was nearly double the proportion that selected “nothing wrong” for Incest for Study 3a (16.67; 12 participants). When these participants are excluded from the analysis of Study 3b (see Table 4 and Figure 3), the percentage of participants appearing to be dumbfounded by Incest (22.86%; 16 participants; or 40%; 28 participants when open-ended responses are included; N = 70) exceeds the percentage of participants appearing to be dumbfounded by Cannibal (20.88%; 19 participants; or 32.97%; 30 participants when open-ended responses are included; N = 91). As such, it appears that the apparent uncharacteristically low rates of observed dumbfounding for Incest in Study 3b, when compared to Cannibal, may be due to the online sample being less inclined to rate the behaviour as morally wrong rather than a difference in this sample’s ability to provide justifications for their judgements to the two scenarios.\n\n\nTable 4\n\n\nPercentage of participants dumbfounded excluding participants who selected nothing wrong\n\n\n\n\n\n\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\nN\n\n\npercent\n\n\n\n\n\n\nStudy 1 (N = 31)\n\n\n0/25\n\n\n0%\n\n\n11/23\n\n\n47.83%\n\n\n18/20\n\n\n90%\n\n\n3/23\n\n\n13.04%\n\n\n\n\nStudy 2 (N = 72)\n\n\n45/64\n\n\n70.31%\n\n\n46/68\n\n\n67.65%\n\n\n54/70\n\n\n77.14%\n\n\n46/62\n\n\n74.19%\n\n\n\n\nStudy 3a (N = 72)\n\n\n19/58\n\n\n32.76%\n\n\n21/68\n\n\n30.88%\n\n\n31/60\n\n\n51.67%\n\n\n22/57\n\n\n38.6%\n\n\n\n\nStudy 3b (N = 101)\n\n\n16/80\n\n\n20%\n\n\n30/91\n\n\n32.97%\n\n\n28/70\n\n\n40%\n\n\n22/77\n\n\n28.57%\n\n\n\n\n\n\n\n\nFigure\n\n\n\nPercentage of dumbfounded responses when “nothing wrong” is excluded."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#intuition-versus-reasoning",
    "href": "publications/searching-for-moral-dumbfounding/index.html#intuition-versus-reasoning",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Intuition versus Reasoning",
    "text": "Intuition versus Reasoning\n\nHaidt, Björklund, and Murphy (2000) attribute the observed trend in dumbfounded responding to differences in type of scenario. They argue that Heinz is a “reasoning” scenario while Cannibal and Incest are “intuition” scenarios. Prinz (2005) suggests that these “intuition” scenarios have an emotional component, specifically that they elicit disgust, which leads to the judgement. Prinz argues that judgements grounded in disgust are more difficult to justify because they are grounded in emotion rather than reason. The variability between scenarios may be evidence for Haidt et al. prediction that judgements on the “intuition” scenarios would be more difficult to justify than the “reasoning” scenarios.\n\n\nStudy 1, the interview, was the only study to produce robust differences between the scenarios.5 The results of the computerised tasks may indicate that there is no difference between the reasoning scenarios and the intuition scenarios. Alternatively, this may have highlighted a difference between an interview and a computerised task that influences the way people make moral judgements.\n\n\nIt is possible that there exists a social influence in an interview setting that changes the way participants respond (e.g., Asch 1956; Sabini 1995; Staub 2013) and, that the interviewer may be seen as a person in authority, demanding justifications for judgements made (e.g., Milgram 1974). This may motivate participants to identify reasons to justify their judgements, leading to the suppression of dumbfounded responses. On the other hand, it may also motivate participants to heed the counter-arguments offered by the experimenter. This may lead to an interaction between scenario difficulty and social pressure to emerge, with the social pressure leading to fewer dumbfounded responses to the easier “reasoning” scenarios, but leading to more dumbfounded responses to the more difficult “intuition” scenarios. It may be the case that the rates of dumbfounding found in the computer tasks provide something of a crude baseline measure of participants’ initial perception of their own ability to justify their judgement of the scenario, having read the scenario and a number of counter-arguments. In the interview, these initial responses to the scenarios are distilled by the discussion with the experimenter to reflect the variation in difficulty between the scenarios."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#implications",
    "href": "publications/searching-for-moral-dumbfounding/index.html#implications",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Implications",
    "text": "Implications\n\nThe existence of moral dumbfounding has informed various theories of moral judgement either directly (e.g., Cushman, Young, and Greene 2010; Haidt 2001; Hauser, Young, and Cushman 2008; Prinz 2005) or indirectly (Crockett 2013; Cushman 2013; Greene 2008, 2013). The original demonstration of moral dumbfounding remains unpublished in peer reviewed form (Haidt, Björklund, and Murphy 2000) and has not been directly replicated. The studies presented here aimed to replicate and extend this original moral dumbfounding study (Haidt, Björklund, and Murphy 2000) and thus, assess the notion that moral dumbfounding is in fact a psychological phenomenon that can be consistently observed. Study 1 successfully replicated the original study. Study 2 piloted the use of a computer task and recorded unexpectedly high rates of dumbfounded responding. Possible reasons for this were identified and addressed in Studies 3a and 3b. Study 3a and 3b recorded more moderate rates of dumbfounding with two different samples. All three studies successfully elicited dumbfounded responding identified as ( a ) admissions of not having reasons; ( b ) use of unsupported declarations as justification of a judgement; or ( c ) use of undefended tautological response as justification for a judgement; however, differences remain between the interview in Study 1 and the computerised task in Studies 3a and 3b. Taking these responses to be indicators of a state of dumbfoundedness, it appears that moral dumbfounding can be evoked in face-to-face and online contexts. As such, the research presented here may be seen as more support for the existence of intuitionist theories of moral judgement (e.g., Cushman, Young, and Greene 2010; Greene 2008; Haidt 2001; Hauser, Young, and Cushman 2008; Prinz 2005) over rationalist theories (e.g., Kohlberg 1971; Topolski et al. 2013)."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#responding-to-criticisms",
    "href": "publications/searching-for-moral-dumbfounding/index.html#responding-to-criticisms",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Responding to Criticisms",
    "text": "Responding to Criticisms\n\nThe present research did not directly address the questions raised by Royzman, Kim, and Leeman (2015). Those researchers suggest that there are two main factors that lead participants to produce responses that appear to be indicative of dumbfounding. Firstly, they argue that dumbfounded responding occurs as a result of social pressure to avoid appearing “uncooperative” (Royzman, Kim, and Leeman 2015, 299), “inattentive” or “stubborn” (Royzman, Kim, and Leeman 2015, 310). However, recall that the original definition of dumbfounding, which Royzman et al., employ, refers to the “stubborn” maintenance of a judgement. This creates a paradoxical situation whereby presenting as stubborn (as part of a dumbfounded response) occurs as a result of an attempt to avoid appearing stubborn. Secondly, they claim that participants’ judgements can be attributed to either norm-based reasons, or reason of potential harm. This claim is tested by presenting participants with questions relating to norm-based reasons and harm-based reasons, and excluding participants from analysis, based on their responses to these questions. They showed that almost all participants who rated the behaviour as wrong also endorsed at least one of these reasons. When controlling for the endorsing of these reasons Royzman et al. report a dumbfounding estimate of 1/53 which they report to be “not significantly greater than 0/53 (z = 1.00, p = .32)” (Royzman, Kim, and Leeman 2015, 309) leading to the conclusion that, when controlling for norm-based reasons or harm-based reasons, moral dumbfounding does not occur. There are three main issues with the way this conclusion is reached.\n\n\nFirstly, the initial estimate of incidences of dumbfounding was 4/53 (7.55%). Based on the same calculations used by Royzman, Kim, and Leeman (2015), this estimate of 4/53 is significantly greater than 0/53, z = 2.0388386, p= .041. These four participants were then interviewed further, during which, the “inconsistencies” in participants’ “responses were pointed out directly” (Royzman, Kim, and Leeman 2015, 308). Following this interview, Royzman et al. were left with a dumbfounding estimate of 1/53 (which they claim is not significantly greater than 0/53).\n\n\nIt is surprising that, having made the claim that dumbfounding arises as a result of social pressure, providing convincing evidence for this claim required a follow up interview, in which participants are exposed to social pressure. Using the same logic employed by Royzman et al. it would not be surprising if participants revised their responses after being “advised to carefully review and, if appropriate, revise” their responses (Royzman, Kim, and Leeman 2015, 308). From this, it appears that incidences of dumbfounding can be reduced by changing the demands of the social situation. In effect, Royzman, Kim, and Leeman (2015) have shown that moral dumbfounding is sensitive to social pressure. Demanding consistency between judgement and the endorsing of principles that may be relevant for a judgement reduces incidences of dumbfounding, whereas demanding consistency between a judgement and information contained in the vignette leads to increased dumbfounding. This is not the same as their claim that moral dumbfounding is caused by social pressure. Furthermore, the role of social pressure in the reduced incidences of dumbfounding observed is not acknowledged.\n\n\nSecondly, following this interview, Royzman, Kim, and Leeman (2015) are still left with one participant who, by their own criteria, can be identified as dumbfounded (Royzman, Kim, and Leeman 2015, 308). No explanation for the responding of this participant is offered, and cannot be explained by the theoretical position adopted in the conclusion. It is argued that one participant from a sample of 53, is not significantly greater than 0/53, z = 1.00, p = .32. Disregarding this estimate of moral dumbfounding as not statistically significant, p = .32, avoids offering an explanation for a response that is inconsistent with the argument made in the paper.\n\n\nThirdly, and most importantly, the current research identifies dumbfounding as a rare demonstration of the separation between intuitions and reasons for these intuitions. Practical challenges to demonstrating this separation have already been identified: ( a ) post-hoc rationalisation and identification of reasons that are consistent with a judgement; ( b ) the possibility that the intuition emerged as a result of a well-rehearsed reasoned response. The work presented by Royzman, Kim, and Leeman (2015) may be viewed as a practical demonstration of this first challenge; helping participants identify reasons that are consistent with their judgement and providing an opportunity them to endorse these reasons. As previously noted, the endorsing of a reason does not imply that the reason contributed to the judgement. This view of moral dumbfounding presents two methodological considerations that need to be addressed before accepting the claim that judgements in the dumbfounding paradigm can be attributed to either norm-based reasons or harm-based reasons. The first relates to participants’ ability to articulate either harm-based or norm-based reasons. The second relates to the consistency with which these reasons guide judgements.\n\n\nFirstly, the final study reported by Royzman, Kim, and Leeman (2015) does not report whether or not participants who endorsed either norm-based reasons or harm-based reasons also articulated the same reason. The mere endorsing of a principle or reason does not provide evidence that this principle guided the making of a judgement. To illustrate this point, consider the following scenario:\n\n\n\nTwo friends (John and Pat) are bored one afternoon and trying to think of something to do. John suggests they go for a swim. Pat declines stating that it’s too much effort - to get changed, and then to get dried and then washed and dried again after; he says he’d rather do something that requires less effort. John agrees and adds “Oh yeah, and there’s that surfing competition on today so the place will be mobbed”. To which Pat replies “Yeah exactly!”.\n\n\n\nWhen John mentioned the surfing competition Pat immediately adopted it as another reason not to go for a swim however it is clear that this reason played no part in Pat’s original judgement. It is possible that in identifying other reasons that are consistent with a particular judgement researchers may falsely attribute the judgement made to these reasons. The studies described by Royzman, Kim, and Leeman (2015) do not sufficiently guard against the possibility of falsely attributing judgements to reasons endorsed, allowing for the possibility that some participants were falsely excluded from analysis. One way to avoid the false exclusion of participants would be to include an open-ended string response option immediately after the presenting of the vignette, in which participants are invited to provide the reason(s) for their judgement. Participants are then only excluded from analysis if they both articulated and endorsed a given principle.\n\n\nSecondly, consider the harm-based reasons, or the application of the harm principle. Royzman, Kim, and Leeman (2015) argue that if participants do not believe that no harm came from the actions of Julie and Mark then concerns of harm may be considered a legitimate reason for judging the behaviour as wrong. Essentially, they have identified the harm principle as “it is wrong for two people to engage in an activity whereby harm may occur”. Royzman, Kim, and Leeman (2015) argue that the application of this principle provides participants with a legitimate reason for their judgements. If this principle is guiding the judgements of participants, then this principle should be applied consistently across differing contexts. Royzman do not demonstrate that the participants in their sample consistently apply this principle across differing contexts (e.g., contact sports/boxing).\n\n\nTwo indicators, measuring dumbfounding by differing standards, have been identified here: admissions of not having reasons, demonstrating an explicit acknowledgement of the absence of reasons; and unsupported declarations, demonstrating a failure to provide reasons when asked. The materials and measures developed here can be used in follow-up work in order address the methodological issues identified in the work of Royzman, Kim, and Leeman (2015) and assess the strength of the concerns they identified in a more rigorous manner."
  },
  {
    "objectID": "publications/searching-for-moral-dumbfounding/index.html#limitations-and-future-directions",
    "href": "publications/searching-for-moral-dumbfounding/index.html#limitations-and-future-directions",
    "title": "Searching for Moral Dumbfounding: Identifying Measurable Indicators of Moral Dumbfounding",
    "section": "Limitations and Future Directions",
    "text": "Limitations and Future Directions\n\nThe current research recorded variability between the different studies that remains unexplained. The interview recorded variation in responses between the different scenarios that was not observed in the computerised tasks. Possible explanations for this difference between computer task and interview have been offered here, however these are merely speculative and should be investigated further.\n\n\nThe studies presented here are exploratory in design. The aim was to identify whether or not the phenomenon of moral dumbfounding could be elicited in a robust fashion. There was no experimental manipulation and analyses were primarily descriptive. These studies raise significant questions about the mechanisms underlying dumbfounded responses to moral judgement tasks, but clearly indicate that such dumbfounded responses can be reliably elicited, and demonstrate interesting variability. Future research is needed to identify specific variables that may moderate dumbfounding; examples may include meaning maintenance and meaning threat (Heine, Proulx, and Vohs 2006; Proulx and Inzlicht 2012), need for closure (Kruglanski and Webster 1996; Kruglanski 2013), or zeal (McGregor et al. 2001; McGregor 2006a, 2006b; McGregor, Nash, and Prentice 2012)."
  }
]